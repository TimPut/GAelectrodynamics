%
% Copyright Â© 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

In general the product of two blades is a multivector, with a selection of different grades.
For example, the product of two bivectors may have grades 0, 2, or 4

\begin{dmath}\label{eqn:generalizedDot:601}
\Be_{12} \lr{ \Be_{21} + \Be_{23} + \Be_{34} }
=
1 + \Be_{13} + \Be_{1234}.
\end{dmath}

Similarily, 
the product of a vector and bivector generally has grades 1 and 3

\begin{dmath}\label{eqn:generalizedDot:621}
\Be_1 \lr{ \Be_{12} + \Be_{23} }
=
\Be_2 + \Be_{123}.
\end{dmath}

We've identified the vector dot product with scalar grade selection of their vector product, the selection of the lowest grade of their product.
This motivates the definition of a general multivector dot product

\index{multivector dot product}
\makedefinition{Multivector dot product}{dfn:generalizedDot:100}{
The dot (or inner) product of two multivectors
\( A = \sum_{i = 0}^N \gpgrade{A}{i}, B = \sum_{i = 0}^N \gpgrade{B}{i} \)
is defined as
\begin{equation*}
A \cdot B \equiv
\sum_{i,j = 0}^N \gpgrade{ A_i B_j }{\Abs{i - j}}.
\end{equation*}
}

If \( A, B \) are k-vectors with equal grade, then the dot product is just the scalar selection of their product

\begin{dmath}\label{eqn:generalizedDot:580}
A \cdot B = \gpgradezero{ A B },
\end{dmath}

and if \( A, B \) are a k-vectors with grades \( i \ne j \) respectively, then their dot product is a single grade selection

\begin{dmath}\label{eqn:generalizedDot:581}
A \cdot B = \gpgrade{ A B }{\Abs{i - j}}.
\end{dmath}

\subsubsection{Dot product of a vector and bivector}

An important example of the generalized dot product is the dot product of a vector and bivector.
Unlike the dot product of two vectors, a vector-bivector dot product is order dependent.

The vector dot product is zero when the two vectors are normal.  This is also true if the vector and bivector are normal, that is, having no common factor, as in

\begin{equation}\label{eqn:generalizedDot:661}
\Be_1 \cdot \Be_{12} = \gpgradeone{ \Be_{123} } = 0.
\end{equation}

On the other hand, a non-zero vector-bivector dot product requires the vector to have some overlap with the bivector.
A bivector formed from the product of two normal vectors \( B = \Ba \Bb, \, \Ba \cdot \Bb = 0 \), will have a non-zero dot product with any vector that lies in \( \Span \setlr{ \Ba, \Bb} \)

\begin{dmath}\label{eqn:generalizedDot:681}
\lr{ \alpha \Ba + \beta \Bb } \cdot (\Ba\Bb)
=
\alpha \Norm{\Ba}^2 \Bb - \beta \Norm{\Bb}^2 \Ba.
\end{dmath}

There is a direct relationship between the
dot product of a vector and a 2-blade (i.e. wedge of two vectors) with a familiar \R{3} vector algebra result.  That expansion is

\maketheorem{Dot product of vector and 2-blade.}{thm:generalizedDot:wedgeDotDistribution}{
The dot product of a vector and a wedge product of two vectors distributes as
\begin{equation*}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\lr{ \Bc \wedge \Bb } \cdot \Ba
=
( \Ba \cdot \Bb ) \Bc
-( \Ba \cdot \Bc ) \Bb.
\end{equation*}

For vectors in \R{3}, this dot product can be expressed as a triple cross product
\begin{equation*}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\lr{ \Bb \cross \Bc } \cross \Ba.
\end{equation*}
} % theorem

The second part of this theorem, only true for \R{3} vectors, is easiest to prove.  To do so, the product of the vector and the wedge can be re-expressed in terms of the cross product

\begin{dmath}\label{eqn:generalizedDot:n}
\Ba \lr{ \Bb \wedge \Bc }
=
\Ba I \lr{ \Bb \cross \Bc }
=
I \Ba \cdot \lr{ \Bb \cross \Bc }
+ 
I \Ba \wedge \lr{ \Bb \cross \Bc }
=
I \Ba \cdot \lr{ \Bb \cross \Bc }
+ 
I^2 \Ba \cross \lr{ \Bb \cross \Bc }.
\end{dmath}

This multivector has a pseudoscalar (grade 3) component, and a vector components.  The vector-bivector dot product selects just the vector component, leaving

\begin{dmath}\label{eqn:generalizedDot:n}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
- \Ba \cross \lr{ \Bb \cross \Bc },
\end{dmath}

which can be reordered to complete the proof.


There are (somewhat tricky) coordinate free ways to prove the distribution identity of this theorem, but a dumber simple expansion in coordinates also does the job.

\subsection{rewrite from here.}

Expansion in coordinates is frowned upon in a number of GA references, but can be a quick way to the results of interest.  Consider such an expansion for a \R{N} vector space

\begin{dmath}\label{eqn:gradeselectionProblems:681}
\begin{aligned}
\Ba \cdot \lr{ \Bb \wedge \Bc } &= \sum_{i, j, k} a_i b_j c_k \Be_i \cdot (\Be_j \wedge \Be_k) \\
\lr{ \Bb \wedge \Bc } \cdot \Ba &= \sum_{i, j, k} a_i b_j c_k (\Be_j \wedge \Be_k) \cdot \Be_i
\end{aligned}
\end{dmath}

Observe that these sums can be restricted to indexes \( i \ne j \), since \( \Bx \wedge \Bx = 0 \) for any \(\Bx\).  The dot products are

\begin{dmath}\label{eqn:gradeselectionProblems:820}
\Be_i \cdot (\Be_j \wedge \Be_k)
=
\gpgradeone{ \Be_i (\Be_j \wedge \Be_k) }
=
\gpgradeone{ \Be_i \Be_j \Be_k },
\end{dmath}

and
\begin{dmath}\label{eqn:gradeselectionProblems:840}
(\Be_j \wedge \Be_k) \cdot \Be_i
=
\gpgradeone{ (\Be_j \wedge \Be_k) \Be_i }
=
\gpgradeone{ \Be_j \Be_k \Be_i }.
\end{dmath}

In each expansion, there are three cases, one where \( i,j,k\) are all unique.  In this case, the vector product is a trivector, so the grade one selection is zero.  That leaves only \( i = j \ne k \), and \( i = k \ne j \).

Consider the \( i = j \) case in the first dot product expansion

\begin{dmath}\label{eqn:gradeselectionProblems:860}
\gpgradeone{ \Be_i \Be_j \Be_k }
=
\gpgradeone{ \Be_i \Be_i \Be_k }
=
\gpgradeone{ \Be_k }
=
\Be_k.
\end{dmath}

For the \( i = k \) case, this is

\begin{dmath}\label{eqn:gradeselectionProblems:880}
\gpgradeone{ \Be_i \Be_j \Be_k }
=
\gpgradeone{ \Be_i (-\Be_k \Be_j) }
=
-\gpgradeone{ \Be_i \Be_i \Be_j }
=
-\gpgradeone{ \Be_j }
=
-\Be_j.
\end{dmath}

Inspection shows that the general pattern is
\begin{dmath}\label{eqn:gradeselectionProblems:900}
\Be_i \cdot (\Be_j \wedge \Be_k) =
(\Be_i \cdot \Be_j) \Be_k
-(\Be_i \cdot \Be_k) \Be_j,
\end{dmath}

and
\begin{dmath}\label{eqn:gradeselectionProblems:920}
(\Be_j \wedge \Be_k) \cdot \Be_i =
(\Be_i \cdot \Be_k) \Be_j
-(\Be_i \cdot \Be_j) \Be_k.
\end{dmath}

Substitution back into \cref{eqn:gradeselectionProblems:681} proves the first result for Euclidean spaces.  For the relation to the cross product in the \R{3} case

\begin{dmath}\label{eqn:gradeselectionProblems:940}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\gpgradeone{
\Ba \lr{ \Bb \wedge \Bc }
}
=
\gpgradeone{
\Ba I \lr{ \Bb \cross \Bc }
}
=
\gpgradeone{
I \Ba \lr{ \Bb \cross \Bc }
}
=
\gpgradeone{
I \lr{
\Ba \wedge \lr{ \Bb \cross \Bc }
+
\Ba \cdot \lr{ \Bb \cross \Bc }
}
}.
\end{dmath}

The dot product leaves the vector selection of a trivector, which is zero.  Expanding the wedge product as a cross product once again gives
\begin{dmath}\label{eqn:gradeselectionProblems:960}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\gpgradeone{
I^2
\Ba \cross \lr{ \Bb \cross \Bc }
}
=
-\Ba \cross \lr{ \Bb \cross \Bc }.
\end{dmath}

