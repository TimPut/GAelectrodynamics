%
% Copyright Â© 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

In general the product of two blades is a multivector, with a selection of different grades.
For example, the product of two bivectors may have grades 0, 2, or 4

\begin{dmath}\label{eqn:generalizedDot:601}
\Be_{12} \lr{ \Be_{21} + \Be_{23} + \Be_{34} }
=
1 + \Be_{13} + \Be_{1234}.
\end{dmath}

Similarily,
the product of a vector and bivector generally has grades 1 and 3

\begin{dmath}\label{eqn:generalizedDot:621}
\Be_1 \lr{ \Be_{12} + \Be_{23} }
=
\Be_2 + \Be_{123}.
\end{dmath}

We've identified the vector dot product with scalar grade selection of their vector product, the selection of the lowest grade of their product.
This motivates the definition of a general multivector dot product

\index{multivector dot product}
\makedefinition{Multivector dot product}{dfn:generalizedDot:100}{
The dot (or inner) product of two multivectors
\( A = \sum_{i = 0}^N \gpgrade{A}{i}, B = \sum_{i = 0}^N \gpgrade{B}{i} \)
is defined as
\begin{equation*}
A \cdot B \equiv
\sum_{i,j = 0}^N \gpgrade{ A_i B_j }{\Abs{i - j}}.
\end{equation*}
}

If \( A, B \) are k-vectors with equal grade, then the dot product is just the scalar selection of their product

\begin{dmath}\label{eqn:generalizedDot:580}
A \cdot B = \gpgradezero{ A B },
\end{dmath}

and if \( A, B \) are a k-vectors with grades \( i \ne j \) respectively, then their dot product is a single grade selection

\begin{dmath}\label{eqn:generalizedDot:581}
A \cdot B = \gpgrade{ A B }{\Abs{i - j}}.
\end{dmath}

\subsubsection{Dot product of a vector and bivector}

An important example of the generalized dot product is the dot product of a vector and bivector.
Unlike the dot product of two vectors, a vector-bivector dot product is order dependent.

The vector dot product is zero when the two vectors are normal.  This is also true if the vector and bivector are normal, that is, having no common factor, as in

\begin{equation}\label{eqn:generalizedDot:661}
\Be_1 \cdot \Be_{23} = \gpgradeone{ \Be_{123} } = 0.
\end{equation}

On the other hand, a non-zero vector-bivector dot product requires the vector to have some overlap with the bivector.
A bivector formed from the product of two normal vectors \( B = \Ba \Bb, \, \Ba \cdot \Bb = 0 \), will have a non-zero dot product with any vector that lies in \( \Span \setlr{ \Ba, \Bb} \)

\begin{dmath}\label{eqn:generalizedDot:681}
\lr{ \alpha \Ba + \beta \Bb } \cdot (\Ba\Bb)
=
\alpha \Norm{\Ba}^2 \Bb - \beta \Norm{\Bb}^2 \Ba.
\end{dmath}

There is a direct relationship between the
dot product of a vector and a 2-blade (i.e. wedge of two vectors) that has a familiar equivalent in \R{3} vector algebra result.  That expansion is

\maketheorem{Dot product of vector and 2-blade.}{thm:generalizedDot:wedgeDotDistribution}{
The dot product of a vector and a wedge product of two vectors distributes as
\begin{equation*}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\lr{ \Bc \wedge \Bb } \cdot \Ba
=
( \Ba \cdot \Bb ) \Bc
-( \Ba \cdot \Bc ) \Bb.
\end{equation*}
} % theorem

There are (somewhat tricky) coordinate free ways to prove this theorem, but a dumber simple expansion in coordinates also does the job.

\begin{dmath}\label{eqn:gradeselectionProblems:681}
\begin{aligned}
\Ba \cdot \lr{ \Bb \wedge \Bc } &= \sum_{i, j, k} a_i b_j c_k \Be_i \cdot (\Be_j \wedge \Be_k) 
= \sum_{i, j \ne k} a_i b_j c_k \gpgradeone{ \Be_i \Be_j \Be_k }
\\
\lr{ \Bc \wedge \Bb } \cdot \Ba &= \sum_{i, j, k} a_i b_j c_k (\Be_k \wedge \Be_j) \cdot \Be_i
= \sum_{i, j \ne k} a_i b_j c_k \gpgradeone{ \Be_k \Be_j \Be_i }
\end{aligned}
\end{dmath}

If all of \( i, j, k \) are unique then \( \gpgradeone{ \Be_i \Be_j \Be_k } = 0 \), so the vector selection is non-zero only when \( i \) equals one of \( j, k \).  For example

\begin{dmath}\label{eqn:generalizedDot:1040}
\begin{aligned}
\gpgradeone{ \Be_1 \Be_1 \Be_2 } &= \Be_2 \\
\gpgradeone{ \Be_1 \Be_2 \Be_1 } &= -\Be_1.
\end{aligned}
\end{dmath}

Given \( j \ne k \), and \( i = j \) or \( i = k \),  then it is simple to show 
(\cref{problem:generalizedDot:distributionUnitVectorsa})
that

\begin{equation}\label{eqn:generalizedDot:1020}
\gpgradeone{ \Be_i \Be_j \Be_k }
= \gpgradeone{ \Be_k \Be_j \Be_i },
\end{equation}

so \( \Ba \cdot \lr{ \Bb \wedge \Bc } = \lr{ \Bc \wedge \Bb } \cdot \Ba \).  Additionally
(\cref{problem:generalizedDot:distributionUnitVectorsb})

\begin{equation}\label{eqn:generalizedDot:980}
\gpgradeone{ \Be_i \Be_j \Be_k }
=
\Be_k \lr{ \Be_j \cdot \Be_i }
-\Be_j \lr{ \Be_k \cdot \Be_i }.
\end{equation}

Plugging \cref{eqn:generalizedDot:980} back into \cref{eqn:gradeselectionProblems:681} proves the theorem

\begin{dmath}\label{eqn:generalizedDot:1060}
\Ba \cdot \lr{ \Bb \wedge \Bc }
= \sum_{i, j, k} a_i b_j c_k \lr{ \Be_k \lr{ \Be_j \cdot \Be_i }
-\Be_j \lr{ \Be_k \cdot \Be_i } }
=
\lr{ \Ba \cdot \Bb } \Bc
- \lr{ \Ba \cdot \Bc } \Bb.
\end{dmath}

\maketheorem{}{thm:generalizedDot:tripleCross}{
For vectors in \R{3}, the dot product of a vector and vector wedge product can be expressed as a triple cross product
\begin{equation*}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\lr{ \Bb \cross \Bc } \cross \Ba.
\end{equation*}
} % theorem

This can be proved by comparing the distribution identity for the triple cross product to that of the vector-bivector dot product.
Alternatively, it can be proven directly by applying the identity \( \Ba \wedge \Bb = I ( \Ba \cross \Bb ) \) to the vector-bivector product, and then selecting the vector grade

\begin{dmath}\label{eqn:generalizedDot:1000}
\Ba \lr{ \Bb \wedge \Bc }
=
\Ba I \lr{ \Bb \cross \Bc }
=
I \lr{ \Ba \cdot \lr{ \Bb \cross \Bc } }
+
I \Ba \wedge \lr{ \Bb \cross \Bc }
=
I \lr{ \Ba \cdot \lr{ \Bb \cross \Bc } }
+
I^2 \Ba \cross \lr{ \Bb \cross \Bc }
=
I \lr{ \Ba \cdot \lr{ \Bb \cross \Bc } }
+
\lr{ \Bb \cross \Bc } \cross \Ba.
\end{dmath}

This multivector has a pseudoscalar (grade 3) component, and a vector component, so

\begin{dmath}\label{eqn:generalizedDot:1080}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\gpgradeone{ \Ba \lr{ \Bb \wedge \Bc } }
=
\lr{ \Bb \cross \Bc } \cross \Ba.
\end{dmath}
