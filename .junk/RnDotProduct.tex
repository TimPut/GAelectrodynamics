%
% Copyright Â© 2016 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

\makeproblem{\R{n} dot product.}{problem:gradeselection:RnDotProduct}{
Show that \ref{dfn:gradeselection:100} when applied to two vectors
is equivalent to the traditional \R{n} dot product.
} % problem

\makeanswer{problem:gradeselection:RnDotProduct}{
Let
\begin{dmath}\label{eqn:gradeselectionProblems:180}
\begin{aligned}
\Bx &= \sum_{i=1}^N x_i \Be_i \\
\By &= \sum_{i=1}^N y_i \Be_i.
\end{aligned}
\end{dmath}

The dot product of these two vectors is
\begin{dmath}\label{eqn:gradeselectionProblems:200}
\Bx \cdot \By
\equiv
\gpgradezero{ \Bx \By }
=
\gpgradezero{
\lr{ \sum_{i=1}^N x_i \Be_i}
\lr{ \sum_{j=1}^N y_j \Be_j}
}
=
\sum_{1 \le i = j \le N}
x_i y_j
\gpgradezero{ \Be_i \Be_j }
+
\sum_{1 \le i \ne j \le N}
x_i y_j
\gpgradezero{ \Be_i \Be_j }
\end{dmath}

In the \( i = j \) sum, the term \( \Be_i \Be_j = \Be_i^2 = 1 \), so the scalar grade selection of that multivector product is just 1.  In the \( i = j \) term, each of the \( \Be_i \Be_j \) products is a bivector, so each of those scalar grade selections is zero.

That leaves

\begin{dmath}\label{eqn:gradeselectionProblems:220}
\Bx \cdot \By
=
\sum_{i =1}^N x_i y_i. \qedmarker
\end{dmath}
} % answer
