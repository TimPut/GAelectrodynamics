%
% Copyright Â© 2016 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

\makeproblem{Vector bivector dot product}{problem:gradeselection:vectorBivectorDot}{
The dot product of a vector and bivector in \R{N} (or in fact any metric) expands as

\boxedEquation{eqn:gradeselection:660}{
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
-\lr{ \Bb \wedge \Bc } \cdot \Ba
=
( \Ba \cdot \Bb ) \Bc
-( \Ba \cdot \Bc ) \Bb.
}

Demonstrate this by coordinate expansion using an orthonormal basis for \R{N}.

The right hand side may look familiar.  Demonstrate, for \R{3} without expansion in coordinates, that

\boxedEquation{eqn:gradeselection:680}{
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
-\Ba \cross \lr{ \Bb \cross \Bc }.
}
} % problem

\makeanswer{problem:gradeselection:vectorBivectorDot}{
Expansion in coordinates is frowned upon in a number of GA references, but can be a quick way to the results of interest.  Consider such an expansion for a \R{N} vector space

\begin{dmath}\label{eqn:gradeselectionProblems:681}
\begin{aligned}
\Ba \cdot \lr{ \Bb \wedge \Bc } &= \sum_{i, j, k} a_i b_j c_k \Be_i \cdot (\Be_j \wedge \Be_k) \\
\lr{ \Bb \wedge \Bc } \cdot \Ba &= \sum_{i, j, k} a_i b_j c_k (\Be_j \wedge \Be_k) \cdot \Be_i
\end{aligned}
\end{dmath}

Observe that these sums can be restricted to indexes \( i \ne j \), since \( \Bx \wedge \Bx = 0 \) for any \(\Bx\).  The dot products are

\begin{dmath}\label{eqn:gradeselectionProblems:820}
\Be_i \cdot (\Be_j \wedge \Be_k)
=
\gpgradeone{ \Be_i (\Be_j \wedge \Be_k) }
=
\gpgradeone{ \Be_i \Be_j \Be_k },
\end{dmath}

and
\begin{dmath}\label{eqn:gradeselectionProblems:840}
(\Be_j \wedge \Be_k) \cdot \Be_i
=
\gpgradeone{ (\Be_j \wedge \Be_k) \Be_i }
=
\gpgradeone{ \Be_j \Be_k \Be_i }.
\end{dmath}

In each expansion, there are three cases, one where \( i,j,k\) are all unique.  In this case, the vector product is a trivector, so the grade one selection is zero.  That leaves only \( i = j \ne k \), and \( i = k \ne j \).

Consider the \( i = j \) case in the first dot product expansion

\begin{dmath}\label{eqn:gradeselectionProblems:860}
\gpgradeone{ \Be_i \Be_j \Be_k }
=
\gpgradeone{ \Be_i \Be_i \Be_k }
=
\gpgradeone{ \Be_k }
=
\Be_k.
\end{dmath}

For the \( i = k \) case, this is

\begin{dmath}\label{eqn:gradeselectionProblems:880}
\gpgradeone{ \Be_i \Be_j \Be_k }
=
\gpgradeone{ \Be_i (-\Be_k \Be_j) }
=
-\gpgradeone{ \Be_i \Be_i \Be_j }
=
-\gpgradeone{ \Be_j }
=
-\Be_j.
\end{dmath}

Inspection shows that the general pattern is
\begin{dmath}\label{eqn:gradeselectionProblems:900}
\Be_i \cdot (\Be_j \wedge \Be_k) =
(\Be_i \cdot \Be_j) \Be_k
-(\Be_i \cdot \Be_k) \Be_j,
\end{dmath}

and
\begin{dmath}\label{eqn:gradeselectionProblems:920}
(\Be_j \wedge \Be_k) \cdot \Be_i =
(\Be_i \cdot \Be_k) \Be_j
-(\Be_i \cdot \Be_j) \Be_k.
\end{dmath}

Substitution back into \cref{eqn:gradeselectionProblems:681} proves the first result for Euclidean spaces.  For the relation to the cross product in the \R{3} case

\begin{dmath}\label{eqn:gradeselectionProblems:940}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\gpgradeone{
\Ba \lr{ \Bb \wedge \Bc }
}
=
\gpgradeone{
\Ba I \lr{ \Bb \cross \Bc }
}
=
\gpgradeone{
I \Ba \lr{ \Bb \cross \Bc }
}
=
\gpgradeone{
I \lr{
\Ba \wedge \lr{ \Bb \cross \Bc }
+
\Ba \cdot \lr{ \Bb \cross \Bc }
}
}.
\end{dmath}

The dot product leaves the vector selection of a trivector, which is zero.  Expanding the wedge product as a cross product once again gives
\begin{dmath}\label{eqn:gradeselectionProblems:960}
\Ba \cdot \lr{ \Bb \wedge \Bc }
=
\gpgradeone{
I^2
\Ba \cross \lr{ \Bb \cross \Bc }
}
=
-\Ba \cross \lr{ \Bb \cross \Bc }.
\end{dmath}

} % answer
