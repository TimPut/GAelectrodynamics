Geometric algebra generalizes and extends vector algebra, providing a mathematical representation for

\begin{itemize}
\item
oriented (signed) points with magnitude (scalars, or 0-vectors),
\item
oriented line segments (vectors, or 1-vectors),
\item
oriented planes (bivectors, or 2-vectors),
\item
oriented volumes (trivectors, or 3-vectors),
\item
oriented hypervolumes (i.e. k-vectors).
\end{itemize}

Sums of scalars, vectors, bivectors, trivectors, or k-vectors, are called multivectors.
A non-commutative (order matters) multivector multiplication operation is also defined.
In particular, a product of two or more vectors is defined.  This vector product explicitly
relates the dot and cross products of traditional three-dimensional vector algebra.
The products of two or more mutually perpendicular vectors generate the higher degree k-vectors in the algebra.
For example, a bivector can be formed from the product of
two perpendicular vectors or sums of such products, and a trivector can be constructed from products of three perpendicular vectors.
%In general a k-vector is a product of \( k \) mutually perpendicular vectors, or a sum of such products.

Vectors already have many generalizations in mathematics, where directed ``arrows'', tuples of real or complex numbers, matrices, functions, polynomials, quantum states, and other objects can all be considered vectors.
In Geometric algebra we are generally not interested in vectors in this abstract sense,
% (matrices, functions, polynomials, tuples of complex numbers, ...),
nor are we interested in infinite dimensional vectors.
The vectors that are the building blocks of Geometric algebra can be thought of as a representation of directed ``arrows in space'' (or space-time), that is, a line segment with direction and magnitude.
Much of our attention can be focused on the familiar vectors of one, two, or three-dimensional space that would be represented as a tuple or column vector of coordinates such as \( \Bv = (x, y, z)\) in vector or linear algebra.
We will prefer to use a representation such as \( \Bv = x \Be_1 + y \Be_2 + z \Be_3 \) where the coordinates are never found isolated from their respective direction vectors.
%, \Bv = x \xcap + y \ycap + z \zcap, or \Bv = x \ahat_x + y \ahat_y + z \ahat_z.

\paragraph{Vector space}

A vector space is an enumeration of the properties and operations that are common to such a set of (perhaps seemingly unrelated) vector-like objects, allowing them to be treated in a unified fashion, regardless of their representation.
The definition of a multivector space will have a striking resemblance to the definition of a vector space, with some subtle but important differences.
It is worthwhile to explicitly state the definition of a vector space, both as a reminder for the reader, and for comparison with the multivector space definition to come.  Some additional concepts from linear algebra are also reviewed here in the lead up to the definition of a multivector space.

\makedefinition{Vector space.}{def:prerequisites:vectorspace}{
A (real) vector space is a set \( V \), the elements of which are called vectors, which has an addition and scalar multiplication operations, such that
for all vectors \( \Bx, \By, \Bz \in V \) and scalars \( a, b \in \bbR \),
the following axioms are satisfied

\begin{tcolorbox}[tab2,tabularx={X|Y},title=Vector space axioms.,boxrule=0.5pt]
    Addition is closed. & \( \Bx + \By \in V \) \\ \hline
    (Scalar) multiplication is closed. & \( a \Bx \in V \) \\ \hline
    Addition is associative. & \( (\Bx + \By) + \Bz = \Bx + (\By + \Bz) \) \\ \hline
    Addition is commutative. & \( \By + \Bx = \Bx + \By \) \\ \hline
    There exists a zero element \( 0 \in V \).  & \( \Bx + 0 = \Bx \) \\ \hline
    For any \( \Bx \in V \) there exists a negative additive inverse \( -\Bx \in V \). & \( \Bx + (-\Bx) = 0 \) \\ \hline
    (Scalar) multiplication is distributive.  & \( a( \Bx + \By ) = a \Bx + a \By \), \( (a + b)\Bx = a \Bx + b\Bx \) \\ \hline
    (Scalar) multiplication is associative. & \( (a b) \Bx = a ( b \Bx ) \) \\ \hline
    There exists a multiplicative identity \( 1 \). & \( 1 \Bx = \Bx \) \\ \hline
\end{tcolorbox}
}

\makeproblem{\R{3}}{problem:prerequisites:R3}{
Define \R{3} as the set of triples \( \setlr{ (x_1, x_2, x_3) | x_i \in \bbR } \).  Given
\( \Bx = (x_1, x_2, x_3) \in \bbR^3 \),
\( \By = (y_1, y_2, y_3) \in \bbR^3 \), and \( c \in \bbR \), then addition and multiplication operations can be defined respectively as

\begin{equation*}
\begin{aligned}
\Bx + \By &\equiv (x_1 + y_1, x_2 + y_2, x_3 + y_3) \\
c \Bx &\equiv (c x_1 , c x_2 , c x_3 ).
\end{aligned}
\end{equation*}

Show that \R{3} is a vector space.
} % problem

\makeproblem{Pauli matrices.}{problem:multivector:20}{
The Pauli matrices are defined as

\begin{equation}\label{eqn:multivector:160}
   \sigma_1 = \PauliX,\quad
   \sigma_2 = \PauliY,\quad
   \sigma_3 = \PauliZ.
\end{equation}

Given any scalars \( a, b, c \in \bbR \), show that the set \( V = \setlr{ a \sigma_1 + b \sigma_2 + c \sigma_3 } \) is a vector space with respect to the operations of matrix addition and multiplication, and
determine the form of the zero and identity elements.

% FIXME: make this a problem at at location where it makes sense:
%\makesubproblem{}{problem:multivector:20:b}
%
%Show that \( \sigma_k^2 = I \), where \( I \) is the 2x2 identity matrix, and that \( \sigma_k \sigma_j = -\sigma_k \sigma_j \) for all \( k \ne j \).
%
%\makesubproblem{}{problem:multivector:20:c}
%
%Using the results of \partref{problem:multivector:20:b}, show that
%\( \lr{ a \sigma_x + b \sigma_y + c \sigma_z }^2 = (a^2 + b^2 + c^2) I \), where \( I \) is the 2x2 identity matrix.
%%This shows that the Pauli matrices are an example \R{3} basis for which the contraction axiom is built right into the representation.
} % problem

\makeproblem{Function space.}{problem:multivector:30}{
   Given real functions \( f(x) = e^x \) and \( g(x) = x^2 \), and scalars \( a,b \in \bbR \) determine whether the set
   \( V = \setlr{ a f(x) + b g(x) } \) is a vector space, and if so,
determine the form of the zero and identity elements.
} % problem

\makeproblem{Polynomial vector space.}{problem:multivector:35}{
  Determine whether or not the set of N'th degree polynomials \( V = \setlr{ \sum_{k=0}^N a_k x^k \mid a_k \in \bbR } \) is a vector space.
} % problem

\paragraph{Basis, span and dimension.}

\makedefinition{Linear combination}{dfn:prerequisites:linearcombination}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).
A linear combination of vectors in \( S \) is any sum
\begin{equation*}
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k.
\end{equation*}
} % definition

\makedefinition{Linear dependence.}{dfn:prerequisites:dependence}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).
This set \( S \) is linearly dependent if any equation
\begin{equation*}
0 =
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k,
\end{equation*}

can be constructed for which not all of the coefficients \( a_i \) are zero.
} % definition

\makedefinition{Linear independence.}{dfn:prerequisites:independence}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).
This set is linearly independent if the there are no equations with \( a_i \ne 0 \) such that
\begin{equation*}
0 =
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k.
\end{equation*}
} % definition

\makedefinition{Span.}{dfn:prerequisites:span}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  The span
of this set is the set of all linear combinations of these vectors, denoted
\begin{equation*}
\Span(S) =
\setlr{
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k}.
\end{equation*}
} % definition

\makedefinition{Subspace.}{dfn:prerequisites:subspace}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  This subset is
a subspace if \( S \) is a vector space under the multiplication and addition operations of the vector space \( V \).
} % definition

\makedefinition{Basis and dimension}{dfn:multivector:basisanddimension}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_n } \) be a linearly independent subset of \( V \).  This set is a basis if \( \Span(S) = V \).  The number of vectors \( n \) in this set is called the dimension of the space.
} % definition

\paragraph{Coordinates and standard basis.}

\makedefinition{Coordinates.}{dfn:prerequisites:coordinates}{
%Given a basis \( B =
FIXME: define
} % definition

\makedefinition{Standard basis.}{dfn:prerequisites:standardbasis}{
FIXME: define
} % definition

%\makedefinition{Basis and coordinates}{dfn:multivector:basis}{
%   If \( N \) is the dimension of a vector space \( V \), a set of \( N \) vectors \( B = \setlr{ \Ba_1, \Ba_2, \cdots , \Ba_N } \) is a basis for that vector space, if it is possible to form any vector \( \Bx \in V \) as a linear combination of those vectors \( \Ba_k \).  That is, there exists scalars \( c_k \) such that for any \( \Bx \in V \)
%
%\begin{equation*}
%   \Bx = \sum_{k = 1}^N c_k \Ba_k.
%\end{equation*}
%
%The numbers \( (c_1, c_2, \cdots, c_N ) \) are referred to as the coordinates of the vector \( \Bx \) with respect to the basis \( B \).
%}

While it is common, especially in engineering, to represent a vector \( \Bx \) as a tuple or column vector, say,

\begin{dmath}\label{eqn:multivector:180}
   \Bx =
\begin{bmatrix}
   c_1 \\
   c_2 \\
   \vdots \\
   c_N \\
\end{bmatrix},
\end{dmath}

where the basis associated with the vector is implied, this will not be done here.  Instead, whenever coordinates are used, they will be explicitly paired with their associated basis vectors.

\makedefinition{Standard basis, and dot product properties.}{dfn:multivector:standardbasis}{
   Any vector space \( V \) used in this book will be assumed to have been generated from a basis \( \setlr{ \Be_1, \Be_2, \cdots, \Be_N } \), associated with a dot product that has the properties

\begin{enumerate}
   \item \( \Be_i \cdot \Be_i = \pm 1 \).
   \item \( \Be_i \cdot \Be_j = 0 \) for any \( i \ne j \).
\end{enumerate}

Such a basis will called a standard basis.  When these dot products are always positive, the vector space is referred to as a Euclidean vector space.
}

\paragraph{FIXME: remove?}
There are many possible standard bases sets.  In \R{3}, it is conventional to refer to \( \Be_1, \Be_2, \Be_3 \) as the standard bases elements if these represent the directions of the x, y, and z directions respectively.  Unless otherwise noted \( \Be_k \) refers to the direction vector for the k-th direction in a standard basis for that space.
The only non-Euclidean vector space of interest in this book (for relativistic material), has a Minkowski dot product.  For such a space, the standard basis elements will be labelled \( \setlr{ \gamma_0, \gamma_1, \gamma_2, \gamma_3 } \), where for \( i \in [1,3] \), \( \gamma_0 \cdot \gamma_0 = \pm 1 = -\gamma_i \cdot \gamma_i \).  The positive sign convention will be used.

%GA requires the vector space to have an associated
%dot product \( \Bx \cdot \By \) that
%defines the notion of perpendicularity for the space.  We will want to extend the scalar multiplication operation of the vector
%space to complex numbers, but
%will not require a (complex) order dependent inner product \( \innerprod{\Bx}{\By} \) for our vector space.
%
%XX: new content.

\paragraph{The metric, length and normality.}

An abstract vector need not have an associated notion of length, nor a notion of perpendicularity (normality).
In abstract vector algebra, length and normality are provided by defining an associated dot product \(\Bx \cdot \By\), or inner product \(\innerprod{\Bx}{\By}\).
In Geometric algebra, length and normality of two vectors are provided by a metric \(g(\Bx, \By)\).
Like the dot product where \( \Bx \cdot \By = \By \cdot \Bx\), this metric is independent of order, a property that is not generally required of the inner product.
However, unlike both the dot and inner products of abstract vector algebra, where \( \Bx \cdot \Bx \ge 0\), and \( \innerprod{\Bx}{\Bx} \ge 0\), the metric \(g(\Bx, \Bx)\) may be negative (i.e. for spacetime vectors).
If \(c \) is any real or complex number, the metric in Geometric algebra is \( g(c \Bx, c \Bx) = c^2 g(\Bx, \Bx)\), unlike the inner product in complex spaces, where \( \innerprod{c \Bx}{c \Bx} = \Abs{c}^2 \innerprod{c \Bx}{c \Bx} \).
Effectively, this means that our underlying direction vectors are always real.


\makedefinition{Norm-squared}{dfn:multivector:norm}{
   The squared norm of a vector \( \Bx \) is defined as

\begin{dmath}\label{eqn:multivector:200}
   \Norm{\Bx}^2 = \Bx \cdot \Bx.
\end{dmath}

This quantity need not be positive.
}

\makedefinition{Length}{dfn:multivector:length}{
   The length of a vector \( \Bx \) is defined as

\begin{equation*}
\Norm{\Bx} =
\sqrt{\Abs{ \Norm{\Bx}^2 }}.
\end{equation*}
}

%A vector space with an associated norm based length is called a normed vector space.  Any dot product space is also a normed vector space.

\makedefinition{Unit vector}{dfn:multivector:unitvector}{
   A vector \( \Bx \) is called a unit vector if its length is one.
} % definition

A unit vector \( \xcap \) may be generated from any vector \( \Bx \) that has a non-zero squared norm by computing

\begin{dmath}\label{eqn:multivector:220}
\xcap = \frac{\Bx}{\sqrt{\Abs{\Norm{\Bx}^2}}}.
\end{dmath}

\makedefinition{Normal}{dfn:multivector:normal}{
   Two vectors are normal, or perpendicular, if their dot product is zero.
}

\makedefinition{Orthonormal}{dfn:multivector:orthonormal}{
   A set of vectors \( \setlr{ \Bx, \By, \cdots, \Bz } \) is an orthonormal set if all pairs of vectors in that set are normal and are also unit vectors.
} % definition

\subsection{Problems}
\makeproblem{Explicit squared norm}{problem:multivector:60}{
   Given a coordinate representation of a vector with respect to a standard basis
\begin{dmath}\label{eqn:multivector:240}
   \Bx = \sum_{i = 1}^N x_i \Be_i,
\end{dmath}

show that the squared norm is
\begin{dmath}\label{eqn:multivector:260}
   \Norm{\Bx}^2 = \Bx \cdot \Bx = \sum_{i = 1}^N x_i^2 (\Be_i \cdot \Be_i).
\end{dmath}

Observe that for a Euclidean vector space this is the squared length in the Pythagorean sense.
}

\makeproblem{Null vector}{problem:multivector:80}{
Given a two dimensional non-Euclidean vector space with basis elements satisfying
\( \gamma_0 \cdot \gamma_0 = 1 = -\gamma_1 \cdot \gamma_1 \), construct a vector that has a squared
norm of 0.  Such a vector is called a null vector.
%   \Bx = \gamma_0 + \gamma_1,
}

%%%\makeproblem{}{problem:multivector:50}{
%%%The most general definition of an Euclidean norm satisfies all of the properties
%%%
%%%\begin{enumerate}
%%%   \item \( \Norm{\Bx} \ge 0 \), and \( \Norm{\Bx} = 0 \iff \Bx = 0 \).
%%%   \item \( \Norm{a \Bx} = \Abs{a} \Norm{\Bx} \).
%%%   \item \( \Norm{\Bx + \By} \le \Norm{\Bx} + \Norm{\By} \).
%%%\end{enumerate}
%%%
%%%If the coordinates of a vector with respect to the standard basis are \( x_i \) then show that the Euclidean norm defined in
%%%that the Pythagorean norm
%%%\begin{equation*}
%%%\Norm{\Bx}^2 = \sum_{i = 1}^N x_i^2,
%%%\end{equation*}
%%%
%%%satisfies these properties.
%%%} % problem
%%%
