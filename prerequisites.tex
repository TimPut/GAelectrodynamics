As Geometric Algegra 
generalizes the concept of vector space, a quick review of some of the more fundamental definitions from linear and vector algebra is worthwhile.

\makedefinition{Vector space.}{def:prerequisites:vectorspace}{
A (real) vector space is a set \( V \), the elements of which are called vectors, which has an addition and scalar multiplication operations, such that 
for all vectors \( \Bx, \By, \Bz \in V \) and scalars \( a, b \in \bbR \),
the following axioms are satisfied

\begin{tcolorbox}[tab2,tabularx={X|Y},title=Vector space axioms.,boxrule=0.5pt]
    Addition is closed. & \( \Bx + \By \in V \) \\ \hline
    (Scalar) multiplication is closed. & \( a \Bx \in V \) \\ \hline
    Addition is associative. & \( (\Bx + \By) + \Bz = \Bx + (\By + \Bz) \) \\ \hline
    Addition is commutative. & \( \By + \Bx = \Bx + \By \) \\ \hline
    There exists a zero element \( 0 \in V \).  & \( \Bx + 0 = \Bx \) \\ \hline
    For any \( \Bx \in V \) there exists a negative additive inverse \( -\Bx \in V \). & \( \Bx + (-\Bx) = 0 \) \\ \hline
    (Scalar) multiplication is distributive.  & \( a( \Bx + \By ) = a \Bx + a \By \), \( (a + b)\Bx = a \Bx + b\Bx \) \\ \hline
    (Scalar) multiplication is associative. & \( (a b) \Bx = a ( b \Bx ) \) \\ \hline
    There exists a multiplicative identity \( 1 \). & \( 1 \Bx = \Bx \) \\ \hline
\end{tcolorbox}
}

The concept of vector space encapsulates the rules common to a number of mathematical structures, and allows seemingly unrelated mathematical constructs like
ordered tuples, matrices, directed ``arrows in space'', and functions to be treated in a unified fashion.  
That said, the general properties of vector spaces are not of primary interest here.  Instead, our use of Geometric algebra uses one of two types of vector spaces as building blocks

\begin{enumerate}
\item Vector spaces that represent lines, planes and volumes, and for which there is an associated Pythagorean notion of perpendicularity (Euclidean spaces).
\item Vector spaces that represent points in ``space-time'', with a time-like direction and one to three space-like directions.
\end{enumerate}

In either case the dimensionality of the underlying vector space is small and finite, with no more than four dimensions required.  
Perpendicularity will be defined by a dot product associated with the vector space, however, in neither case do we require an order dependent, or complex, 
dot product (i.e. an inner product).
There will be circumstances where we allow the coeffients of our vectors to be complex, but the underlying direction vectors should be thought of as real.

\makeproblem{\R{3}}{problem:prerequisites:R3}{
Define \R{3} as the set of triples \( \setlr{ (x_1, x_2, x_3) | x_i \in \bbR } \).  Given 
\( \Bx = (x_1, x_2, x_3) \in \bbR^3 \), 
\( \By = (y_1, y_2, y_3) \in \bbR^3 \), and \( c \in \bbR \), then addition and multiplication operations can be defined respectively as

\begin{equation*}
\begin{aligned}
\Bx + \By &\equiv (x_1 + y_1, x_2 + y_2, x_3 + y_3) \\
c \Bx &\equiv (c x_1 , c x_2 , c x_3 ).
\end{aligned}
\end{equation*}

Show that \R{3} is a vector space.
} % problem

\makeproblem{Pauli matrices.}{problem:multivector:20}{
The Pauli matrices are defined as

\begin{equation}\label{eqn:multivector:160}
   \sigma_1 = \PauliX,\quad
   \sigma_2 = \PauliY,\quad
   \sigma_3 = \PauliZ.
\end{equation}

Given any scalars \( a, b, c \in \bbR \), show that the set \( V = \setlr{ a \sigma_1 + b \sigma_2 + c \sigma_3 } \) is a vector space with respect to the operations of matrix addition and multiplication, and 
determine the form of the zero and identity elements.

% FIXME: make this a problem at at location where it makes sense:
%\makesubproblem{}{problem:multivector:20:b}
%
%Show that \( \sigma_k^2 = I \), where \( I \) is the 2x2 identity matrix, and that \( \sigma_k \sigma_j = -\sigma_k \sigma_j \) for all \( k \ne j \).
%
%\makesubproblem{}{problem:multivector:20:c}
%
%Using the results of \partref{problem:multivector:20:b}, show that
%\( \lr{ a \sigma_x + b \sigma_y + c \sigma_z }^2 = (a^2 + b^2 + c^2) I \), where \( I \) is the 2x2 identity matrix.
%%This shows that the Pauli matrices are an example \R{3} basis for which the contraction axiom is built right into the representation.
} % problem

\makeproblem{Function space.}{problem:multivector:30}{
   Given real functions \( f(x) = e^x \) and \( g(x) = x^2 \), and scalars \( a,b \in \bbR \) determine whether the set
   \( V = \setlr{ a f(x) + b g(x) } \) is a vector space, and if so, 
determine the form of the zero and identity elements.
} % problem

\makedefinition{Linear combination}{dfn:prerequisites:linearcombination}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  
A linear combination of vectors in \( S \) is any sum
\begin{equation*}
a_1 \Bx_1 
+
a_2 \Bx_2 
+ 
\cdots
+
a_k \Bx_k.
\end{equation*}
} % definition

\makedefinition{Linear dependence.}{dfn:prerequisites:dependence}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  
This set \( S \) is linearly dependent if any equation
\begin{equation*}
0 =
a_1 \Bx_1 
+
a_2 \Bx_2 
+ 
\cdots
+
a_k \Bx_k,
\end{equation*}

can be constructed for which not all of the coefficients \( a_i \) are zero.
} % definition

\makedefinition{Linear independence.}{dfn:prerequisites:independence}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  
This set is linearly independent if the there are no equations with \( a_i \ne 0 \) such that
\begin{equation*}
0 =
a_1 \Bx_1 
+
a_2 \Bx_2 
+ 
\cdots
+
a_k \Bx_k.
\end{equation*}
} % definition

\makedefinition{Span.}{dfn:prerequisites:span}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  The span
of this set is the set of all linear combinations of these vectors, denoted
\begin{equation*}
\Span(S) =
\setlr{
a_1 \Bx_1 
+
a_2 \Bx_2 
+ 
\cdots
+
a_k \Bx_k}.
\end{equation*}
} % definition

\makedefinition{Subspace.}{dfn:prerequisites:subspace}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  This subset is 
a subspace if \( S \) is a vector space under the multiplication and addition operations of the vector space \( V \).
} % definition

\makedefinition{Basis and dimension}{dfn:multivector:basisanddimension}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_n } \) be a linearly independent subset of \( V \).  This set is a basis if \( \Span(S) = V \).  The number of vectors \( n \) in this set is called the dimension of the space.
} % definition

\makedefinition{Coordinates.}{dfn:prerequisites:coordinates}{
FIXME: define
} % definition

\makedefinition{Standard basis.}{dfn:prerequisites:standardbasis}{
FIXME: define
} % definition

%\makedefinition{Basis and coordinates}{dfn:multivector:basis}{
%   If \( N \) is the dimension of a vector space \( V \), a set of \( N \) vectors \( B = \setlr{ \Ba_1, \Ba_2, \cdots , \Ba_N } \) is a basis for that vector space, if it is possible to form any vector \( \Bx \in V \) as a linear combination of those vectors \( \Ba_k \).  That is, there exists scalars \( c_k \) such that for any \( \Bx \in V \)
%
%\begin{equation*}
%   \Bx = \sum_{k = 1}^N c_k \Ba_k.
%\end{equation*}
%
%The numbers \( (c_1, c_2, \cdots, c_N ) \) are referred to as the coordinates of the vector \( \Bx \) with respect to the basis \( B \).
%}

While it is common, especially in engineering, to represent a vector \( \Bx \) as a tuple or column vector, say,

\begin{dmath}\label{eqn:multivector:180}
   \Bx =
\begin{bmatrix}
   c_1 \\
   c_2 \\
   \vdots \\
   c_N \\
\end{bmatrix},
\end{dmath}

where the basis associated with the vector is implied, this will not be done here.  Instead, whenever coordinates are used, they will be explicitly paired with their associated basis vectors.

\makedefinition{Standard basis, and dot product properties.}{dfn:multivector:standardbasis}{
   Any vector space \( V \) used in this book will be assumed to have been generated from a basis \( \setlr{ \Be_1, \Be_2, \cdots, \Be_N } \), associated with a dot product that has the properties

\begin{enumerate}
   \item \( \Be_i \cdot \Be_i = \pm 1 \).
   \item \( \Be_i \cdot \Be_j = 0 \) for any \( i \ne j \).
\end{enumerate}

Such a basis will called a standard basis.  When these dot products are always positive, the vector space is referred to as a Euclidean vector space.
}

There are many possible standard bases sets.  In \R{3}, it is conventional to refer to \( \Be_1, \Be_2, \Be_3 \) as the standard bases elements if these represent the directions of the x, y, and z directions respectively.  Unless otherwise noted \( \Be_k \) refers to the direction vector for the k-th direction in a standard basis for that space.

The only non-Euclidean vector space of interest in this book (for relativistic material), has a Minkowski dot product.  For such a space, the standard basis elements will be labelled \( \setlr{ \gamma_0, \gamma_1, \gamma_2, \gamma_3 } \), where for \( i \in [1,3] \), \( \gamma_0 \cdot \gamma_0 = \pm 1 = -\gamma_i \cdot \gamma_i \).  The positive sign convention will be used.

GA requires the vector space to have an associated
dot product \( \Bx \cdot \By \) that 
defines the notion of perpendicularity for the space.  We will want to extend the scalar multiplication operation of the vector 
space to complex numbers, but 
will not require a (complex) order dependent inner product \( \innerprod{\Bx}{\By} \) for our vector space.
Effectively, this means that our underlying direction vectors are always real.



\makedefinition{Norm-squared}{dfn:multivector:norm}{
   The squared norm of a vector \( \Bx \) is defined as

\begin{dmath}\label{eqn:multivector:200}
   \Norm{\Bx}^2 = \Bx \cdot \Bx.
\end{dmath}

This quantity need not be positive.
}

\makedefinition{Length}{dfn:multivector:length}{
   The length of a vector \( \Bx \) is defined as 

\begin{equation*}
\Norm{\Bx} = 
\sqrt{\Abs{ \Norm{\Bx}^2 }}.
\end{equation*}
}

A vector space with an associated norm based length is called a normed vector space.  Any dot product space is also a normed vector space.

\makedefinition{Unit vector}{dfn:multivector:unitvector}{
   A vector \( \Bx \) is called a unit vector if its length is one.
} % definition

A unit vector \( \xcap \) may be generated from any vector \( \Bx \) that has a non-zero squared norm by computing

\begin{dmath}\label{eqn:multivector:220}
\xcap = \frac{\Bx}{\sqrt{\Abs{\Norm{\Bx}^2}}}.
\end{dmath}

\makedefinition{Normal}{dfn:multivector:normal}{
   Two vectors are normal, or perpendicular, if their dot product is zero.
}

\makedefinition{Orthonormal}{dfn:multivector:orthonormal}{
   A set of vectors \( \setlr{ \Bx, \By, \cdots, \Bz } \) is an orthonormal set if all pairs of vectors in that set are normal and are also unit vectors.
} % definition

\subsection{Problems}
\makeproblem{Explicit squared norm}{problem:multivector:60}{
   Given a coordinate representation of a vector with respect to a standard basis
\begin{dmath}\label{eqn:multivector:240}
   \Bx = \sum_{i = 1}^N x_i \Be_i,
\end{dmath}

show that the squared norm is
\begin{dmath}\label{eqn:multivector:260}
   \Norm{\Bx}^2 = \Bx \cdot \Bx = \sum_{i = 1}^N x_i^2 (\Be_i \cdot \Be_i).
\end{dmath}

Observe that for a Euclidean vector space this is the squared length in the Pythagorean sense.
}

\makeproblem{Null vector}{problem:multivector:80}{
Given a two dimensional non-Euclidean vector space with basis elements satisfying 
\( \gamma_0 \cdot \gamma_0 = 1 = -\gamma_1 \cdot \gamma_1 \), construct a vector that has a squared
norm of 0.  Such a vector is called a null vector.
%   \Bx = \gamma_0 + \gamma_1,
}

%%%\makeproblem{}{problem:multivector:50}{
%%%The most general definition of an Euclidean norm satisfies all of the properties
%%%
%%%\begin{enumerate}
%%%   \item \( \Norm{\Bx} \ge 0 \), and \( \Norm{\Bx} = 0 \iff \Bx = 0 \).
%%%   \item \( \Norm{a \Bx} = \Abs{a} \Norm{\Bx} \).
%%%   \item \( \Norm{\Bx + \By} \le \Norm{\Bx} + \Norm{\By} \).
%%%\end{enumerate}
%%%
%%%If the coordinates of a vector with respect to the standard basis are \( x_i \) then show that the Euclidean norm defined in
%%%that the Pythagorean norm
%%%\begin{equation*}
%%%\Norm{\Bx}^2 = \sum_{i = 1}^N x_i^2,
%%%\end{equation*}
%%%
%%%satisfies these properties.
%%%} % problem
%%%
