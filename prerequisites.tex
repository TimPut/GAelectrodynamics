\section{Basic ideas before we get to the math}
Geometric algebra, or GA for short, generalizes and extends vector algebra, providing a mathematical representation for geometrical objects of each dimension in the space.
In a three dimensional space, there are representations for all of

\begin{itemize}
\item
oriented (signed) points with magnitude (\boldTextAndIndex{scalars}, or 0-vectors),
\item
oriented line segments (\boldTextAndIndex{vectors}, or 1-vectors),
\item
oriented planes (\boldTextAndIndex{bivectors}, or 2-vectors), and
\item
oriented volumes (\boldTextAndIndex{trivectors}, or 3-vectors),
\end{itemize}

and in higher dimensional spaces, any oriented hypervolume of dimension \( k \), a \boldTextAndIndex{k-vector}, can be representated.
Exactly what is meant by an oriented plane or volume will be discussed later.

What
differentiates GA from vector algebra is the presumption of a \boldTextAndIndex{multiplication operation}.
This
multiplication operation is generally non-commutative (i.e. order matters), and is the generator of higher order k-vectors.
In particular,
bivectors are formed from the products of two perpendicular vectors,
and trivectors are constructed from products of three perpendicular vectors.
In general, k-vectors are constructed by multiplying \( k \) mutually perpendicular vectors, or by summing such products.
This multiplication operation will be seen to relate the dot and cross products of three dimensional vector algebra.

When we first learn vector algebra, it is stressed that addition of scalars and vectors is undefined.
That is not the case in GA, where a sum of k-vectors (with different \(k\)), called a \boldTextAndIndex{multivector},
is well formed and also required to represent some products.
For example, the general product of two vectors will be seen to have a scalar (0-vector) and a bivector (2-vector) component.

%  We have the special names, bivector and trivector for \( k = 2 \) and \( k = 3 \) respectively, but
%Such an object is called a k-vector, however, for \( k > 3 \) there no special names (bivector

\section{Linear algebra review and prerequisite concepts}

The following section reviews concepts from linear algebra that are required to algebraically express the concepts of perpendicularity, and systematically introduce definitons for k-vectors, multivectors, and multivector spaces.

It should be pointed out that GA uses an unconventional definition of dot product, where the dot product of a vector with itself may be negative.

\subsection{Vector space}

Vectors have many generalizations in mathematics, where directed ``arrows'', tuples of real or complex numbers, matrices, functions, polynomials, quantum states, and other
(perhaps seemingly unrelated)
mathematical objects
can all be considered vectors.
A vector space is an enumeration of the properties and operations that are common to such a set of
vector-like objects, allowing them to be treated in a unified fashion, regardless of their representation.
Our initial goal is to define a \boldTextAndIndex{multivector space}, a construct with
striking resemblance to that of a vector space, with some subtle but important differences.
To highlight those differences, it is worthwhile to first review the definition of a vector space and some other basic ideas from linear algebra.

\makedefinition{Vector space.}{def:prerequisites:vectorspace}{
A (real) vector space is a set \( V \), the elements of which are called vectors, which has an addition and scalar multiplication operations, such that
for all vectors \( \Bx, \By, \Bz \in V \) and scalars \( a, b \in \bbR \),
the following axioms are satisfied

\begin{tcolorbox}[tab2,tabularx={X|Y},title=Vector space axioms.,boxrule=0.5pt]
    Addition is closed. & \( \Bx + \By \in V \) \\ \hline
    (Scalar) multiplication is closed. & \( a \Bx \in V \) \\ \hline
    Addition is associative. & \( (\Bx + \By) + \Bz = \Bx + (\By + \Bz) \) \\ \hline
    Addition is commutative. & \( \By + \Bx = \Bx + \By \) \\ \hline
    There exists a zero element \( 0 \in V \).  & \( \Bx + 0 = \Bx \) \\ \hline
    For any \( \Bx \in V \) there exists a negative additive inverse \( -\Bx \in V \). & \( \Bx + (-\Bx) = 0 \) \\ \hline
    (Scalar) multiplication is distributive.  & \( a( \Bx + \By ) = a \Bx + a \By \), \( (a + b)\Bx = a \Bx + b\Bx \) \\ \hline
    (Scalar) multiplication is associative. & \( (a b) \Bx = a ( b \Bx ) \) \\ \hline
    There exists a multiplicative identity \( 1 \). & \( 1 \Bx = \Bx \) \\ \hline
\end{tcolorbox}
}

Despite the generality of this definition, it should be noted that in GA
we are generally not interested in vector spaces with abstract elements like matrices, functions, polynomials, or
tuples of complex numbers, nor are we interested in infinite dimensional vector spaces.
For electromagnetic applications of GA we are generally interested in two or three dimensional real vector spaces (i.e.
\R{2} and \R{3}), or in the
space-time vector spaces of special relativity, where a time like ``direction'' is added into the mix along with 1-3 spatial directions.

\makeproblem{\R{3}}{problem:prerequisites:R3}{
Define \R{3} as the set of triples \( \setlr{ (x_1, x_2, x_3) | x_i \in \bbR } \).  Given
\( \Bx = (x_1, x_2, x_3) \in \bbR^3 \),
\( \By = (y_1, y_2, y_3) \in \bbR^3 \), and \( c \in \bbR \), then addition and multiplication operations can be defined respectively as

\begin{equation*}
\begin{aligned}
\Bx + \By &\equiv (x_1 + y_1, x_2 + y_2, x_3 + y_3) \\
c \Bx &\equiv (c x_1 , c x_2 , c x_3 ).
\end{aligned}
\end{equation*}

Show that \R{3} is a vector space.
} % problem

\makeproblem{Pauli matrices.}{problem:multivector:20}{
The Pauli matrices are defined as

\begin{equation}\label{eqn:multivector:160}
   \sigma_1 = \PauliX,\quad
   \sigma_2 = \PauliY,\quad
   \sigma_3 = \PauliZ.
\end{equation}

Given any scalars \( a, b, c \in \bbR \), show that the set \( V = \setlr{ a \sigma_1 + b \sigma_2 + c \sigma_3 } \) is a vector space with respect to the operations of matrix addition and multiplication, and
determine the form of the zero and identity elements.

% FIXME: make this a problem at at location where it makes sense:
%\makesubproblem{}{problem:multivector:20:b}
%
%Show that \( \sigma_k^2 = I \), where \( I \) is the 2x2 identity matrix, and that \( \sigma_k \sigma_j = -\sigma_k \sigma_j \) for all \( k \ne j \).
%
%\makesubproblem{}{problem:multivector:20:c}
%
%Using the results of \partref{problem:multivector:20:b}, show that
%\( \lr{ a \sigma_x + b \sigma_y + c \sigma_z }^2 = (a^2 + b^2 + c^2) I \), where \( I \) is the 2x2 identity matrix.
%%This shows that the Pauli matrices are an example \R{3} basis for which the contraction axiom is built right into the representation.
} % problem

%\makeproblem{Function space.}{problem:multivector:30}{
%   Given real functions \( f(x) = e^x \) and \( g(x) = x^2 \), and scalars \( a,b \in \bbR \) determine whether the set
%   \( V = \setlr{ a f(x) + b g(x) } \) is a vector space, and if so,
%determine the form of the zero and identity elements.
%} % problem

\makeproblem{Polynomial vector space.}{problem:multivector:35}{
  Show that the set of N'th degree polynomials \( V = \setlr{ \sum_{k=0}^N a_k x^k \mid a_k \in \bbR } \) is a vector space.
} % problem

\subsection{Basis, span and dimension.}

\makedefinition{Linear combination}{dfn:prerequisites:linearcombination}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).
A linear combination of vectors in \( S \) is any sum
\begin{equation*}
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k.
\end{equation*}
} % definition

\makedefinition{Linear dependence.}{dfn:prerequisites:dependence}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).
This set \( S \) is linearly dependent if any equation
\begin{equation*}
0 =
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k,
\end{equation*}

can be constructed for which not all of the coefficients \( a_i \) are zero.
} % definition

\makedefinition{Linear independence.}{dfn:prerequisites:independence}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).
This set is linearly independent if the there are no equations with \( a_i \ne 0 \) such that
\begin{equation*}
0 =
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k.
\end{equation*}
} % definition

\makedefinition{Span.}{dfn:prerequisites:span}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  The span
of this set is the set of all linear combinations of these vectors, denoted
\begin{equation*}
\Span(S) =
\setlr{
a_1 \Bx_1
+
a_2 \Bx_2
+
\cdots
+
a_k \Bx_k}.
\end{equation*}
} % definition

\makedefinition{Subspace.}{dfn:prerequisites:subspace}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_k } \) be a subset of a vector space \( V \).  This subset is
a subspace if \( S \) is a vector space under the multiplication and addition operations of the vector space \( V \).
} % definition

\makedefinition{Basis and dimension}{dfn:multivector:basisanddimension}{
Let \( S = \setlr{ \Bx_1, \Bx_2, \cdots, \Bx_n } \) be a linearly independent subset of \( V \).  This set is a basis if \( \Span(S) = V \).  The number of vectors \( n \) in this set is called the dimension of the space.
} % definition

\subsection{Standard basis, length and normality.}

\makedefinition{Dot product.}{dfn:prerequisites:dotproduct}{
Let \( \Bx, \By \) be vectors from a vector space \( V \).
A dot product \( \Bx \cdot \By \) is a mapping \( V \cross V \rightarrow \bbR \)
with the following properties

\begin{tcolorbox}[tab2,tabularx={X|Y},title=Dot product properties.,boxrule=0.5pt]
    Symmetric in both arguments & \( \Bx \cdot \By = \By \cdot \Bx \) \\ \hline
    Bilinear & \( (a \Bx + b \By) \cdot (a' \Bx' + b' \By' ) = 
a a' (\Bx \cdot \Bx') + b b' (\By \cdot \By')
+
a b' (\Bx \cdot \By') + b a' (\By \cdot \Bx') \)
\\ \hline
    (Optional) Positive definite & \( \Bx \cdot \Bx \ge 0 \) \\ \hline
\end{tcolorbox}
} % definition

In GA it can be useful to omit the requirement for a dot product to have the positive definite property.  This has specific relevance in electrodynamics, when four-vector (relativistic) vector spaces are used.

Because the dot product is bilinear, it is
specified completely by the dot products of a set of basis elements for the space.  For example,
given a basis \( \setlr{ \Be_1, \Be_2, \cdots, \Be_N} \), and two vectors

\begin{dmath}\label{eqn:multivector:240}
\begin{aligned}
   \Bx &= \sum_{i = 1}^N x_i \Be_i \\
   \By &= \sum_{i = 1}^N y_i \Be_i,
\end{aligned}
\end{dmath}

the dot product of the two is

\begin{dmath}\label{eqn:prerequisites:260}
\Bx \cdot \By
=
   \lr{ \sum_{i = 1}^N x_i \Be_i } \cdot
   \lr{ \sum_{j = 1}^N y_j \Be_j }
=
   \sum_{i,j = 1}^N x_i y_j \lr{ \Be_i \cdot \Be_j }.
\end{dmath}

Such an expansion in coordinates can be written in matrix form as

\begin{dmath}\label{eqn:prerequisites:280}
\Bx \cdot \By
=
\Bx^\T G \By,
\end{dmath}

where \( G \) is the symmetric matrix with elements \( \Be_i \cdot \Be_j \).  This matrix \( G \), or its elements \( g_{ij} \) is also called the metric for the space.

In this book the metric is always diagonal, with all diagonal values having an absolute value of one.

\makedefinition{Length}{dfn:multivector:norm}{
   The squared norm of a vector \( \Bx \) is defined as

\begin{dmath}\label{eqn:multivector:200}
   \Norm{\Bx}^2 = \Bx \cdot \Bx,
\end{dmath}

a quantity that need not be positive.  The length of a vector \( \Bx \) is defined as

\begin{equation*}
\Norm{\Bx} =
\sqrt{\Abs{ \Norm{\Bx}^2 }}.
\end{equation*}
}

%A vector space with an associated norm based length is called a normed vector space.  Any dot product space is also a normed vector space.

\makedefinition{Unit vector}{dfn:multivector:unitvector}{
   A vector \( \Bx \) is called a unit vector if its absolute squared length is one, \( \Abs{\Bx \cdot \Bx} = 1 \).
} % definition

%A unit vector \( \xcap \) may be generated from any vector \( \Bx \) that has a non-zero squared norm by computing
%
%\begin{dmath}\label{eqn:multivector:220}
%\xcap = \frac{\Bx}{\sqrt{\Abs{\Norm{\Bx}^2}}}.
%\end{dmath}
%
\makedefinition{Normal}{dfn:multivector:normal}{
   Two vectors \( \Bx, \By \) are normal, or orthogonal, if their dot product is zero, \( \Bx \cdot \By = 0 \).
}

\makedefinition{Orthonormal}{dfn:multivector:orthonormal}{
   Two vectors are orthonormal if they are both unit vectors and normal to each other, \( \Bx \cdot \By = 0 \), \( \Abs{\Bx \cdot \Bx} = 1, \Abs{\By \cdot \By} = 1 \).

   A set of vectors \( \setlr{ \Bx, \By, \cdots, \Bz } \) is an orthonormal set if all pairs of vectors in that set are orthonormal.
}

\makedefinition{Standard basis.}{dfn:prerequisites:standardbasis}{
   A basis
\( \setlr{ \Be_1, \Be_2, \cdots, \Be_N} \) is called a standard basis if that set is orthonormal.
} % definition

\makedefinition{Euclidean space.}{dfn:prerequisites:euclideanspace}{
   A vector space with basis
   \( \setlr{ \Be_1, \Be_2, \cdots, \Be_N} \) is called Euclidean if all the dot product pairs between the basis elements are not only orthonormal, but positive definite.  That is

\begin{equation*}
\Be_i \cdot \Be_j = \delta_{ij}.
\end{equation*}
} % definition
