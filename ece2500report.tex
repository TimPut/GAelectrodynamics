%
% Copyright © 2018 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
\input{../latex/blogpost.tex}
\renewcommand{\basename}{ece2500report}
%\renewcommand{\dirname}{notes/phy1520/}
\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}
%\newcommand{\dateintitle}{}
%\newcommand{\keywords}{}

% too many alphabets fix:
% https://tex.stackexchange.com/a/243541/15
\newcommand\hmmax{0}
\newcommand\bmmax{0}

\input{../latex/peeter_prologue_print2.tex}

\usepackage{peeters_layout_exercise}
\usepackage{peeters_braket}
\usepackage{peeters_figures}

%\newcommand{\dLambertian}[0]{\square}
\newcommand{\dLambertian}[0]{\Box}

\newcommand{\dispdot}[2][.2ex]{\dot{\raisebox{0pt}[\dimexpr\height+#1][\depth]{$#2$}}}% \dispdot[<displace>]{<stuff>}
\newcommand{\dotBJ}[0]{\dispdot{\mathbf{J}}}

\newcommand{\stgrad}[0]{\lr{ \spacegrad + \inv{c} \PD{t}{}}}
\newcommand{\conjstgrad}[0]{\lr{ \spacegrad - \inv{c} \PD{t}{}}}

\usepackage{siunitx}
%\usepackage{mhchem} % \ce{}
\usepackage{macros_bm} % \bcM
\usepackage{macros_cal}
%\usepackage{macros_qed} % \qedmarker
\usepackage{txfonts} % \ointclockwise
\usepackage{enumerate}
\usepackage{mmacells}

\beginArtNoToc

\generatetitle{Project report ECE2500.  Geometric Algebra for Electrical Engineers}

\section{Motivation.}
This is the report for an ECE2500 M.Eng project course.

\subsubsection{Goals.}
This project had a few goals
\begin{enumerate}
\item Perform a literature review of applications of geometric algebra
%\footnote{Geometric algebra will be defined precisely later, along with bivector, trivector, multivector and other geometric algebra generalizations of the vector.}
to the study of electromagnetism.
\item Identify the subset of the literature that had direct relevance to electrical engineering.
\item Create a complete, and as compact as possible, introduction of the prerequisites required
for a graduate or advanced undergraduate electrical engineering student to be able to apply
geometric algebra to problems in electromagnetism.
\end{enumerate}

Geometric algebra
generalizes vectors, providing algebraic representations of not just directed line segments, but also points, plane segments, volumes, and higher degree geometric objects (hypervolumes.)
The geometric algebra representation of planes, volumes and hypervolumes requires a vector dot product, a vector multiplication operation, and a generalized addition operation.
The dot product
provides the length of a vector and a test for whether or not any two vectors are perpendicular.
The vector multiplication operation is used to construct
directed plane segments (bivectors),
and directed volumes (trivectors), which are built from the respective products of two or three mutually perpendicular vectors.
The addition operation allows for sums of scalars, vectors, or any products of vectors.  Such a sum is called a multivector.

The power to add scalars, vectors, and products of vectors can be exploited to simplify much of electromagnetism.
In particular, Maxwell's equations for isotropic media can be merged into a single multivector equation
\begin{dmath}\label{eqn:quaternion2maxwellWithGA:20}
\stgrad \lr{ \BE + I c \BB } = \eta\lr{ c \rho - \BJ },
\end{dmath}
where \( \spacegrad \) is the gradient, \( I = \Be_1 \Be_2 \Be_3 \) is the ordered product of the three \R{3} basis vectors, \( c = 1/\sqrt{\mu\epsilon}\) is the group velocity of the medium, \( \eta = \sqrt{\mu/\epsilon} \), \( \BE, \BB \) are the electric and magnetic fields, and
\( \rho \) and \( \BJ \) are the charge and current densities.
We will write this as
\begin{dmath}\label{eqn:ece2500report:40}
\stgrad F = J,
\end{dmath}
where \( F = \BE + I c \BB \) is the combined (multivector) electromagnetic field, and \( J = \eta\lr{ c \rho - \BJ } \) is the multivector current.
As a single PDE, the complete Green's function toolbox may be thrown at
\cref{eqn:ece2500report:40}, inverting Maxwell's equation for the electromagnetic field, given any charge and current density distribution
\begin{dmath}\label{eqn:ece2500report:60}
F(\Bx, t)
= \int dt' dV' G(\Bx, \Bx' ; t, t') J(\Bx', t').
\end{dmath}
Green's functions may also be applied to static and frequency domain field configurations.
Solving for, or working with, the combined field \( F \) shows the
hidden structure behind a number of seemingly
disparate ideas in electromagnetism.
This project explored a number of ideas along these lines.
For example, a
Green's function solutions for the static field configurations simultaneously yields Coulomb's and the Biot-Savart law.
Plane, circular and elliptical waves may be expressed compactly in a multivector form, naturally expressing the mutual perpendicularity of the electric field, magnetic field and the propagation directions, as well
as the relationships between the electric and magnetic field components.
The field energy density and Poynting vectors have a simple multivector form expressed in terms of \( F \) alone.
Calculations of radiation pressure can be performed using only the normal component of what is known as the energy momentum tensor in the conventional representation, which has a particularly compact multivector
representation.

Much of the geometric algebra literature for electrodynamics is presented with a relativistic bias, or assumes high levels of mathematical or physics sophistication.
The aim of this work was an attempt to make the study of electromagnetism using geometric algebra more accessible, especially to an electrical engineering audience.
In particular, this project explored non-relativistic applications of geometric algebra to electromagnetism.
The end product of this project was a fairly small self contained book, titled ``Geometric Algebra for Electrical Engineers''.
This book includes an introduction to Euclidean geometric algebra focused on \R{2} and \R{3} (64 pages), an introduction to geometric calculus and multivector Green's functions (64 pages), applications to electromagnetism (82 pages), and some appendices.
This report summarizes some results from this book, omitting most derivations, and attempts to provide an overview that may be used as a road map for the book for further exploration.
Many of the fundamental results of electromagnetism are derived directly from
\cref{eqn:ece2500report:40}, the multivector Maxwell's equation, in a streamlined and compact fashion.
This includes some new results, and many of the existing non-relativistic results from the geometric algebra literature.
As a
conceptual bridge, the book includes many examples of how to extract
familiar conventional results from simpler multivector representations.
Also included are some sample calculations exploiting unique capabilities that geometric algebra provides.  In particular, vectors in a plane may be manipulated much like complex numbers, which has a number of advantages over working with coordinates explicitly.

It is my belief that systematically working through all of the introductory and advanced topics in electromagnetism using geometric algebra
would provide significant insight, as well as a new set of powerful tools and procedures of practical value to the electrical engineer or physics practitioner.
The book produced in this project provides the prerequisite material for such exploration, and some first steps along the path of such an expedition.

\section{Chapter I: Geometric Algebra.}
\section{Chapter II: Geometric Calculus.}
\section{Chapter III: Electromagnetism.}
\subsection{Conventional Maxwell's equations.}
In the book it is presumed that the reader is familiar with Maxwell's equations, and that no attempt to motivate them is required.  We start with Maxwell's equations with the antenna theory extensions (fictious magnetic sources)
\begin{dmath}\label{eqn:ece2500report:2540}
\begin{aligned}
\spacegrad \cross \BE &= - \BM - \PD{t}{\BB} \\
\spacegrad \cross \BH &= \BJ + \PD{t}{\BD} \\
\spacegrad \cdot \BD &= \rho \\
\spacegrad \cdot \BB &= \rho_\txtm.
\end{aligned}
\end{dmath}
where \( \BE, \BH, \BD, \BB \) are the conventional electric and magnetic field intensities and flux densities,
\( \rho, \rho_\txtm \) are the electric and (fictitious-)magnetic charge densities, 
and \( \BJ, \BM \) are the electric and (fictitious-)magnetic current densities.
Much of the book presumes isotropic constitutive relationships between the electric and magnetic fields
\begin{dmath}
\label{eqn:freespace:300}
\begin{aligned}
\BB &= \mu \BH \\
\BD &= \epsilon \BE,
\end{aligned}
\end{dmath}
where \( \epsilon = \epsilon_r \epsilon_0 \) is the permittivity of the medium, and \( \mu = \mu_r \mu_0 \) is the permeability of the medium.
\subsection{Maxwell's equation.}
For isotropic media and constitutive relationships \cref{eqn:freespace:300} we may define
a multivector field that includes both electric and magnetic components
\makedefinition{Electromagnetic field strength.}{dfn:isotropicMaxwells:640}{
The \textit{electromagnetic field strength} ([\si{V/m}] (Volts/meter)) is defined as
\begin{equation*}
F = \BE + I \eta \BH \quad(= \BE + I c \BB),
\end{equation*}
where
\begin{itemize}
\item \( \eta = \sqrt{\mu/\epsilon} \) (\( [\Omega] \) Ohms), is the impedance of the media.
\item \( c = 1/\sqrt{\epsilon\mu} \) ([\si{m/s}] meters/second), is the group velocity of a wave in the media.  When \( \epsilon = \epsilon_0, \mu = \mu_0 \), \( c \) is the speed of light.
\end{itemize}
\( F \) is called the \textit{F}araday by some authors.
} % definition

The factors of \( \eta \) (or \( c \)) that multiply the magnetic fields are for dimensional consistency, since \( [\sqrt{\epsilon} \BE] = [\sqrt{\mu} \BH] = [\BB/\sqrt{\mu}]\).
The justification for imposing a dual (or complex) structure on the electromagnetic field strength can be found in the historical development of
Maxwell's equations, but we will also see such a structure arise naturally in short order.

No information is lost by imposing the complex structure of
\cref{dfn:isotropicMaxwells:640}, since we can always obtain the
electric field vector \( \BE \) and the magnetic field bivector \( I \BH \) by grade selection
from the electromagnetic field strength when desired
\begin{dmath}\label{eqn:isotropicMaxwells:620}
\begin{aligned}
\BE &= \gpgradeone{ F } \\
I \BH &= \inv{\eta} \gpgradetwo{ F }.
\end{aligned}
\end{dmath}

We will also
define a multivector current containing all charge densites and current densities
\makedefinition{Multivector current.}{dfn:isotropicMaxwells:660}{
The \textit{current} ([\si{A/m^2}] (Amperes/square meter)) is defined as
\begin{equation*}
J = \eta \lr{ c \rho - \BJ } + I \lr{ c \rho_\txtm - \BM }.
\end{equation*}
} % definition
When the fictitious magnetic source terms \((\rho_\txtm, \BM)\) are included, the current has one grade for each possible source (scalar, vector, bivector, trivector).  With only conventional electric sources, the current is still a multivector, but contains only scalar and vector grades.

Given the multivector field and current, it is now possible to state Maxwell's equation (singular) in its geometric algebra form
\maketheorem{Maxwell's equation.}{dfn:isotropicMaxwells:680}{
Maxwell's equation is a multivector equation relating the change in the electromagnetic field strength to charge and current densities and is written as
\begin{equation*}
\stgrad F = J.
\end{equation*}
} % theorem
Maxwell's equation in this form will be the starting place for all the subsequent analysis in this book.
As mentioned in \cref{chap:GreensFunctions}, the operator \( \spacegrad + (1/c) \partial_t \) will be called the \textit{spacetime gradient}\footnote{This form of spacetime gradient is given a special symbol by a number of authors, but there is no general agreement on what to use.
Instead of entering the fight, it will be written it out in full in this book.}.

See the book for a proof of \cref{dfn:isotropicMaxwells:680}.  The workhorse of the proof is the identity \( \spacegrad \Bb = \spacegrad \cdot \Bb + I \spacegrad \cross \Bb \) which allows Maxwell's equations into two gradient equations, one for each of \( \spacegrad \BE \), and \( \spacegrad \BH \), which can then be further grouped to complete the proof.
There is a lot of information packed into this single equation.  Expanded in full it is
\begin{dmath}\label{eqn:isotropicMaxwells:580}
\stgrad \lr{ \BE + I \eta \BH } = \eta\lr{ c \rho - \BJ } + I \lr{ c \rho_\txtm - \BM }.
\end{dmath}

\subsection{Wave equation and continuity.}
Some would argue that the conventional form \cref{eqn:freespace:3100} of Maxwell's equations have built in redundancy since continuity equations on the charge and current densities couple some of these equations.
We will take an opposing view, and show that such continuity equations are neccessary consequences of Maxwell's equation in its wave equation form, and derive those conditions.
This amounts to a statement that the multivector current \( J \) is not completely unconstrained.

\maketheorem{Electromagnetic wave equation and continuity conditions.}{thm:continuity:600}{
The electromagnetic field is a solution to the non-homogeneous wave equation
\begin{equation*}
%\lr{ \spacegrad^2 - \inv{c^2} \PDSq{t}{} }
\dLambertian
F =
\conjstgrad J.
\end{equation*}
In source free conditions, this reduces to a homogeneous wave equation, with group velocity \( c \), the speed of the wave in the media.
When expanded explicitly in terms of electric and magnetic fields, and charge and current densities, this single equation resolves to a
non-homogeneous wave equation for each of the electric and magnetic fields
\begin{equation*}
\begin{aligned}
%\lr{ \spacegrad^2 - \inv{c^2} \PDSq{t}{} }
\dLambertian
\BE
%&= \gpgrade{\conjstgrad J}{1}
&= \inv{\epsilon} \spacegrad \rho + \mu \PD{t}{\BJ} + \spacegrad \cross \BM \\
%\lr{ \spacegrad^2 - \inv{c^2} \PDSq{t}{} }
\dLambertian
\BH
%&= \inv{I \eta} \gpgrade{\conjstgrad J}{2}
&= \inv{\mu} \spacegrad \rho_\txtm + \epsilon \PD{t}{\BM} - \spacegrad \cross \BJ,
\end{aligned}
\end{equation*}
as well as a pair of continuity equations coupling the respective charge and current densities
\begin{equation*}
\begin{aligned}
\spacegrad \cdot \BJ + \PD{t}{\rho} &= 0 \\
\spacegrad \cdot \BM + \PD{t}{\rho_\txtm} &= 0.
\end{aligned}
\end{equation*}
} % theorem

The proof is in the book, but basically just requires operating on Maxwell's equation with \( \conjstgrad \), which yields two equations
equations, one for grades 1,2 and one for grades 0,3
\begin{dmath}\label{eqn:continuity:130}
\begin{aligned}
%\lr{ \spacegrad^2 - \inv{c^2} \PDSq{t}{} }
\dLambertian
F &= \gpgrade{ \conjstgrad J }{1,2} \\
                                           0 &= \gpgrade{ \conjstgrad J }{0,3}.
\end{aligned}
\end{dmath}
The grade 0,3 selection of \cref{eqn:continuity:130} provides the continuity equations.
\subsection{Plane waves.}
With all sources zero,
the free space Maxwell's equation as given by \cref{dfn:isotropicMaxwells:680} for the
electromagnetic field strength reduces to just
\begin{dmath}\label{eqn:planewavesMultivector:300}
\stgrad F(\Bx, t) = 0.
\end{dmath}

Utilizing a phasor representation of the form \cref{dfn:greensFunctionOverview:300},
we will define the
phasor representation of the field as
\makedefinition{Plane wave.}{dfn:planewavesMultivector:680}{
We represent the
electromagnetic field strength
plane wave solution of Maxwell's equation in phasor form as
\begin{equation*}
F(\Bx, t) = \Real \lr{ F(\Bk) e^{ j \omega t }  },
\end{equation*}
where the complex valued multivector \( F(\Bk) \) also has a presumed exponential dependence
\begin{equation*}
F(\Bk)
=
\tilde{F}
e^{ -j \Bk \cdot \Bx }.
\end{equation*}
} % definition

In the book, we show that solutions of the electromagnetic field wave equation have the form
\maketheorem{Plane wave solutions to Maxwell's equation.}{thm:planewavesMultivector:620}{
Single frequency plane wave solutions of Maxwell's equation have the form
\begin{equation*}
F(\Bx, t)
=
\Real \lr{
\lr{ 1 + \kcap }
\kcap \wedge \BE\,
e^{-j \Bk \cdot \Bx + j \omega t}
}
,
\end{equation*}
where \( \Norm{\Bk} = \omega/c \), \( \kcap = \Bk/\Norm{\Bk} \) is the unit vector pointing along the propagation direction, and \( \BE \) is any complex-valued vector variable.  When a \( \BE \cdot \Bk = 0 \) constraint is imposed on the vector variable \( \BE \), that variable can be interpreted as the electric field, and the solution reduces to
\begin{equation*}
F(\Bx, t)
=
\Real \lr{
\lr{ 1 + \kcap }
\BE\,
e^{-j \Bk \cdot \Bx + j \omega t}
}
,
\end{equation*}
showing that the field phasor \( F(\Bk) = \BE(\Bk) + I \eta \BH(\Bk) \) splits naturally into electric and magnetic components
\begin{equation*}
\begin{aligned}
\BE(\Bk) &= \BE e^{-j \Bk \cdot \Bx} \\
\eta \BH(\Bk) &= \kcap \cross \BE \, e^{-j \Bk \cdot \Bx},
\end{aligned}
\end{equation*}
where the directions \( \kcap, \BE, \BH \) form a right handed triple.
} % theorem

A full proof and discussion is in the book.  The key step is that after insertion of the presumed phasor relationship, we find that
\begin{dmath}\label{eqn:planewavesMultivector:60}
0
=
-j \lr{ \Bk - \frac{\omega}{c} } F(\Bk),
\end{dmath}
which can be satisfied by insisting that \( F \) has a \( \Bk + \omega/c \) factor and that \( \Norm{\Bk} = \omega/c \).  The observation that
\( \kcap, \BE, \BH \) form a right handed triple, is expressed geometrically by \( I = \kcap \Ecap \Hcap \), from which we can also find \( \kcap = \Ecap \cross \Hcap \).

\subsection{Statics solution.}
If we restrict attention to time invariant fields (\( \partial_t F = 0\)) and time invariant sources (\(\partial_t J = 0\)),
Maxwell's equation is reduced to an invertible first order gradient equation
\begin{dmath}\label{eqn:statics:20}
\spacegrad F(\Bx) = J(\Bx),
\end{dmath}

\maketheorem{Maxwell's statics solution.}{thm:statics:100}{
The solution to the Maxwell statics equation is given by
\begin{equation*}
F(\Bx)
= \inv{4\pi} \int_V dV' \frac{\gpgrade{(\Bx - \Bx') J(\Bx')}{1,2}}{\Norm{\Bx - \Bx'}^3} + F_0,
\end{equation*}
where \( F_0 \) is any function for which \( \spacegrad F_0 = 0 \).
The explicit expansion in electric and magnetic fields and charge and current densities is given by
\begin{equation*}
\begin{aligned}
\BE(\Bx)
&=
\inv{4\pi} \int_V dV' \inv{\Norm{\Bx - \Bx'}^3}
\lr{
{\color{DarkOliveGreen}
   \inv{\epsilon}(\Bx - \Bx') \rho(\Bx')
}
   +
   (\Bx - \Bx') \cross \BM(\Bx')
} \\
\BH(\Bx)
&=
\inv{4\pi} \int_V dV' \inv{\Norm{\Bx - \Bx'}^3}
\lr{
{\color{Maroon}
  \BJ(\Bx') \cross (\Bx - \Bx')
}
+ \inv{\mu} (\Bx - \Bx') \rho_m(\Bx')
}.
\end{aligned}
\end{equation*}
} % theorem

We see that the solution incorporates both a {\color{DarkOliveGreen}Coulomb's law} contribution and a {\color{Maroon}Biot-Savart law} contribution, as well as their magnetic source analogues if applicable.

The proof is essentially a convolution with the (vector valued) Green's function for the (first order) gradient \cref{eqn:greensFunctionFirstOrderHelmholtz:900}.
\subsection{Statics: Enclosed charge.}
In conventional electrostatics we obtain a relation between the normal electric field component and the enclosed charge by integrating the electric field divergence.
The geometric algebra generalization of this relates the product of the normal and the electromagnetic field strength related to the enclosed multivector current
\maketheorem{Enclosed multivector current.}{thm:enclosedCurrent:60}{
The total multivector current in the volume is related to the surface integral of \( \ncap F \) over the boundary of the volume by
\begin{equation*}
\int_{\partial V} dA \ncap F = \int_V dV J.
\end{equation*}
This is a multivector equation, carrying inforamation for each grade in the multivector current, and after explicit expansion is equivalent to
\begin{equation*}
\begin{aligned}
\int_{\partial V} dA\, \ncap \cdot \BE        &=  \inv{\epsilon} \int_V dV\, \rho \\
\int_{\partial V} dA\, \ncap \cross \BH       &=                 \int_V dV\, \BJ \\
\int_{\partial V} dA\, \ncap \cross \BE       &=               - \int_V dV\, \BM \\
\int_{\partial V} dA\, \ncap \cdot \BH        &=  \inv{\mu} \int_V dV\, \rho_\txtm.
\end{aligned}
\end{equation*}
} % theorem

To prove \cref{thm:enclosedCurrent:60} one evaluate the volume integral of the gradient of the field using \cref{thm:volumeintegral:100}.  The rest of the proof follows by grade selection.  For full details on both see the book.
\subsection{Statics: Enclosed current.}
Ampere's law may be generalized to line integrals of the total electromagnetic field strength.
\maketheorem{Line integral of the field.}{thm:amperes:280}{
The line integral of the electromagnetic field strength is
\begin{equation*}
\ointclockwise_{\partial A} d\Bx\, F
=
I \int_A dA \lr{ \ncap J - \PD{n}{F} },
\end{equation*}
where \( \PDi{n}{F} = \lr{ \ncap \cdot \spacegrad } F \).
Expressed in terms of the conventional consistent fields and sources, this multivector relationship expands to four equations, one for each grade
\begin{equation*}
\begin{aligned}
\ointclockwise_{\partial A} d\Bx \cdot \BE &=  \int_A dA\, \ncap \cdot \BM \\
\ointclockwise_{\partial A} d\Bx \cross \BH
&=
\int_A dA
\lr{
   - \ncap \cross \BJ
   + \frac{ \ncap \rho_\txtm }{\mu}
   - \PD{n}{\BH}
} \\
\ointclockwise_{\partial A} d\Bx \cross \BE &=
\int_A dA
\lr{
     \ncap \cross \BM
   + \frac{\ncap \rho}{\epsilon}
   - \PD{n}{\BE}
} \\
\ointclockwise_{\partial A} d\Bx \cdot \BH &= -\int_A dA\, \ncap \cdot \BJ.
\end{aligned}
\end{equation*}
} % theorem
The last of the scalar equations in
\cref{thm:amperes:280}
is Ampere's law
\begin{equation}\label{eqn:amperes:20}
\ointctrclockwise_{\partial A} d\Bx \cdot \BH = \int_A \ncap \cdot \BJ = I_{\textrm{enc}},
\end{equation}
and the first is the dual of Ampere's law for (fictious) magnetic current density\footnote{Even without the fictitious magnetic sources, neither the name nor applications of the two cross product line integrals with the normal derivatives are familiar to the author.}.
In \cref{eqn:amperes:20} the flux of the electric current density equals the enclosed current flowing through an open surface.  This enclosed current equals the line integral of the magnetic field around the boundary of that surface.

The proof and additional details can be found in the book.

It is worth pointing out that for pure magnetostatics problems where \( J = \eta \BJ, F = I \eta \BH \), that Ampere's law can be written in a trivector form
\begin{equation}\label{eqn:amperes:260}
\ointclockwise_{\partial A} d\Bx \wedge F = I \int_A dA\, \ncap \cdot J = I \eta \int_A dA\, \ncap \cdot \BJ.
\end{equation}
This encodes the fact that the magnetic field component of the total electromagnetic field strength is most naturally expressed in
geometric algebra as a bivector.

\subsection{Statics: Example field calculations.}
A number of worked examples were calculated to illustrate geometric algebra techniques.
\begin{itemize}
\item A finite line charge with line charge density \( \lambda \).  With \( i = \Be_1 \Be_3 \), the (electric) field was found to be \(
F = \ifrac{\lambda \Be_1}{(4 \pi \epsilon r)} \int_{a/r}^{b/r} du \lr{ e^{i\theta} - u }\lr{ 1 + u^2 - 2 u \cos\theta }^{-3/2} \).  The scalar portion of the integral is strictly a scale factor for the component of the field that lies along the x-axis, whereas the ``complex exponential'' factor of the integrand represents a rotational term along the direction \( \Be_1 e^{i\theta} = \Be_1 \cos\theta + \Be_3 \sin\theta \).  In problems like this, with only two degrees of freedom, there will often be a complex like representation possible using geometric algebra.
%This is illustrated in \cref{fig:linecharge:linechargeFig1}.
%\imageFigure{../figures/GAelectrodynamics/linechargeFig1}{Line charge density.}{fig:linecharge:linechargeFig1}{0.3}
\item The field for infinite static charge and current densities lying along the z-axis \( \rho(\Bx) = \lambda \delta(x) \delta(y), \BJ(\Bx) = \Bv \rho(\Bx) \) was found to be \( F = \ifrac{\lambda}{(2\pi \epsilon R)} \rhocap \lr{ 1 - \Bv/c}\).  The field splits naturally into electric (vector) and magnetic (bivector) grades as \( F = \BE \lr{ 1 - \Bv/c } = \BE + I \lr{ \ifrac{\Bv}{c} \cross \BE } \).
\item As a problem, the reader was asked to compute the field for
the magnetic charge density \( \rho_m = \lambda_m \delta(x) \delta(y) \), and current density \( \BM = v \Be_3 \rho_m = \Bv \rho_m \).  That field is
\( F = \ifrac{\lambda_m c}{(4 \pi R)} I \rhocap \lr{ 1 - \ifrac{\Bv}{c} } \), which may be split into electric and magnetic components as
\( F = \BB \cross \Bv + c I \BB \), where \( \BB = \lambda_m \rhocap/(4 \pi R) \).
\item The field for a uniform infinite planar charge density \( \rho(\Bx) = \sigma \delta(z) \) and associated current density \( \BJ(\Bx) = \Bv \rho(\Bx) \), where \( \Bv = v \Be_1 e^{i\theta}, \quad i = \Be_{12} \) was found to be
\( F = \ifrac{\sigma \sgn(z)}{(4 \pi \epsilon)} \Be_3 \lr{ 1 - \frac{\Bv}{c}} \).  As should be expected by superposition, the field splits neatly into electric field (vector) and magnetic field (bivector) components associated with the respective pure electrostatic and magnetostatics problems.
\item As a problem the reader is asked to show that the field for an infinite planar
magnetic charge density \( \rho_m = \sigma_m \delta(z) \), and current density \( \BM = \Bv \rho_m, \Bv = v \Be_1 e^{i\theta}, i = \Be_{12}\) is \( F = \ifrac{\sigma_m c \sgn(z)}{(4 \pi)} i \lr{ 1 - \ifrac{\Bv}{c} } \).
\item The field for a line charge density \( \lambda \) along a circular arc segment \( \phi' \in [a,b] \), of radius \( r \) in the x-y plane was found to be \( F = \ifrac{\lambda r}{(4 \pi \epsilon_0 R^2)} \int_{a-\phi}^{b-\phi} d\alpha \lr{ \rcap + \phicap u i e^{i \alpha } } \lr{ 1 + u^2 - 2 u \sin\theta \cos \alpha }^{-3/2} \), where \( i = \Be_{12} \).  This problem is often given as an example or problem in electrostatics, but usually for circular charge distribution, and an observation point on the z-axis where symmetries kill off all but the z-axis component of the field.  The freedom to represent rotational terms as complex exponentials in geometric algebra allows the more general problem to be calculated without much additional difficulty.  The resulting integrals can be evaluated easily with any existing numerical integration software as the vector factors \( \rcap, \phicap \) may be pulled out of the integrals, leaving strict scalar or complex valued integrands.
%\imageFigure{../figures/GAelectrodynamics/lineChargeArcFig1}{Circular line charge.}{fig:circularlinecharge:circularlinechargeFig1}{0.5}
\item To illustrate the algebraic flexibility available, the circular ring charge problem is tackled in cylindrical coordinates instead of spherical (as previous).  For a static charge line density \( \lambda \) on a ring at \( z = 0 \), and an azimuthal current density \( \BJ = \Bv \rho \), we find a closed form solution for the field is found \( F = \ifrac{\lambda}{(4 \pi \epsilon R)}
\lr{
\lr{ \tilde{z} \Be_3 + \tilde{r} \rhocap -\ifrac{v i}{c} } A
- \lr{
\rhocap + \ifrac{v}{c} \lr{ \tilde{z} \Be_{3} \rhocap + \tilde{r} i } } B
} \), where \( \tilde{z} = z/R, \tilde{r} = r/z \).
The symmetry of the ring configuration allows for a closed form solution (numerical integration not required) of the field, but comes with the cost of requiring
elliptic integrals \( A, B \), which are detailed in the book along with the derivation and plots of the resulting fields.
%\imageFigure{../figures/GAelectrodynamics/chargeAndCurrentOnRingFig1}{Field due to a circular distribution.}{fig:chargeAndCurrentOnRing:chargeAndCurrentOnRingFig1}{0.3}
%}
\item The final worked statics problem in the book is a use of Ampere's law, to compute the magnetic field for a
pair of z-axis oriented electric currents of magnitude \( I_1, I_2 \) flowing through the \( z = 0 \) points \( \Bp_1, \Bp_2 \) on the x-y plane.  The geometry and derivation is detailed in the book, but we use the multivector line integral form of Ampere's law \( \ointctrclockwise_{\partial A} d\Bx F = -I \int_A dA \Be_3 (-\eta \BJ) = I \eta I_\txte \), and superposition to compute the field
\( F = \sum_{k = 1,2} \ifrac{\eta I_k}{(2 \pi)} \ifrac{1}{(\Be_3 \wedge \lr{ \Br - \Bp_k})} \).
The bivector (magnetic) nature of a field with only electric current density sources is naturally represented by the wedge product \( \Be_3 \wedge \lr{ \Br - \Bp_k} \) which is a vector product of \( \Be_3 \) and the projection of \( \Br - \Bp_k \) onto the x-y plane.
%\cref{fig:amperesLawBetweenTwoCurrents:amperesLawBetweenTwoCurrentsFig1}.
%\imageFigure{../figures/GAelectrodynamics/amperesLawBetweenTwoCurrentsFig1}{Magnetic field between two current sources.}{fig:amperesLawBetweenTwoCurrents:amperesLawBetweenTwoCurrentsFig1}{0.3}
\end{itemize}
\subsection{Dynamics.}
Maxwell's equation (\cref{dfn:isotropicMaxwells:680}) is invertable, with solution.
\maketheorem{Jefimenkos solution.}{thm:jefimenkosEquations:120}{
The solution of Maxwell's equation is given by
\begin{equation*}
F(\Bx, t)
=
F_0(\Bx, t)
+
\inv{4 \pi}
\int dV'
\lr{
   \frac{\rcap}{r^2} J(\Bx', t_r)
   +
   \inv{c r} \lr{ 1 + \rcap } \dispdot{J}(\Bx', t_r)
},
\end{equation*}
where \( F_0(\Bx, t) \) is any specific solution of the homogenoous equation \( \lr{ \spacegrad + (1/c) \partial_t } F_0 = 0 \),
time derivatives are denoted by overdots, and all times are evaluated at the retarded time \( t_r = t - r/c \).
When expanded in terms of the electric and magnetic fields (ignoring magnetic sources), the non-homoegenous portion of this solution is known as the
Jefimenkos' equations
\begin{dmath}\label{eqn:jefimenkosEquations:100}
\begin{aligned}
\BE &=
\inv{4 \pi}
\int dV'
\lr{
\frac{\rcap}{\epsilon r} \lr{
\frac{\rho(\Bx', t_r)}{r} + \frac{\dispdot{\rho}(\Bx', t_r) }{c} }
   - \frac{\eta }{ c r } \dotBJ(\Bx', t_r)
} \\
\BH &=
\inv{4 \pi}
\int dV'
\lr{
   \frac{1}{c r} \dotBJ(\Bx', t_r)
+
   \frac{1}{r^2} \BJ(\Bx', t_r)
} \cross \rcap,
\end{aligned}
\end{dmath}
%which checks against Griffiths.
} % theorem
This is found fairly easily using the Green's function for the spacetime gradient \cref{thm:greensFunctionSpacetimeGradient:120}, and the details can be found in the book.  Unlike the conventional approach (\citep{griffiths1999introduction}), we are able to find the field directly without first having to determine the retarded time potentials, nor having to take their derivatives.

\subsection{Energy and momentum.}
In the energy and momentum section of the book, the geometric algebra formulation of the field energy density, Poynting vector, Maxwell stress tensor, and more generally, the energy momentum tensor are fully detailed.
\makedefinition{(Conventional) Energy and momentum density and Poynting vector.}{dfn:poyntingF:1220}{
The quantities \( \calE \) and \( \bcP \) defined as
%\label{eqn:poyntingF:20}
\begin{equation*}
\begin{aligned}
\calE &
%=
%\inv{2} \lr{ \BD \cdot \BE + \BB \cdot \BH }
= \inv{2} \lr{ \epsilon \BE^2 + \mu \BH^2 } \\
\bcP c &= \inv{c} \BE \cross \BH,
\end{aligned}
\end{equation*}
are known respectively as the field energy and momentum densities.
\( \BS = c^2 \bcP = \BE \cross \BH \) is called the Poynting vector.
} % definition
%\makedefinition{(Conventional) energy momentum and Maxwell stress tensors.}{dfn:poyntingF:1180}{
%The rank-2 symmetric tensor \( \Theta^{\mu\nu} \), with components
%%\label{eqn:poyntingF:840}
%\begin{equation*}
%\begin{aligned}
%\Theta^{00} &= \frac{\epsilon}{2} \lr{ \BE^2 + \eta^2 \BH^2 } \\
%\Theta^{0i} &= \inv{c} \lr{ \BE \cross \BH } \cdot \Be_i \\
%\Theta^{ij} &= -\epsilon \lr{ E_i E_j + \eta^2 H_i H_j - \inv{2} \delta_{ij} \lr{ \BE^2 + \eta^2 \BH^2 } },
%\end{aligned}
%\end{equation*}
%is called the energy momentum tensor.
%The spatial index subset of this tensor is known as the Maxwell stress tensor, and is often
%represented in dyadic notation
%\begin{equation*}
%\lr{ \Ba \cdot \lrT } \cdot \Bb
%=
%\sum_{i,j} a_i T_{ij} b_j,
%\end{equation*}
%or
%\begin{equation*}
%\Ba \cdot \lrT \equiv \sum_{i,j} a_i T_{ij} \Be_j
%\end{equation*}
%where \( T_{ij} = -\Theta^{ij} \).
%} % definition
%Here we use the usual convention of Greek indices such as \( \mu,\nu \) for ranging over both time (0) and spatial \( \setlr{1,2,3} \) indexes, and
%and Latin letters such as \( i, j \)
%for the ``spatial'' indexes
%\( \setlr{1,2,3} \).
%The names and notation for the tensors vary considerably\footnote{\( \Theta^{\mu\nu} \) in
%\cref{dfn:poyntingF:1180}
%is called the symmetric
%stress tensor by some authors \citep{jackson1975cew},
%and the energy momentum tensor by others, and is sometimes written \( T^{\mu\nu} \) (\citep{landau1980classical}, \citep{doran2003gap}).
%The sign conventions and notation for the spatial components \( \Theta^{ij} \) vary as well, but all authors appear to call this subset the Maxwell stress tensor.
%The Maxwell stress tensor may be written as \( \sigma_{ij} (=-\Theta^{ij}) \) \citep{landau1980classical}, or as
%\( T_{ij} (=-\Theta^{ij}) \)
%(\citep{griffiths1999introduction}, \citep{jackson1975cew}.)
%}.
In geometric algebra the energy momentum tensor, and the Maxwell stress tensor may be represented as linear grade 0,1 multivector valued functions of a grade 0,1 multivector.
\makedefinition{Energy momentum and Maxwell stress tensors.}{dfn:poyntingF:1200}{
We define the \textit{energy momentum tensor} as
\begin{equation*}
T(a) = \inv{2} \epsilon F a F^\dagger,
\end{equation*}
where \( a \) is a 0,1 multivector parameter.
We introduce a shorthand notation for grade one selection with vector valued parameters
\begin{equation*}
\BT(\Ba) = \gpgradeone{T(\Ba)},
\end{equation*}
and call this the \textit{Maxwell stress tensor}.
} % definition

\maketheorem{Expansion of the energy momentum tensor.}{thm:poyntingF:1240}{
Given a scalar parameter \( \alpha \), and a vector parameter \( \Ba = \sum_k a_k \Be_k \), the energy momentum tensor of
\cref{dfn:poyntingF:1200} is a grade 0,1 multivector, and may be expanded in terms of \( \calE, \BS \) and \( \BT(\Ba) \) as
%\label{eqn:poyntingF:1120}
\begin{equation*}
T(\alpha + \Ba)
=
\alpha \lr{
   \calE + \frac{\BS}{c}
}
-
\Ba \cdot \frac{\BS}{c}
+ \BT(\Ba),
\end{equation*}
where \( \BT(\Be_i) \cdot \Be_j = -\Theta^{ij} \), or \( \BT(\Ba) = \Ba \cdot \lrT \).
} % theorem

\Cref{thm:poyntingF:1240} relates the geometric algebra definition of the energy momentum tensor to the quantities found in the conventional
electromagnetism literature.  In the book, the conventional indexed representation is detailed more completely for comparision purposes.

Associated with the energy momentum tensor are a number of conservation relationships, which are most compactly stated utilizing the adjoint of the energy momenutum tensor.
\makedefinition{Adjoint.}{dfn:poyntingTheorem:1120}{
The adjoint \( \overbar{A}(x) \) of a linear operator \( A(x) \) is defined implicitly by the scalar selection
\begin{equation*}
\gpgradezero{ y \overbar{A}(x) } =
\gpgradezero{ x A(y) }.
\end{equation*}
} % definition
\maketheorem{Poynting's theorem (differential form.)}{thm:poyntingTheorem:1180}{
The adjoint energy momentum tensor of the spacetime gradient satisfies the following multivector equation
\begin{equation*}
\overbar{T}(\spacegrad + (1/c)\partial_t) = \frac{\epsilon}{2} \lr{ F^\dagger J + J^\dagger F }.
\end{equation*}
Note that the multivector \( F^\dagger J + J^\dagger F \) can only have scalar and vector grades, since it equals its reverse.
This equation can be put into a form that is more obviously a conservation law by stating it as a set of
scalar grade identities
\begin{equation*}
\spacegrad \cdot \gpgradeone{ T(a) } + \inv{c} \PD{t}{} \gpgradezero{ T(a) }
=
\frac{\epsilon}{2} \gpgradezero{ a( F^\dagger J + J \dagger F) }.
\end{equation*}
These may be written as respective
scalar and vector grades equations
%%which expands to the multivector equation
%\begin{equation*}
%\inv{c} \PD{t}{} \lr{ \calE - \frac{\BS}{c} }
%+ \spacegrad \cdot \frac{\BS}{c}
%+ \BT(\spacegrad)
%=
%-\inv{c} \lr{ \BE \cdot \BJ + \BH \cdot \BM }
%+
%\rho \BE + \epsilon \BE \cross \BM
%+
%\rho_\txtm \BH + \mu \BJ \cross \BH,
%\end{equation*}
%or as separate scalar and vector equations
\begin{equation*}
\begin{aligned}
\inv{c} \PD{t}{\calE} + \spacegrad \cdot \frac{\BS}{c} &= -\inv{c} \lr{ \BE \cdot \BJ + \BH \cdot \BM } \\
-\inv{c^2} \PD{t}{\BS} + \BT(\spacegrad) &= \rho \BE + \epsilon \BE \cross \BM + \rho_\txtm \BH + \mu \BJ \cross \BH.
\end{aligned}
\end{equation*}
Conventionally, only the scalar grade relating the time rate of change of the energy density to the flux of the Poynting vector, is called Poynting's theorem.
} % theorem
or in an integral form
\maketheorem{Poynting's theorem (integral form.)}{thm:poyntingTheoremRewrite:1420}{
\begin{dmath}\label{eqn:poyntingTheoremRewrite:1400}
\begin{aligned}
&\PD{t}{}
\int_V \calE dV
=
-\int_{\partial V} dA \ncap \cdot \BS
-
\int_V dV \lr{
   \BJ \cdot \BE
   +
   \BM \cdot \BH
} \\
&
\int_V dV \lr{ \rho \BE + \BJ \cross \BB }
+ \int_V dV \lr{ \rho_\txtm \BH - \epsilon \BM \cross \BE }
=
-
\PD{t}{ }
\int_V dV \bcP
+
\int_{\partial V} dA \BT(\ncap).
\end{aligned}
\end{dmath}
} % theorem

As the field in the volume is carrying the (electromagnetic) momentum \( \Bp_{\textrm{em}} = \int_V dV \bcP \), we can identify the sum of the Maxwell stress tensor's normal component over the bounding integral as time rate of change of the mechanical and electromagnetic momentum
\begin{equation}\label{eqn:ece2500report:2560}
\frac{d\Bp_{\textrm{mech}}}{dt} + \frac{d\Bp_{\textrm{em}}}{dt} = \int_{\partial V} dA \BT(\ncap).
\end{equation}
The rate of change of mechanical momentum density \( \ifrac{d\Bp_{\textrm{mech}}}{dt} \) is, in fact, the continuous equivalent of the Lorentz force, which is found to be a direct consequence of conservation relationships associated with Maxwell's equation.

There were many details left out above.  Please refer to the text for more information.

\subsubsection{Example energy momentum calculations.}
To illustrate the ideas above, the energy momentum tensor components for all of the static fields computed previously are determined.  For brievity, these are omitted from this report, but it should be noted that we see that geometric algebra allows for a particularly compact coordinate free representation of the energy momentum tensor components.
\subsubsection{Complex power.}
The geometric algebra forms of the \( T(1) \) (field energy density and Poynting vector) are found to be
\maketheorem{Complex power representation.}{thm:poyntingFComplexPower:300}{
Given a time domain representation of a phasor based field \( F = F(\omega) \)
\begin{equation*}
F(t)
= \Real\lr{ F e^{j \omega t} },
\end{equation*}
the energy momentum tensor multivector \( T(1) \) has the representation
\begin{equation*}
T(1) = \calE + \frac{\BS}{c}
=
\frac{\epsilon}{4} \Real \lr{ F^\conj F^\dagger + F F^\dagger e^{2 j \omega t} }.
\end{equation*}
With the usual definition of the complex Poynting vector
%\label{eqn:poyntingFComplexPower:240}
\begin{equation*}
\calS = \inv{2} \BE \cross \BH^\conj = \inv{2} \lr{ I \BH^\conj } \cdot \BE,
\end{equation*}
the energy and momentum components of \( T(1) \), for real \( \mu, \epsilon \) are
%\label{eqn:poyntingFComplexPower:260}
\begin{equation*}
\begin{aligned}
\calE &=
\inv{4} \lr{
\epsilon \Abs{\BE}^2 + \mu \Abs{\BH}^2 }
+
\inv{4} \Real
\lr{
   \lr{ \epsilon \BE^2 + \mu \BH^2}
   e^{2 j \omega t }
} \\
\BS &= \Real \calS
+
\inv{2} \Real
\lr{
\lr{ \BE \cross \BH }
   e^{2 j \omega t }
}.
\end{aligned}
\end{equation*}
} % theorem

\subsection{Lorentz force.}
The Lorentz force equation can be stated in terms of the total electromagnetic field strength and chacurrent density
\maketheorem{Lorentz force and power.}{thm:lorentzForce:300}{
Given an energy momentum multivector \( T = \calE + c \Bp \), and a charge associated with a small bounded multivector current density \( Q = \int_V J dV \),
the respective power and force experienced by particles with electric (and/or magnetic) charges is described by
\cref{dfn:lorentzForce:280} is
\begin{equation*}
\inv{c} \frac{dT}{dt} = \gpgrade{ F Q^\dagger }{0,1} = \inv{2} \lr{ F^\dagger Q + F Q^\dagger }.
\end{equation*}
where \( \gpgradezero{dT/dt} = \ifrac{d\calE}{dt} \) is the power and \( \gpgradeone{dT/dt} = c \ifrac{d\Bp}{dt} \) is the force on the particle, and
\( Q^\dagger \) is the electric or magnetic charge/velocity multivector of \cref{dfn:lorentzForce:280}.
The conventional representation of the Lorentz force/power equations
\begin{equation*}
\begin{aligned}
\gpgradeone{ F Q^\dagger } &= \ddt{\Bp} = q \lr{ \BE + \Bv \cross \BB } \\
c \gpgradezero{ F Q^\dagger } &= \ddt{\calE} = q \BE \cdot \Bv.
\end{aligned}
\end{equation*}
%given by \cref{eqn:freespace:180}
may be recovered by grade selection operations.
For magnetic particles, such a grade selection gives
\begin{equation*}
\begin{aligned}
\gpgradeone{ F Q^\dagger } &= \frac{d\Bp}{dt} = q_\txtm \lr{ c \BB - \inv{c} \Bv \cross \BE } \\
c \gpgradezero{ F Q^\dagger } &= \frac{d\calE}{dt} = \inv{\eta} q_\txtm \BB \cdot \frac{\Bv}{c}.
\end{aligned}
\end{equation*}
} % theorem

\subsubsection{Constant magnetic field.}
As another example of geometric algebra in action, the Lorentz force equation for a constant external magnetic field bivector \( F = I c \BB \)
\begin{dmath}\label{eqn:lorentzForce_constantMagnetic:60}
m \frac{d\Bv}{dt} = q F \cdot \frac{\Bv}{c},
\end{dmath}
is solved in a fashion unique to this algebra.  With
\( \Omega = -\ifrac{q F}{m c} \), the Lorentz force equation is reduced to \( \ifrac{d\Bv}{dt} = \Bv \cdot \Omega \), which may be solved
using a multivector integration factor.  The solution is shown to be
\begin{dmath}\label{eqn:lorentzForce_constantMagnetic:200}
\Bv(t) = e^{-\Omega t/2} \Bv(0) e^{\Omega t/2}.
\end{dmath}
Any component of the initial velocity \( \Bv(0)_\perp \) perpendicular to the \( \Omega \) plane is untouched by this rotation operation, whereas components of the initial velocity \( \Bv(0)_\parallel \) that lie in the \( \Omega \) plane will trace out a circular path, so the velocity of the charged particle traces out a helical path.

A multivector integration factor method for solving the Lorentz force equation in constant mixed external electric and magnetic fields can be found in \citep{hestenes1999nfc}.  More general examples are considered in the literature cited in the book.

\subsection{Polarization}
Following the usual convention, the geometric algebra treatment of polarization in the book
aligns the propagation direction along the z-axis.
The field is
\begin{dmath}\label{eqn:polarization:20}
\begin{aligned}
F(\Bx, \omega) &= (1 + \Be_3) \BE e^{-j \beta z} \\
F(\Bx, t) &= \Real\lr{ F(\Bx, \omega) e^{j \omega t} },
\end{aligned}
\end{dmath}
where \( \BE \cdot \Be_3 = 0 \).
Here the imaginary \( j \) has no intrinsic geometrical interpretation, but we are able to dispense with it and use geometric imaginaries instead.  This is done by first assuming the electric field is given by \( \BE = \lr{ \alpha_1 + j \beta_1 } \Be_1 + \lr{ \alpha_2 + j \beta_2 } \Be_2 \), so that the
time domain representation of the field is given by
\begin{dmath}\label{eqn:polarization_circular:160}
F(\Bx, t) = (1 + \Be_3) \lr{
\lr{ \alpha_1 \Be_1 + \alpha_2 \Be_2 } \cos\lr{ \omega t - \beta z }
-\lr{ \beta_1 \Be_1 + \beta_2 \Be_2 } \sin\lr{ \omega t - \beta z }
}.
\end{dmath}

Two geometric representations are possible.  The first uses the pseudoscalar for the transverse plane \( \Be_{12} \), denoted \( i \) here, and the other uses the \R{3} pseudoscalar as the imaginary.
\subsubsection{Transverse plane imaginary.}
\maketheorem{Circular polarization coefficients.}{thm:polarizationRewrite:700}{
The time domain representation of the field in \cref{eqn:polarization_circular:160} can be stated in terms of the total phase as
\begin{equation*}
F = \lr{ 1 + \Be_3 } \Be_1 \lr{ \alpha_\txtR e^{i\phi} + \alpha_\txtL e^{-i\phi} },
\end{equation*}
where
\begin{equation*}
\begin{aligned}
\alpha_\txtR &= \inv{2}\lr{ c_1 + i c_2 } \\
\alpha_\txtL &= \inv{2}\lr{ c_1 - i c_2 }^\dagger,
\end{aligned}
\end{equation*}
where \( c_1, c_2 \) are the 0,2 grade multivector representations of the Jones vector coordinates
\begin{equation*}
\begin{aligned}
c_1 &= \alpha_1 + i \beta_1 \\
c_2 &= \alpha_2 + i \beta_2,
\end{aligned}
\end{equation*}
and \( \phi(z,t) = \omega t - \beta z \) is the phase angle.
} % definition

\begin{itemize}
\item
Linear polarization at an angle \( \psi\) from the x-axis in the transverse plane is given by
\( \alpha_\txtR = \inv{2}\Norm{\BE} e^{i(\psi + \theta)},
\alpha_\txtL = \inv{2}\Norm{\BE} e^{i(\psi - \theta)} \), for which the field is \( F = \lr{ 1 + \Be_3 } \Norm{\BE} \Be_1 e^{i \psi} \cos(\phi + \theta) \),
where \( \theta \) is an initial phase angle.
\item Following the IEEE antenna convention discussed in \citep{balanis1989advanced}, we define right(left) circular polarization as the
a change in phase that
results in the electric field tracing out a (clockwise,counterclockwise) circle
\begin{dmath}\label{eqn:polarization_circular:180}
\begin{aligned}
\BE_\txtR &= \Norm{\BE} \lr{ \Be_1 \cos\phi + \Be_2 \sin\phi } = \Norm{\BE} \Be_1 \exp\lr{  \Be_{12} \phi } \\
\BE_\txtL &= \Norm{\BE} \lr{ \Be_1 \cos\phi - \Be_2 \sin\phi } = \Norm{\BE} \Be_1 \exp\lr{ -\Be_{12} \phi }.
\end{aligned}
\end{dmath}
Right and left circular polarization in this representation are given by
\(\alpha_\txtR = \Norm{\BE}, \alpha_\txtL = 0 \) and \(\alpha_\txtL = \Norm{\BE}, \alpha_\txtR = 0 \) respectively.  The right(left) polarized fields are just
\( F = (1 + \Be_3) \Norm{\BE} \Be_1 e^{\pm i(\omega t - k z)} \).
\item An ellipically polzarized field is given by
\( \alpha_\txtR = \inv{2}\lr{ E_a - E_b },
\alpha_\txtL = \inv{2}\lr{ E_a + E_b } \), or
\begin{dmath}\label{eqn:ece2500report:2580}
F = \inv{2} (1 + \Be_3) \Be_1 \lr{ (E_a + E_b) e^{i\phi} + (E_a - E_b) e^{-i\phi} }
\end{dmath}
A hyperbolic hyperbolic parameterization of the elliptically polarized wave is also discussed in the book.  In particular,
\begin{dmath}\label{eqn:polarization_elliptical:380}
\begin{aligned}
F &= e E_a \lr{ 1 + \Be_3 } \Be_1 e^{ i \psi } \cosh\lr{ m + i \phi} \\
m &= \tanh^{-1}\lr{ E_b/E_a } \\
e &= \sqrt{1 - {(E_b/E_a)}^2 },
\end{aligned}
\end{dmath}
where \( E_a(E_b) \) are the magnitudes of the electric field components lying along the semi-major axis directed along \(
\begin{bmatrix}
\Be_1 \\
\Be_2
\end{bmatrix}
e^{i\psi} \) respectively.
Additional discussion and diagrams can be found in the book.
%\imageFigure{../figures/GAelectrodynamics/ellipticalPolarizationFig1}{Electric field with elliptical polarization.}{fig:ellipticalPolarization:ellipticalPolarizationFig1}{0.3}
\end{itemize}

Each of the polarizations considered above (linear, circular, elliptical) have the same general form
\begin{dmath}\label{eqn:polarizationRewrite:760}
F = \lr{ 1 + \Be_3 } \Be_1 e^{i\psi} f(\phi),
\end{dmath}
where \( f(\phi) \) is a complex valued function (i.e. grade 0,2).  The structure of \cref{eqn:polarizationRewrite:760} could be more general than considered so far.  For example, a Gaussian modulation could be added into the mix with \( f(\phi) = e^{i \phi - (\phi/\sigma)^2/2 } \).  The simple complex structure that encodes all the phase dependence allows the
energy, momentum and Maxwell stress tensor to be computed easily.
\maketheorem{Energy momentum tensor components for a plane wave.}{thm:polarizationRewrite:780}{
The energy momentum tensor components for the plane wave given by \cref{eqn:polarizationRewrite:760} are
\begin{equation*}
\begin{aligned}
T(1) &= -T(\Be_3) = \epsilon \lr{ 1 + \Be_3 } f f^\dagger \quad \lr{ = \calE + \frac{\BS}{c} } \\
T(\Be_1) &= T(\Be_2) = 0.
\end{aligned}
\end{equation*}
} % theorem

Only the propagation direction of a plane wave, regardless of its polarization (or even whether or not there are Gaussian or other damping factors), carries any energy or momentum, and only the propagation direction component of the Maxwell stress tensor \( \BT(\Ba) \) is non-zero.

Using \cref{thm:polarizationRewrite:780} the energy momentum vector may be computed for each of the polarizations considered above.
\begin{itemize}
\item
For the linearly polarization \( T(1) = \frac{\epsilon}{2} \lr{ 1 + \Be_3 } \Norm{\BE}^2 \cos^2( \phi + \theta ) \).
\item For the circularly polarization \( T(1) = \frac{\epsilon}{2} (1 + \Be_3) \Norm{\BE}^2 \).
A circularly polarized wave carries maximum energy and momentum, whereas the energy and momentum of a linearly polarized wave
oscillates with the phase angle.
\item For the elliptical polarization \(
T(1)
= \frac{\epsilon}{2} \lr{ 1 + \Be_3 } e^2 E_a^2 \lr{ \frac{E_b^2}{E_a^2} + 2 \lr{ 1 - \frac{E_b^2}{E_a^2} } \cos^2 \phi } \).
As expected, the phase dependent portion of the energy momentum tensor vanishes as the wave function approaches circular polarization.
\end{itemize}

\subsubsection{Pseudoscalar imaginary.}
Alternatively, it is possible to encode the sines and cosines in the time domain representation of the field in terms of the \R{3} pseudoscalar.

\maketheorem{Circular polarization coefficients.}{thm:polarizationRewrite:940}{
The time domain representation of the field in \cref{eqn:polarization_circular:160} can be stated in terms of the total phase as
\begin{equation*}
F = \lr{ 1 + \Be_3 } \Be_1 \lr{ \alpha_\txtR e^{-I\phi} + \alpha_\txtL e^{I\phi} },
\end{equation*}
where
\begin{equation*}
\begin{aligned}
\alpha_\txtR &= \inv{2}\lr{ c_1 + I c_2 }^\dagger \\
\alpha_\txtL &= \inv{2}\lr{ c_1 - I c_2 },
\end{aligned}
\end{equation*}
where \( c_1, c_2 \) are the 0,2 grade multivector representation of the Jones vector coordinates
\begin{equation*}
\begin{aligned}
c_1 &= \alpha_1 + I \beta_1 \\
c_2 &= \alpha_2 + I \beta_2,
\end{aligned}
\end{equation*}
defined here as 0,3 complex numbers, using \( I \) as the imaginary.
} % definition

There appear to be some advantages to pseudoscalar description of polarization, especially for computing energy momentum tensor components since \( I \) commutes with all grades.  For example, we can see practically by inspection that
\begin{equation}\label{eqn:polarization_pseudoscalarImaginary:620}
T(1) = \calE + \frac{\BS}{v} =
\epsilon \lr{ 1 + \Be_3 } \lr{ \Abs{\alpha_\txtR}^2 + \Abs{\alpha_\txtL}^2 },
\end{equation}
where the absolute value is computed using the reverse as the conjugation operation \( \Abs{z}^2 = z z^\dagger \).

\subsection{Transverse fields in a waveguide.}
One topic from waveguide theory is considered in the text.
\maketheorem{Transverse and propagation field solutions.}{thm:transverseField:348}{
Given a field propagating along the z-axis (either forward or backwards), with angular frequency \( \omega \), represented by the real part of
\begin{equation*}
F(x, y, z, t) = F(x, y) e^{j \omega t \mp j k z},
\end{equation*}
where \( \BE = \BE_z + \BE_t, \BH = \BH_z + \BH_t \), and \( \BA_z = (\BA \cdot \Be_3) \Be_3, \BA \in \BE, \BH \), the multivector field components in the axial and transverse ``directions''
\( F = F_z + F_t \)
are given by
\begin{equation*}
\begin{aligned}
F_z &= \inv{2} \lr{ F + \Be_3 F \Be_3 } \\
F_t &= \inv{2} \lr{ F - \Be_3 F \Be_3 },
\end{aligned}
\end{equation*}
and related to each other by
\begin{equation*}
\begin{aligned}
F_t &= j \inv{ \frac{\omega}{c} \mp k \Be_3 } \spacegrad_t F_z \\
F_z &= j \inv{ \frac{\omega}{c} \mp k \Be_3 } \spacegrad_t F_t,
\end{aligned}
\end{equation*}
Written out explicitly, the transverse field component expands as
\begin{equation*}
\begin{aligned}
\BE_t &=
\frac{j}{{\frac{\omega}{c}}^2 - k^2}
\lr{
   \pm k \spacegrad_t E_z
   + \frac{\omega \eta}{c} \Be_3 \cross \spacegrad_t H_z
}
\\
\eta \BH_t &=
\frac{j}{{\frac{\omega}{c}}^2 - k^2}
\lr{
   \pm k \eta \spacegrad_t H_z
   -
   \frac{\omega}{c}
   \Be_3 \cross \spacegrad_t E_z
}.
\end{aligned}
\end{equation*}
} % theorem

Details and proof are in the book.  This and many other examples in the book show that we pay a significant additional cost to work with separate electric and magnetic field components compared to working with the complete electromagnetic field multivector \( F \) in its entirety.

\subsection{Multivector potential.}
Conventional electromagnetism utilizes scalar and vector potentials, which may be generalized to a multivector potential containing their sums.
\makedefinition{Multivector potential.}{thm:generalPotential:80}{
The electromagnetic field strength \( F \) for a \textit{multivector potential} \( A \) is
\begin{equation*}
F = \gpgrade{\conjstgrad A}{1,2}.
\end{equation*}
} % definition

The grades of the multivector potentials may be chosen to match SI conventions, and those of \citep{balanis2005antenna} (which includes fictitious magnetic sources) are used in the book as follows.
\maketheorem{Fields and the potential wave equations.}{thm:generalPotential:40}{
Given
%\label{eqn:gaugeTransformation:1111}
\begin{equation*}
A =
      - \phi
      + c \BA
      + \eta I \lr{ -\phi_m + c \BF },
\end{equation*}
where
\begin{enumerate}
\item \( \phi \) is the scalar potential \si{V} (Volts).
\item \( \BA \) is the vector potential \si{W/m} (Webers/meter).
\item \( \phi_m \) is the scalar potential for (fictitious) magnetic sources \si{A} (Amperes).
\item \( \BF \) is the vector potential for (fictitious) magnetic sources \si{C} (Coulombs),
\end{enumerate}
the electric field vector and the magnetic field bivector associated with a potential \( A \) are
\begin{equation*}
\begin{aligned}
\BE &=
\gpgrade{\conjstgrad A}{1}
=
   - \spacegrad \phi
   - \PD{t}{\BA}
   - \inv{\epsilon} \spacegrad \cross \BF \\
I \eta \BH &=
\gpgrade{\conjstgrad A}{2}
=
   I \eta
   \lr{
      - \spacegrad \phi_\txtm
      - \PD{t}{\BF}
      + \inv{\mu} \spacegrad \cross \BA
   }
.
\end{aligned}
\end{equation*}
The potentials are related to the sources by
\begin{equation*}
\begin{aligned}
\dLambertian
\phi &= -\frac{\rho}{\epsilon} - \PD{t}{} \lr{ \spacegrad \cdot \BA + \inv{c^2} \PD{t}{\phi} } \\
\dLambertian
\BA &= -\mu \BJ + \spacegrad \lr{ \spacegrad \cdot \BA + \inv{c^2} \PD{t}{\phi} } \\
\dLambertian
\BF &= - \epsilon \BM + \spacegrad \lr{ \spacegrad \cdot \BF + \inv{c^2} \PD{t}{\phi_\txtm} } \\
\dLambertian
\phi_\txtm &= -\frac{\rho_\txtm}{\mu} - \PD{t}{} \lr{ \spacegrad \cdot \BF + \inv{c^2} \PD{t}{\phi_\txtm} }
\end{aligned}
\end{equation*}
} % theorem

Also detailed in the book is the multivector formulation of gauge transformation that allows the grade selection operation in
\cref{thm:generalPotential:80} to be removed.

\maketheorem{Gauge invariance.}{thm:gaugeTransformation:60}{
The spacetime gradient of a grade 0,3 multivector \( \Psi \) may be added to a multivector potential
\begin{equation*}
A' = A + \stgrad \Psi,
\end{equation*}
without changing the field.
That is
\begin{equation*}
F
= \gpgrade{\conjstgrad A}{1,2}
= \gpgrade{\conjstgrad A'}{1,2}.
\end{equation*}
} % theorem

We say that we are working in the Lorenz gauge, if the 0,3 grades of \( \conjstgrad A \) are zero, or a transformation that kills those grades is made.
\maketheorem{Lorentz gauge transformation.}{thm:gaugeTransformation:140}{
Given any multivector potential \( A \) solution of Maxwell's equation, the transformation
\begin{equation*}
A' = A - \stgrad \Psi,
\end{equation*}
where
\begin{equation*}
\dLambertian \Psi = \gpgrade{ \conjstgrad A }{0,3},
\end{equation*}
allows Maxwell's equation to be written in wave equation form
\begin{equation*}
\dLambertian A' = J.
\end{equation*}
} % theorem

Please see the book for proofs and additional details, including the explcit integral solution of \( \Psi \) required for the transformation to the Lorentz gauge.

\subsection{Far field.}
The geometric algebra form for the far field associated with a vector (or dual-vector) potential has a fairly simple coordinate free form.
\maketheorem{Far field magnetic vector potential.}{thm:potentialSection_farfield:1}{
Given a spherical wave vector(dual-vector) potentials with representations
%\label{eqn:potentialSection_farfield:2400}
\begin{equation*}
\begin{aligned}
\BA &= \frac{e^{-j k r}}{r} \bcA( \theta, \phi ) \\
\BF &= \frac{e^{-j k r}}{r} \bcF( \theta, \phi ),
\end{aligned}
\end{equation*}
the far field (\(r \gg 1 \)) electromagnetic fields are given respectively
%\label{eqn:potentialSection_farfield:2520}{
\begin{equation*}
\begin{aligned}
F &= -j \omega \lr{ 1 + \rcap } \lr{ \rcap \wedge \BA} \\
F &= -j \omega \eta I \lr{ \rcap + 1 } \lr{ \rcap \wedge \BF }.
\end{aligned}
\end{equation*}
} % theorem
Noting that \( \rcap (\rcap \wedge \BA) \) is the rejection of the radial component of \( \BA \) (i.e. is a vector not a multivector) allows the far-field solution to easily be split into electric and magnetic field components if desired.  These are detailed in the book, along with the proof of \cref{thm:potentialSection_farfield:1}.  Also included in the book is an example calculation of \( F = -j \omega \lr{ 1 + \rcap } \lr{ \rcap \wedge \BA} \) for the dipole vector potential.

\subsection{Dielectric and magnetic media.}
The majority of the electromagnetic theory covered in the book focused on fields with the 
isotropic constitutive relationships \cref{eqn:freespace:300}.  For more general consitutive relationships the geometric algebra form of Maxwell's equations requires a pair of 
multivector equations, fields and sources as follows.
\maketheorem{Maxwell's equations in media.}{thm:dielectric:20}{
Maxwell's equations in media are
\begin{equation*}
\begin{aligned}
\gpgrade{ \stgrad F }{0,1} &= J_\txte \\
\gpgrade{ \stgrad G }{2,3} &= I J_\txtm,
\end{aligned}
\end{equation*}
where \( c \) is the group velocity of \( F, G \) in the medium,
the fields are grade 1,2 multivectors
\begin{equation*}
\begin{aligned}
F &= \BD + \frac{I}{c}\BH \\
G &= \BE + I c \BB,
\end{aligned}
\end{equation*}
and the sources are grade 0,1 multivectors
\begin{equation*}
\begin{aligned}
J_\txte &= \rho - \inv{c}\BJ \\
J_\txtm &= c \rho_\txtm - \BM.
\end{aligned}
\end{equation*}
} % theorem

Along with some discussion of solution of these more complicated equations, gauge-like transformations of the fields are discussed.  
More work is required to fully flesh out this topic, especially given that \( F, G \) may be directly coupled, allowing neither field to be solved for independently.

\subsection{Boundary value conditions.}
Independent of the techniques used to find the multivector fields \( F, G \) for electromagnetism in matter, we may use \cref{thm:dielectric:20} to easily derive the boundary
value conditions for the fields spanning a surface with surface currents or charges, or even just a discontinuity in the media.
\maketheorem{Boundary value relations.}{thm:boundarySurfaceSources:480}{
The difference in the normal and tangential components of the electromagnetic field spanning a surface on which there are
a surface current or surface charge or current densities \( J_\txte = J_{\textrm{es}} \delta(n), J_\txtm = J_{\textrm{ms}} \delta(n) \)
can be related to those surface sources as follows
%\label{eqn:boundarySurfaceSources:420}
\begin{equation*}
\begin{aligned}
\gpgrade{\ncap (F_2 - F_1) }{0,1} &= J_{\textrm{es}} \\
\gpgrade{\ncap (G_2 - G_1) }{2,3} &= I J_{\textrm{ms}},
\end{aligned}
\end{equation*}
where \( F_k = \BD_k + I \BH_k/c, G_k = \BE_k + I c \BB_k, k = 1,2 \) are the fields in the
where \( \ncap = \ncap_2 = -\ncap_1 \) is the outwards facing normal in the second medium.
In terms of the conventional constituent fields, these may be written
%\label{eqn:boundarySurfaceSources:460}
\begin{equation*}
\begin{aligned}
\ncap \cdot \lr{ \BD_2 - \BD_1 } &= \rho_\txts \\
\ncap \cross \lr{ \BH_2 - \BH_1 } &= \BJ_\txts \\
\ncap \cdot \lr{ \BB_2 - \BB_1 } &= \rho_{\textrm{ms}} \\
\ncap \cross \lr{ \BE_2 - \BE_1 } &= -\BM_\txts.
\end{aligned}
\end{equation*}
} % theorem

A simple proof is possible by integrating the pair of Maxwell's equations over the pillbox configuration, allowing the height \( n \) of that pillbox above or below the surface to tend to zero,
and the area of the pillbox top to also tend to zero, using \cref{thm:volumeintegral:100} to transform the multivector integrals to boundary integrals.  Figures and the full proof are available in the book.

Note that in the special case where there are surface charge and current densities along the interface surface, but the media is uniform (\(\epsilon_1 = \epsilon_2, \mu_1 = \mu_2\)), then the field and current relationship has a particularily simple form \citep{chappell2014geometric}
\begin{dmath}\label{eqn:boundarySurfaceSources:421}
\ncap (F_2 - F_1) = J_s.
\end{dmath}

\section{Conclusions.}

\section{Chapter II: Geometric Calculus.}

There were two goals of this chapter.  The first was to present the theory of multivector integration along curves and surfaces, and the second was to derive all the Green's functions required for electromagnetism.
The multivector integration theory incorporates all of vector calculus as well as the more abstract theory of differential forms or integration on manifolds, but also generalizes both.

\subsection{Reciprocal frames.}
In order to work with the
curvilinear non-orthonormal bases that will be encountered in general integration theory, the concept of a reciprocal frame is required.

\makedefinition{Reciprocal frame}{dfn:reciprocal:frame}{
Given a basis for a subspace \( \setlr{ \Bx_1, \Bx_2, \cdots \Bx_n } \), where the vectors \( \Bx_i \) are not necessarily orthonormal, the reciprocal frame is defined as the set of vectors \( \setlr{ \Bx^1, \Bx^2, \cdots \Bx^n } \) satisfying
\begin{dmath*}
\Bx_i \cdot \Bx^j = {\delta_i}^j,
\end{dmath*}
where the vector \( \Bx^j \) is not the j-th power of \( \Bx \), but is a superscript index, the conventional way of denoting a reciprocal frame vector, and \( {\delta_i}^j \) is the Kronecker delta.
} % definition

Note that any orthonormal basis is also its reciprocal basis.  As also done in tensor and relativistic theory, we use mixed index (upper and lower) variables when working with curvilinear coordinates.  Summation (Einstein) convention was not used in the text, and all sums that could be implied by appropriately mixed indexes quantities are marked with an explicit summation symbol.

A vector may be expressed in terms of either the curvilinear or reciprocal basis 
\begin{equation}\label{eqn:ece2500report:n}
\Ba 
= \sum_i \lr{ \Ba \cdot \Bx^i } \Bx_i,
= \sum_i \lr{ \Ba \cdot \Bx_i } \Bx^i
\end{equation}
showing that reciprocal frame can be used to compute the coordinates of a vector in the curvilinear basis and the curvilinear basis can be used to compute the coordinates with respect to the reciprocal basis.  
Because we never use coordinates \( a_i = \Ba \cdot \Bx_i, a^i = \Ba \cdot \Bx^i \) isolated from the associated basis, we have no need of the covariant nor contravariant tensor nomenclature.  The text contains a number of examples of reciprocal frames, methods of computation, Mathematica examples, and other related concepts including computation of coordinates of higher grade blades with respect to curvilinear coordinates.

\subsection{Curvilinear coordinates and hypervolume elements.}
Curvilinear coordinates can be defined for any subspace spanned by a parameterized vector into that space.  In the book we use one, two or three parameter bases.  For a vector parameterization \( \Bx(u_1, u_2, u_3) \) we define the curvilinear basis elements \( \Bx_i = \PDi{\Bx}{u_i} \), and associated vector valued differentials \( d\Bx_i = \Bx_i du_i \).  Bivector valued area elements \( d^2 \Bx \equiv d\Bx_1 \wedge d\Bx_2 \) are formed by wedging the two vector differentials for a two parameter manifold, 
whereas trivector valued volume elements  \( d^3 \Bx \equiv d\Bx_1 \wedge d\Bx_2 \wedge d\Bx_3 \)
are formed by wedging all three of the differentials for the parameterized space.  Such hypervolume elements have an orientation, as changing the order of any two differentials toggles the sign.  As the book is concerned primarily with Euclidean spaces, the absolute values of such area and volume differentials are \( \sqrt{-\lr{ d^2 \Bx}^2}, \sqrt{-\lr{ d^3 \Bx}^3 } \), which are the respective areas and volumes of the paralellogram and paralellopiped spanned by the vector differentials in each of the parameterization directions.

The book contains examples, figures, Mathematica calculations, and discussion that elaborates and explains these concepts in detail.

\subsection{Gradient and vector derivative.}

The final tool required to formulate multivector integration theory is the vector derivative, the projection of the gradient onto the integration manifold.  

\maketheorem{Reciprocal frame vectors}{thm:curvilinearGradient:1}{
Given a curvilinear basis with elements \( \Bx_i = \PDi{u_i}{\Bx} \), the reciprocal frame vectors are given by
\begin{dmath*}
\Bx^i = \spacegrad u_i.
\end{dmath*}
} % theorem

\maketheorem{Curvilinear representation of the gradient}{thm:curvilinearGradient:2}{
Given an N-parameter vector parameterization
\( \Bx = \Bx(u_1, u_2, \cdots, u_N) \)
of \R{N},
with curvilinear basis elements \( \Bx_i = \PDi{u_i}{\Bx} \), the gradient can be expressed as
\begin{dmath*}
\spacegrad = \sum_i \Bx^i \PD{u_i}{}.
\end{dmath*}
It is often convenient to define \( \partial_i \equiv \PDi{u_i}{} \), so that the gradient can be expressed in mixed index representation
\begin{dmath*}
\spacegrad = \sum_i \Bx^i \partial_i.
\end{dmath*}
%or the same with sums over mixed indexes implied.
} % theorem

\makedefinition{Vector derivative}{thm:gradient:100}{
Given an k-parameter vector parameterization
\( \Bx = \Bx(u_1, u_2, \cdots, u_k) \) of \R{N} with \( k \le N \),
and curvilinear basis elements \( \Bx_i = \PDi{u_i}{\Bx} \), the vector derivative \( \boldpartial \) is defined as
\begin{dmath*}
\boldpartial = \sum_{i=1}^k \Bx^i \partial_i.
\end{dmath*}
} % theorem

In general the gradient contains the vector derivative and we can write \( \spacegrad = \boldpartial + \spacegrad_\perp \), but 
 when the dimension of the subspace (number of parameters) equals the dimension of the underlying vector space, the vector derivative and gradient are identical.
We may think of the vector derivative as the projection of the gradient onto the tangent space at the point of evaluation.

In the book many many examples, figures, and Mathematica calculations are used to illustrate and explain the vector derivative and associated curvilinear parameterizations.
This includes calculations of curvilinear coordinates, area and volume elements, and the vector derivative
for polar, spherical and a toroidal coordinate systems.

\chapter{REWRITE/PRUNE marker.}
\subsection{Integration theory.}
We wish to generalize the concepts of line, surface and volume integrals to hypervolumes and multivector functions, and define a hypervolume integral as

\makedefinition{Multivector integral.}{dfn:fundamentalTheoremOfCalculus:240}{
Given a hypervolume parameterized by \( k \) parameters, k-volume volume element \( d^k \Bx \), and
multivector functions \( F, G \), a k-volume integral with the vector derivative acting to the right on \( G \) is written as
\begin{equation*}
\int_V d^k\Bx \rboldpartial G,
\end{equation*}
a k-volume integral with the vector derivative acting to the left on \( F \) is written as
\begin{equation*}
\int_V F d^k\Bx \lboldpartial,
\end{equation*}
and a k-volume integral with the vector derivative acting bidirectionally on \( F, G \) is written as
\begin{equation*}
\int_V F d^k\Bx \lrboldpartial G
\equiv
\int_V \lr{ F d^k\Bx \lboldpartial} G
+
\int_V F d^k\Bx \lr{ \rboldpartial G }.
\end{equation*}
The explicit meaning of these directionally acting derivative operations is given by the chain rule coordinate expansion
\begin{dmath*}
F d^k \Bx \lrboldpartial G
=
F d^k \Bx \lr{ \sum_i \Bx^i {\stackrel{ \leftrightarrow }{\partial_i}} } G
=
(\partial_i F) d^k \Bx \sum_i \Bx^i G
+
F d^k \Bx \sum_i \Bx^i (\partial_i G)
\equiv
(F d^k \Bx \lboldpartial) G
+
F d^k \Bx (\rboldpartial G),
\end{dmath*}
with \( \boldpartial \) acting on \( F \) and \( G \), but not the volume element \( d^k \Bx \), which may also be a function of the implied parameterization.
} % definition

The vector derivative
% (and gradient)
may not commute with \( F, G \) nor the volume element \( d^k \Bx \), so we are forced to use some notation to indicate what the vector derivative (or gradient) acts on.
In conventional right acting cases, where there is no ambiguity, arrows will usually be omitted, but braces may also be used to indicate the scope of derivative operators.
This bidirectional notation will also be used for the gradient, especially for volume integrals in \R{3} where the vector derivative is identitical to the gradient.

Some authors use the Hestenes dot notation, with overdots or primes to indicatating the exact scope of multivector derivative operators, as in
\begin{dmath}\label{eqn:fundamentalTheoremOfCalculus:260}
\dot{F} d^k \Bx \dot{\boldpartial} \dot{G} =
\dot{F} d^k \Bx \dot{\boldpartial} G
+
F d^k \Bx \dot{\boldpartial} \dot{G}.
\end{dmath}
The dot notation has the advantage of emphasizing that the action of the vector derivative (or gradient) is on the functions \( F, G \), and not on the hypervolume element \( d^k \Bx \).
However, in this book, where primed operators such as \( \spacegrad' \) are used to indicate that derivatives are taken with respect to primed \( \Bx' \) variables, a mix of dots and ticks would have been confusing.
%Over arrows also have the advantage of being visually conspicuous.

\subsubsection{Fundamental theorem.}

The fundamental theorem of geometric calculus is a generalization of many conventional scalar and vector integral theorems, and relates a hypervolume integral to its boundary.
This is a a powerful theorem, which we will use with Green's functions to solve Maxwell's equation, but also to derive the geometric algebra form of Stokes' theorem, from which most of the familiar integral calculus results follow.

\maketheorem{Fundamental theorem of geometric calculus}{thm:fundamentalTheoremOfCalculus:1}{
Given
multivectors \(F, G \),
a parameterization \( \Bx = \Bx(u_1, u_2, \cdots) \), with hypervolume element \( d^k \Bx = d^k u I_k \), where
\( I_k = \Bx_1 \wedge \Bx_2 \wedge \cdots \wedge \Bx_k \), the hypervolume integral is related to the boundary integral by
\begin{equation*}
\int_V F d^k \Bx \lrboldpartial G = \oint_{\partial V} F d^{k-1} \Bx G,
\end{equation*}
where \( \partial V \) represents the boundary of the volume, and \( d^{k-1} \Bx \) is the hypersurface element.  The hypersurface element and boundary integral is defined for \( k > 1 \) as
\begin{equation*}
\oint_{\partial V} F d^{k-1} \Bx G
\equiv
\sum_i \int d^{k-1} u_i \evalbar{ \lr{ F \lr{ I_k \cdot \Bx^i} G }}{\Delta u_i},
\end{equation*}
where \( d^{k-1} u_i \) is the product of all \( du_j \) except for \( du_i \).
For
\( k = 1 \) the hypersurface element and associated
boundary ``integral''
is really just convienent general shorthand, and
should be taken to mean the evaluation of the \( F G \) multivector product over the range of the parameter
\begin{equation*}
\oint_{\partial V} F d^{0} \Bx G
\equiv
\evalbar{ F G }{\Delta u_1}.
\end{equation*}
} % theorem

The geometry of the hypersurface element \( d^{k-1} \Bx \) will be made more clear when we
consider the specific cases of \( k = 1, 2, 3 \), representing generalized line, surface, and volume integrals respectively.
Instead of terrorizing the reader with a general proof
\cref{thm:fundamentalTheoremOfCalculus:1},
which requires some unpleasant index gymnastics,
this book
will separately state and prove the fundamental theorem of calculus
for each of the \( k = 1, 2, 3 \) cases that are of interest for problems in \R{2} and \R{3}.
For the interested reader, a sketch of the general proof
of \cref{thm:fundamentalTheoremOfCalculus:1}
is available in \cref{chap:gagcProof}.

Before moving on to the line, surface, and volume integral cases, we will state and prove the
general Stokes' theorem in its geometric algebra form.

%}
%\EndArticle
   \subsubsection{Stokes' theorem.}
%
%
An important consequence of the fundamental theorem of geometric calculus is the
geometric algebra generalization of Stokes' theorem.
The Stokes' theorem that we know from conventional vector calculus relates
\R{3} surface integrals to the line integral around a bounding surface.
The geometric algebra form of Stokes' theorem is equivalent to Stokes' theorem from the theory of differential forms, which relates
hypervolume integrals of blades\footnote{Blades are isomorphic to the k-forms found in the theory of differential forms.} to the integrals over their hypersurface boundaries, a much more general result.

\maketheorem{Stokes' theorem}{thm:stokesTheoremGeometricAlgebra:1740}{
Given a \(k\) volume element \(d^k \Bx \) and an s-blade \( F, s < k \)
\begin{equation*}
\int_V d^k \Bx \cdot (\boldpartial \wedge F) = \int_{\partial V} d^{k-1} \Bx \cdot F.
\end{equation*}
}

We will see that most of the well known scalar and vector integral theorems can easily be derived as direct consequences of \cref{thm:stokesTheoremGeometricAlgebra:1740}, itself a special case of \cref{thm:fundamentalTheoremOfCalculus:1}.

We can prove Stokes' theorem
from \cref{thm:fundamentalTheoremOfCalculus:1}
by setting \( F = 1 \), and requiring that \( G \)
is an s-blade, with grade \( s < k \).
The proof follows by selecting the \( k-(s+1) \) grade, the lowest grade of \( d^k \Bx (\boldpartial \wedge G) \), from both sides of \cref{thm:fundamentalTheoremOfCalculus:1}.

For the grade selection of the hypervolume integral we have
\begin{dmath}\label{eqn:stokesTheoremTheStatement:100}
\gpgrade{ \int_V d^k \Bx \boldpartial G }{k-(s+1)}
=
\gpgrade{
\int_V d^k \Bx (\boldpartial \cdot G )
+
\int_V d^k \Bx (\boldpartial \wedge G )
}{k-(s-1)},
\end{dmath}
however, the lowest grade of \( d^k \Bx (\boldpartial \cdot G ) \) is \( k -(s-1) = k - s + 1 > k - (s+1) \), so the divergence integral is zero.  As \( d^{k-1} \Bx \) is a \( k - 1 \) blade
\begin{dmath}\label{eqn:stokesTheoremTheStatement:110}
\int_V d^k \Bx \cdot (\boldpartial \wedge G )
= \int_{\partial V} \gpgrade{d^{k-1} \Bx G}{k-(s+1)}
= \int_{\partial V} d^{k-1} \Bx \cdot G,
\end{dmath}
proving the theorem.
   \subsubsection{Line integral.}
%
%
%{
%\subsection{Line integral.}
\label{chap:lineintegral}

A single parameter curve, and the corresponding differential with respect to that parameter, is plotted in
\cref{fig:oneParameterDifferential:oneParameterDifferentialFig1}.
%, for a parameterization over \( [a, b] \in [0,1]\otimes[0,1] \).

\imageFigure{../figures/GAelectrodynamics/oneParameterDifferentialFig1}{One parameter manifold.}{fig:oneParameterDifferential:oneParameterDifferentialFig1}{0.2}

The differential with respect to the parameter \( a \) is
\begin{equation}\label{eqn:lineintegral:20}
d\Bx_a = \PD{a}{\Bx} da = \Bx_a da.
\end{equation}

The vector derivative has just one component
\begin{dmath}\label{eqn:lineintegral:40}
\boldpartial
=
\sum_i \Bx^i (\Bx_i \cdot \spacegrad)
=
\Bx^a \PD{a}{}
\equiv
\Bx^a \partial_a.
\end{dmath}

The line integral specialization of \cref{dfn:fundamentalTheoremOfCalculus:240} can now be stated

\makedefinition{Multivector line integral.}{dfn:lineintegral:100}{
Given an connected curve \( C \) parameterized by a single parameter, and multivector functions \( F, G \), we define the line integral as
\begin{equation*}
\int_C F d^1\Bx \lrboldpartial G
\equiv
\int_C \lr{ F d^1\Bx \lboldpartial} G
+
\int_C F d^1\Bx \lr{ \rboldpartial G },
\end{equation*}
where the one parameter differential form \( d^1 \Bx = da\, \Bx_a \) varies over the curve.
} % definition

The line integral specialization of \cref{thm:fundamentalTheoremOfCalculus:1} is

\maketheorem{Multivector line integral.}{thm:lineintegral:100}{
Given an connected curve \( C \) parameterized by a single parameter, and multivector functions \( F, G \), the line integral
\begin{equation*}
\int_C F d^1\Bx \boldpartial G
= \evalbar{F G}{\Delta a}.
\end{equation*}
} % theorem

The proof follows by expansion.
For
the (single variable) parameterization \( a \) above
\begin{dmath}\label{eqn:lineintegral:120}
\int_C F d^1\Bx \lrboldpartial G
=
\int_C \lr{ F d^1\Bx \lboldpartial} G
+
\int_C F d^1\Bx \lr{ \rboldpartial G }
=
\int_C \PD{a}{F} da \Bx_a \Bx^a G
+
\int_C F da \Bx_a \Bx^a \PD{a}{G}
=
\int_C da \PD{a}{F} G
+
\int_C da F \PD{a}{G}
=
\int_C da \PD{a}{} \lr{ F G }
=
F(a_1) G(a_1) -
F(a_0) G(a_0),
\end{dmath}
where the boundaries of the parameterization have been assumed to be
%\( C\lcolon a \in [a_0, a_1] \).
\( C: a \in [a_0, a_1] \).
We have a perfect cancellation of the reciprocal frame \( \Bx^a \) with the vector \( \Bx_a \) that lies along the curve, since \( \Bx^a \Bx_a = 1 \).  This leaves a perfect derivative of the product of \( F G \), which can be integrated over the length of the curve, yielding the difference of the product with respect to the parameterization of the end points of the curve.

For a single parameter subspace
the reciprocal frame vector \( \Bx^a \)
is trivial to calculate, as it is just the inverse of \( \Bx_a \), that is \( \Bx^a = \Bx_a/\Norm{\Bx_a}^2 \).
Observe that we did not actually have to calculate it, but instead only require that the vector is invertible.

An important (and familiar) special case of \cref{thm:lineintegral:100} is the fundamental theorem of calculus for line integrals, which can be obtained by using a
single scalar function \( f \)

\maketheorem{Line integral of a scalar function (Stokes').}{thm:lineintegral:180}{
Given a scalar function \( f \), its line integral is given by
\begin{equation*}
\int_C d^1\Bx \cdot \lrboldpartial f =
\int_C d^1\Bx \cdot \spacegrad f = \evalbar{f}{\Delta a}.
\end{equation*}
} % theorem

Writing out \cref{thm:lineintegral:100} with \( F = 1, G = f(\Bx(a)) \), we have
\begin{dmath}\label{eqn:lineintegral:140}
\int_C d^1\Bx \boldpartial f = \evalbar{f}{\Delta a}.
\end{dmath}

This is a multivector equation with scalar and bivector grades on the left hand side, but only scalar grades on the right.  Equating grades yields two equations

\begin{subequations}
\label{eqn:lineintegral:180}
\begin{dmath}\label{eqn:lineintegral:160}
\int_C d^1\Bx \cdot \boldpartial f = \evalbar{f}{\Delta a}
\end{dmath}
\begin{dmath}\label{eqn:lineintegral:200}
\int_C d^1\Bx \wedge \boldpartial f = 0
\end{dmath}
\end{subequations}

Because \( d^1\Bx \cdot \boldpartial = d^1\Bx \cdot \spacegrad \), we can replace the vector derivative with the gradient in \cref{eqn:lineintegral:160}, which yields the conventional line integral result, proving the theorem.

%}
%\EndNoBibArticle
   \subsubsection{Surface integral.}
%
%
%{
%\subsection{Surface integral.}
%\label{chap:surfaceintegral}

A two parameter curve, and the corresponding differentials with respect to those parameters, is plotted in
\cref{fig:twoParameterDifferential:twoParameterDifferentialFig1}.

\imageFigure{../figures/GAelectrodynamics/twoParameterDifferentialFig1}{Two parameter manifold differentials.}{fig:twoParameterDifferential:twoParameterDifferentialFig1}{0.4}

Given parameters \( a, b \), the differentials along each of the parameterization directions are
\begin{dmath}\label{eqn:surfaceintegral:100}
\begin{aligned}
d\Bx_a &= \PD{a}{\Bx} da = \Bx_a da \\
d\Bx_b &= \PD{b}{\Bx} db = \Bx_b db.
\end{aligned}
\end{dmath}

The bivector valued surface area element for this parameterization is
\begin{equation}\label{eqn:surfaceintegral:120}
d^2 \Bx
=
d\Bx_a \wedge
d\Bx_b
=
da db (\Bx_a \wedge \Bx_b).
\end{equation}

The vector derivative, the projection of the gradient onto the surface at the point of integration (also called the tangent space), now has two components
\begin{dmath}\label{eqn:surfaceintegral:200}
\boldpartial
=
\sum_i \Bx^i (\Bx_i \cdot \spacegrad)
=
\Bx^a \PD{a}{}
+
\Bx^b \PD{b}{}
\equiv
\Bx^a \partial_a
+
\Bx^b \partial_b.
\end{dmath}

The surface integral specialization of \cref{dfn:fundamentalTheoremOfCalculus:240} can now be stated

\makedefinition{Multivector surface integral.}{dfn:surfaceintegral:100}{
Given an connected surface \( S \) parameterized by two parameters, and multivector functions \( F, G \), we define the surface integral as
\begin{equation*}
\int_S F d^2\Bx \boldpartial G
\equiv
\int_S \lr{ F d^2\Bx \lboldpartial} G
+
\int_S F d^2\Bx \lr{ \rboldpartial G },
\end{equation*}
where the two parameter differential form \( d^2 \Bx = da db\, \Bx_a \wedge \Bx_b \) varies over the surface.
} % definition

%%As mentioned in a line integral context,
%%multivectors may not commute with the vector derivative or the differential, so we allow the vector derivative to act bidirectionally using the chain rule.
%%The scope of the action of the vector derivative when acting only to the left or right is indicated using braces above.
%%Should we wish to only integrate single functions, we can set either of the other to \( 1 \), yielding integrals of the form
%%\( \int_S F d^2\Bx \lboldpartial, \) or \( \int_S d^2\Bx \boldpartial G \).

The surface integral specialization of \cref{thm:fundamentalTheoremOfCalculus:1} is

\maketheorem{Multivector surface integral.}{thm:surfaceintegral:100}{
Given an connected surface \( S \) parameterized by two parameters, and multivector functions \( F, G \), the surface integral
\begin{equation*}
\int_S F d^2\Bx \boldpartial G
= \ointclockwise_{\partial S} F d\Bx G,
\end{equation*}
where \( \partial S \) is the boundary of the surface \( S \).
} % theorem
In \R{2} with \( d^2 \Bx = i dA, i = \Be_{12} \), this may be written
\begin{dmath}\label{eqn:surfaceintegral:760}
\int_S F i \spacegrad G\, dA
= \ointclockwise_{\partial S} F d\Bx G.
\end{dmath}
As \( i \) does not commute with all \R{2} multivectors, unless \( F = 1 \) we cannot generally pull \( i \) out of the integral.
In \R{3}, with \( d^2 \Bx = I \ncap dA \), we may use a scalar area element, but cannot generally replace the vector derivative with the gradient.
%\begin{dmath}\label{eqn:surfaceintegral:780}
%I \int_S F \ncap \boldpartial G\, dA
%= \ointclockwise_{\partial S} F d\Bx G.
%\end{dmath}

To see why this works, we would first like to reduce the product of the area element and the vector derivative
\begin{dmath}\label{eqn:surfaceintegral:300}
d^2\Bx \boldpartial
=
da db\, \lr{ \Bx_a \wedge \Bx_b } \lr{ \Bx^a \partial_a + \Bx^b \partial_b }.
\end{dmath}

Since \( \Bx^a \in \Span \setlr{ \Bx_a, \Bx_b } \), this multivector product has only a vector grade.  That is
\begin{dmath}\label{eqn:surfaceintegral:320}
\lr{ \Bx_a \wedge \Bx_b } \Bx^a
=
\lr{ \Bx_a \wedge \Bx_b } \cdot \Bx^a
+
\cancel{ \lr{ \Bx_a \wedge \Bx_b } \wedge \Bx^a }
=
\lr{ \Bx_a \wedge \Bx_b } \cdot \Bx^a
=
\Bx_a \lr{ \Bx_b \cdot \Bx^a } -
\Bx_b \lr{ \Bx_a \cdot \Bx^a }
=
-\Bx_b.
\end{dmath}

Similarly
\begin{dmath}\label{eqn:surfaceintegral:340}
\lr{ \Bx_a \wedge \Bx_b } \Bx^b
=
\lr{ \Bx_a \wedge \Bx_b } \cdot \Bx^b
+
\cancel{ \lr{ \Bx_a \wedge \Bx_b } \wedge \Bx^b }
=
\lr{ \Bx_a \wedge \Bx_b } \cdot \Bx^b
=
\Bx_a \lr{ \Bx_b \cdot \Bx^b } -
\Bx_b \lr{ \Bx_a \cdot \Bx^b }
=
\Bx_a,
\end{dmath}
so
\begin{dmath}\label{eqn:surfaceintegral:360}
d^2\Bx \boldpartial
=
\Bx_a \partial_b
-\Bx_b \partial_a.
\end{dmath}

Inserting this into the surface integral, we find
\begin{dmath}\label{eqn:surfaceintegral:380}
\int_S F d^2\Bx \boldpartial G
=
\int_S \lr{ F d^2\Bx \lboldpartial} G
+
\int_S F d^2\Bx \lr{ \rboldpartial G }
=
\int_S da db\, \lr{ \partial_b F \Bx_a - \partial_a F \Bx_b } G
+
\int_S da db\, F \lr{ \Bx_a \partial_b G - \Bx_b \partial_a G }
=
\int_S da db\, \lr{ \PD{b}{F} \PD{a}{\Bx} - \PD{a}{F} \PD{b}{\Bx} } G
+
\int_S da db\, F \lr{ \PD{a}{\Bx} \PD{b}{G} - \PD{b}{\Bx} \PD{a}{G} }
=
\int_S da db\, \PD{b}{} \lr{ F \PD{a}{\Bx} G } - \int_S da db\, \PD{a}{} \lr{ F \PD{b}{\Bx} G }
-
\int_S da db\, F \lr{ \PD{b}{} \PD{a}{\Bx} - \PD{a}{} \PD{b}{\Bx} } G
=
\int_S da db\, \PD{b}{} \lr{ F \PD{a}{\Bx} G } - \int_S da db\, \PD{a}{} \lr{ F \PD{b}{\Bx} G }.
\end{dmath}

This leaves two perfect differentials, which can both be integrated separately.  That gives
\begin{dmath}\label{eqn:surfaceintegral:400}
\int_S F d^2\Bx \boldpartial G
=
\int_{\Delta a} da\, \evalbar{\lr{ F \PD{a}{\Bx} G }}{\Delta b} - \int_{\Delta b} db\, \evalbar{\lr{ F \PD{b}{\Bx} G }}{\Delta a}
=
\int_{\Delta a} \evalbar{\lr{ F d\Bx_a G }}{\Delta b} - \int_{\Delta b} \evalbar{\lr{ F d\Bx_b G }}{\Delta a}.
\end{dmath}

Suppose we are integrating over the unit parameter volume space \( [a, b] \in [0,1] \otimes [0,1] \) as illustrated in
\cref{fig:twoParameterDifferentialBoundary:twoParameterDifferentialBoundaryFig2}.
\imageFigure{../figures/GAelectrodynamics/twoParameterDifferentialBoundaryFig2}{Contour for two parameter surface boundary.}{fig:twoParameterDifferentialBoundary:twoParameterDifferentialBoundaryFig2}{0.4}

Comparing to the figure we see that we've ended up with a clockwise line integral around the boundary of the surface.
For a given subset of the surface, the bivector area element can be chosen small enough that it lies in the tangent space
to the surface at the point of integration.
In that case, a larger bounding loop can be concepualized as the sum of a number of smaller ones, as sketched
in \cref{fig:loopIntegralInfinitesimalSum:loopIntegralInfinitesimalSumFig2},
in which case the
contributions of the interior loop segments cancel out.

\imageFigure{../figures/gabook/loopIntegralInfinitesimalSumFig2}{Sum of infinitesimal loops.}{fig:loopIntegralInfinitesimalSum:loopIntegralInfinitesimalSumFig2}{0.35}

\subsubsection{Two parameter Stokes' theorem.}

Two special cases of \cref{thm:surfaceintegral:100} when scalar and vector functions are integrated over a surface.  For scalar functions we have

\maketheorem{Surface integral of scalar function (Stokes').}{thm:surfaceintegral:420}{
Given a scalar function \( f(\Bx) \) its surface integrals is given by
\begin{equation*}
\int_S d^2 \Bx \cdot \boldpartial f =
\int_S d^2 \Bx \cdot \spacegrad f = \ointclockwise_{\partial S} d\Bx f.
\end{equation*}
In \R{3} this can be written as
\begin{equation*}
\int_S dA \ncap \cross \spacegrad f = \ointctrclockwise_{\partial S} d\Bx f,
\end{equation*}
where \( \ncap \) is the outwards normal specified by \( d^2 \Bx = I \ncap dA \).
} % theorem

To show the first part, we can split the (multivector) surface integral into vector and trivector grades
\begin{dmath}\label{eqn:surfaceintegral:440}
\int_S d^2\Bx \boldpartial f
=
\int_S d^2\Bx \cdot \boldpartial f
+
\int_S d^2\Bx \wedge \boldpartial f.
\end{dmath}

Since \( \Bx^a, \Bx^b \) both lie in the span of \( \setlr{ \Bx_a, \Bx_b } \),
\( d^2\Bx \wedge \boldpartial = 0 \), killing the second integral in \cref{eqn:surfaceintegral:440}.
If the gradient is decomposed into its projection along the tangent
space (the vector derivative) and its perpendicular components, only the vector derivative components of the
gradient contribute to its dot product with the area element.  That is
\begin{dmath}\label{eqn:surfaceintegral:460}
d^2 \Bx \cdot \spacegrad
=
d^2 \Bx \cdot \lr{ \Bx^a \partial_a + \Bx^b \partial_b + \cdots }
=
d^2 \Bx \cdot \lr{ \Bx^a \partial_a + \Bx^b \partial_b }
=
d^2 \Bx \cdot \boldpartial.
\end{dmath}

This means that for a scalar function
\begin{dmath}\label{eqn:surfaceintegral:480}
\int_S d^2\Bx \boldpartial f
=
\int_S d^2\Bx \cdot \spacegrad f.
\end{dmath}

The second part of the theorem follows by grade selection, and application of a duality transformation for the area element
\begin{dmath}\label{eqn:surfaceintegral:500}
d^2 \Bx \cdot \spacegrad f
=
\gpgradeone{ d^2 \Bx \spacegrad f }
=
dA \gpgradeone{ I \ncap \spacegrad f }
=
dA \gpgradeone{ I \lr{ \ncap \cdot \spacegrad f + I \ncap \cross \spacegrad f} }
=
-dA \ncap \cross \spacegrad f.
\end{dmath}

back substitution of \cref{eqn:surfaceintegral:500} completes the proof of \cref{thm:surfaceintegral:420}.

For vector functions we have

\maketheorem{Surface integral of a vector function (Stokes').}{thm:surfaceintegral:500}{
Given a vector function \( \Bf(\Bx) \) the surface integral is given by
\begin{equation*}
\int_S d^2 \Bx \cdot (\spacegrad \wedge \Bf) = \ointclockwise_{\partial S} d\Bx \cdot \Bf.
\end{equation*}
In \R{3} this can be written as
\begin{equation*}
\int_S dA \ncap \cdot \lr{ \spacegrad \cross \Bf} = \ointctrclockwise_{\partial S} d\Bx \cdot \Bf,
\end{equation*}
where \( \ncap \) is the outwards normal specified by \( d^2 \Bx = I \ncap dA \).
} % theorem

%%This follows by setting \( F = 1, G = \Bf \) in \cref{thm:surfaceintegral:100} and selecting the scalar grade.  In particular we may form the
%%scalar selection of \( d^2 \Bx \boldpartial \Bf \) in two different ways.  The first is
%%\begin{dmath}\label{eqn:surfaceintegral:520}
%%\gpgradezero{ d^2 \Bx \boldpartial \Bf }
%%=
%%\gpgradezero{ (d^2 \Bx \cdot \boldpartial + d^2 \Bx \wedge \boldpartial ) \Bf }.
%%\end{dmath}
%%
%%The \( d^2 \Bx \wedge \boldpartial \) product with \( \Bf \) has only bivector and quad-vector components (the latter is zero in \R{3}), so its scalar grade selection is zero, and we are left with
%%\begin{dmath}\label{eqn:surfaceintegral:540}
%%\gpgradezero{ d^2 \Bx \boldpartial \Bf }
%%=
%%(d^2 \Bx \cdot \boldpartial) \cdot \Bf
%%=
%%(d^2 \Bx \cdot \spacegrad) \cdot \Bf,
%%\end{dmath}
%%where we have used \cref{eqn:surfaceintegral:460} again.  This product can also be written as
%%\begin{dmath}\label{eqn:surfaceintegral:560}
%%(d^2 \Bx \cdot \spacegrad) \cdot \Bf
%%=
%%\gpgradezero{ (d^2 \Bx \cdot \spacegrad) \Bf }
%%=
%%\gpgradezero{ (d^2 \Bx \spacegrad - d^2 \Bx \wedge \spacegrad) \Bf }
%%=
%%\gpgradezero{ d^2 \Bx \spacegrad \Bf }
%%=
%%\gpgradezero{ d^2 \Bx \lr{ \cancel{ \spacegrad \cdot \Bf } + \spacegrad \wedge \Bf } }
%%=
%%d^2 \Bx \cdot \lr{ \spacegrad \wedge \Bf }.
%%\end{dmath}
%%
%%\begin{dmath}\label{eqn:surfaceintegral:580}
%%\ointclockwise_{\partial S} d\Bx \cdot \Bf
%%=
%%\gpgradezero{ \int_S d^2\Bx \boldpartial \Bf }
%%=
%%\int_S \lr{ d^2\Bx \cdot \spacegrad } \cdot \Bf
%%=
%%\int_S d^2\Bx \cdot \lr{ \spacegrad \wedge \Bf },
%%\end{dmath}
%%as claimed.  In particular in \R{3}, we have
%%\begin{dmath}\label{eqn:surfaceintegral:600}
%%d^2\Bx \cdot \lr{ \spacegrad \wedge \Bf }
%%=
%%dA \gpgradezero{ I \ncap I \lr{ \spacegrad \cross \Bf } }
%%=
%%-dA \ncap \cdot \lr{ \spacegrad \cross \Bf }.
%%\end{dmath}
%%
%%Substitution into \cref{eqn:surfaceintegral:580} proves the last part of \cref{thm:surfaceintegral:500}.
%%
\subsubsection{Green's theorem.}

\Cref{thm:surfaceintegral:500}, when stated in terms of coordinates, is another well known result

\maketheorem{Green's theorem.}{thm:surfaceintegral:620}{
Given a vector \( \Bf = \sum_i f_i \Bx^i \) in \R{N}, and a surface parameterized by \( \Bx = \Bx(u_1, u_2) \)
\begin{equation*}
\int_S du_1 du_2 \lr{ \PD{u_2}{f_1} - \PD{u_1}{f_2} }
=
\ointclockwise_{\partial S} du_1 f_1 + du_2 f_2.
\end{equation*}
This is
often stated for \R{2} with a Cartesian \(x,y\) parameterization, such as \( \Bf = P \Be_1 + Q \Be_2 \).  In that case
\begin{equation*}
\int_S dx dy \lr{ \PD{y}{P} - \PD{x}{Q} }
=
\ointclockwise_{\partial S} P dx + Q dy.
\end{equation*}
} % theorem

FIXME: Add an example (lots to pick from in any 3rd term calc text)

The first equality in \cref{thm:surfaceintegral:620} holds in \R{N} for vectors expressed in terms of an arbitrary curvilinear basis.
Only the (curvilinear) coordinates of the vector \( \Bf \) contribute to this integral, and only those that lie in the tangent space.
The reciprocal basis vectors \( \Bx^i \) are also nowhere to be seen.  This is because they are either obliterated in dot products with \( \Bx_j \), or cancel due to mixed partial equality.

To see how this occurs let's look at the
area integrand of \cref{thm:surfaceintegral:500}
\begin{dmath}\label{eqn:surfaceintegral:660}
d^2 \Bx \cdot \lr{ \spacegrad \wedge \Bf }
=
du_1 du_2\, \lr{ \Bx_1 \wedge \Bx_2 } \cdot \lr{ \sum_{ij} \lr{ \Bx^i \partial_i } \wedge \lr{ f_j \Bx^j } }
=
du_1 du_2\, \sum_{ij} \lr{ \lr{ \Bx_1 \wedge \Bx_2 } \cdot \Bx^i } \cdot \lr{ \partial_i (f_j \Bx^j) }
=
du_1 du_2\, \sum_{ij} \lr{ \lr{ \Bx_1 \wedge \Bx_2 } \cdot \Bx^i } \cdot \Bx^j \partial_i f_j
+
du_1 du_2\, \sum_{ij} f_j \lr{ \lr{ \Bx_1 \wedge \Bx_2 } \cdot \Bx^i } \cdot (\partial_i \Bx^j).
\end{dmath}

With a bit of trouble, we will see that the second integrand is zero.  On the other hand, the first integrand
simplifies
without too much trouble
\begin{dmath}\label{eqn:surfaceintegral:680}
\sum_{ij} \lr{ \lr{ \Bx_1 \wedge \Bx_2 } \cdot \Bx^i } \cdot \Bx^j \partial_i f_j
=
\sum_{ij} \lr{ \Bx_1 \delta_{2i} - \Bx_2 \delta_{1i} } \cdot \Bx^j \partial_i f_j
=
\sum_{j} \Bx_1 \cdot \Bx^j \partial_2 f_j -\Bx_2 \cdot \Bx^j \partial_1 f_j
=
\partial_2 f_1 - \partial_1 f_2.
\end{dmath}

For the second integrand, we have
\begin{dmath}\label{eqn:surfaceintegral:700}
\sum_{ij} f_j \lr{ \lr{ \Bx_1 \wedge \Bx_2 } \cdot \Bx^i } \cdot (\partial_i \Bx^j)
=
\sum_{j} f_j \sum_i \lr{ \Bx_1 \delta_{2i} - \Bx_2 \delta_{1i} } \cdot (\partial_i \Bx_j)
=
\sum_{j} f_j
\lr{
\Bx_1 \cdot (\partial_2 \Bx^j)
-
\Bx_2 \cdot (\partial_1 \Bx^j)
}
\end{dmath}

We can apply the chain rule (backwards) to the portion in brackets to find
\begin{dmath}\label{eqn:surfaceintegral:720}
\Bx_1 \cdot (\partial_2 \Bx^j)
-
\Bx_2 \cdot (\partial_1 \Bx^j)
=
\cancel{\partial_2 \lr{ \Bx_1 \cdot \Bx^j }}
-
(\partial_2 \Bx_1) \cdot \Bx^j
-
\cancel{\partial_1 \lr{ \Bx_2 \cdot \Bx^j }}
+
(\partial_1 \Bx_2) \cdot \Bx^j
=
\Bx_j \cdot \lr{ \partial_1 \Bx_2 - \partial_2 \Bx_1 }
=
\Bx_j \cdot \lr{ \PD{u_1}{} \PD{u_2}{\Bx} - \PD{u_2}{} \PD{u_1}{\Bx} }
= 0.
\end{dmath}

In this reduction the derivatives of \( \Bx_i \cdot \Bx^j = \delta_{ij} \) were killed since those are constants (either zero or one).  The final step relies on the fact that we assume our vector parameterization is well behaved enought that the mixed partials are zero.

Substituting these results into
\cref{thm:surfaceintegral:500} we find
\begin{dmath}\label{eqn:surfaceintegral:740}
\ointclockwise_{\partial S} d\Bx \cdot \Bf
=
\ointclockwise_{\partial S} \lr{ du_1 \Bx_1 + du_2 \Bx_2 } \cdot \lr{ \sum_i f_i \Bx^i }
=
\ointclockwise_{\partial S} du_1 f_1 + du_2 f_2
=
\int_S du_1 du_2\, \lr{ \partial_2 f_1 - \partial_1 f_2 },
\end{dmath}
which completes the proof.

%}
   \subsubsection{Volume integral.}
%
%
%{
\label{chap:volumeintegral}

%\subsubsection{Volume integral.}
A three parameter curve, and the corresponding differentials with respect to those parameters, is sketched in
\cref{fig:normalsOnVolumeAreaElement:normalsOnVolumeAreaElementFig11}.

\imageFigure{../figures/gabook/normalsOnVolumeAreaElementFig11}{Three parameter volume element.}{fig:normalsOnVolumeAreaElement:normalsOnVolumeAreaElementFig11}{0.4}

Given parameters \( u_1, u_2, u_3 \), we can denote the differentials along each of the parameterization directions as
\begin{dmath}\label{eqn:volumeintegral:100}
\begin{aligned}
d\Bx_1 &= \PD{u_1}{\Bx} du_1 = \Bx_1 du_1 \\
d\Bx_2 &= \PD{u_2}{\Bx} du_2 = \Bx_2 du_2 \\
d\Bx_3 &= \PD{u_3}{\Bx} du_3 = \Bx_3 du_3.
\end{aligned}
\end{dmath}

The trivector valued volume element for this parameterization is
\begin{equation}\label{eqn:volumeintegral:120}
d^3 \Bx
=
d\Bx_1 \wedge
d\Bx_1 \wedge
d\Bx_1
=
d^3 u\, (\Bx_1 \wedge \Bx_2 \wedge \Bx_3),
\end{equation}
where \( d^3 u = du_1 du_2 du_3 \).
The vector derivative, the projection of the gradient onto the volume at the point of integration (also called the tangent space), now has three components
\begin{dmath}\label{eqn:volumeintegral:200}
\boldpartial
=
\sum_i \Bx^i (\Bx_i \cdot \spacegrad)
=
\Bx^1 \PD{u_1}{}
+
\Bx^2 \PD{u_2}{}
+
\Bx^3 \PD{u_3}{}
\equiv
\Bx^1 \partial_1
+
\Bx^2 \partial_2
+
\Bx^3 \partial_3.
\end{dmath}

The volume integral specialization of \cref{dfn:fundamentalTheoremOfCalculus:240} can now be stated

\makedefinition{Multivector volume integral.}{dfn:volumeintegral:100}{
Given an connected volume \( V \) parameterized by two parameters, and multivector functions \( F, G \), we define the volume integral as
\begin{equation*}
\int_V F d^3\Bx \lrboldpartial G
\equiv
\int_V \lr{ F d^3\Bx \lboldpartial} G
+
\int_V F d^3\Bx \lr{ \rboldpartial G },
\end{equation*}
where the three parameter differential form \( d^3 \Bx = d^3 u\, \Bx_1 \wedge \Bx_2 \wedge \Bx_3, d^3 u = du_1 du_2 du_3 \) varies over the volume, and \( \lrboldpartial \) acts on \( F, G \), but not the volume element \( d^2 \Bx \).
} % definition

The volume integral specialization of \cref{thm:fundamentalTheoremOfCalculus:1} is

\maketheorem{Multivector volume integral.}{thm:volumeintegral:100}{
Given an connected volume \( V \) parameterized by three parameters for which \( d\Bx_1, d\Bx_2, d\Bx_3 \) is a right handed triple, and multivector functions \( F, G \), a volume integral can be reduced to a surface integral as follows
\begin{equation*}
\int_V F d^3\Bx \lrboldpartial G
= \ointctrclockwise_{\partial V} F d^2\Bx G,
\end{equation*}
where \( \partial V \) is the boundary of the volume \( V \), and \( d^2 \Bx \) is the counterclockwise oriented area element on the boundary of the volume.
In \R{3} with \( d^3 \Bx = I dV \), \( d^2 \Bx = I \ncap dA \), this can be written
\begin{equation*}
\int_V dV F \lrboldpartial G
= \int_{\partial V} dA F \ncap G,
\end{equation*}
} % theorem
To see why this works, and define \( d^2 \Bx \) more precisely, we would first like to reduce the product of the volume element and the vector derivative
\begin{dmath}\label{eqn:volumeintegral:300}
d^3\Bx \boldpartial
=
d^3 u\, \lr{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } \lr{ \Bx^1 \partial_1 + \Bx^2 \partial_2 + \Bx^3 \partial_3 }.
\end{dmath}

Since all \( \Bx^i \) lie within \( \Span \setlr{ \Bx_1, \Bx_2, \Bx_3 } \), this multivector product has only a vector grade.  That is
\begin{dmath}\label{eqn:volumeintegral:320}
\lr{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } \Bx^i
=
\lr{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } \cdot \Bx^i
+
\cancel{ \lr{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } \wedge \Bx^i },
\end{dmath}
for all \( \Bx^i \).  These products reduces to
\begin{dmath}\label{eqn:volumeintegral:1621}
\begin{aligned}
\lr{ \Bx_2 \wedge \Bx_3 \wedge \Bx_1 } \Bx^1 &= \Bx_2 \wedge \Bx_3 \\
\lr{ \Bx_3 \wedge \Bx_1 \wedge \Bx_2 } \Bx^2 &= \Bx_3 \wedge \Bx_1 \\
\lr{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } \Bx^3 &= \Bx_1 \wedge \Bx_2.
\end{aligned}
\end{dmath}

Inserting \cref{eqn:volumeintegral:1621}
into the volume integral, we find
\begin{dmath}\label{eqn:volumeintegral:380}
\int_V F d^3\Bx \boldpartial G
=
\int_V \lr{ F d^3\Bx \lboldpartial} G
+
\int_V F d^3\Bx \lr{ \rboldpartial G }
=
\int_V d^3 u\, \lr{
   (\partial_1 F) \Bx_2 \wedge \Bx_3 G
   +
   (\partial_2 F) \Bx_3 \wedge \Bx_1 G
   +
   (\partial_3 F) \Bx_1 \wedge \Bx_2 G
}
+
\int_V d^3 u\, \lr{
   F \Bx_2 \wedge \Bx_3 (\partial_1 G)
   +
   F \Bx_3 \wedge \Bx_1 (\partial_2 G)
   +
   F \Bx_1 \wedge \Bx_2 (\partial_3 G)
}
=
\int_V d^3 u\, \lr{
   \partial_1 (F \Bx_2 \wedge \Bx_3 G)
   +
   \partial_2 (F \Bx_3 \wedge \Bx_1 G)
   +
   \partial_3 (F \Bx_1 \wedge \Bx_2 G)
}
-
\int_V d^3 u\, \lr{
   F (\partial_1 (\Bx_2 \wedge \Bx_3)) G
   +
   F (\partial_2 (\Bx_3 \wedge \Bx_1)) G
   +
   F (\partial_3 (\Bx_1 \wedge \Bx_2)) G
}
=
\int_V d^3 u\, \lr{
   \partial_1 (F \Bx_2 \wedge \Bx_3 G)
   +
   \partial_2 (F \Bx_3 \wedge \Bx_1 G)
   +
   \partial_3 (F \Bx_1 \wedge \Bx_2 G)
}
-
\int_V d^3 u\, F
\lr{
   \partial_1 (\Bx_2 \wedge \Bx_3)
   +
   \partial_2 (\Bx_3 \wedge \Bx_1)
   +
   \partial_3 (\Bx_1 \wedge \Bx_2)
}
G
.
\end{dmath}

The sum within the second integral is
\begin{dmath}\label{eqn:volumeintegral:1111}
\begin{aligned}
\sum_{i = 1}^3 \partial_i \lr{ I_k \cdot \Bx^i }
&=
\partial_3 \lr{ (\Bx_1 \wedge \Bx_2 \wedge \Bx_3) \cdot \Bx^3 }
+
\partial_1 \lr{ (\Bx_2 \wedge \Bx_3 \wedge \Bx_1) \cdot \Bx^1 }
+
\partial_2 \lr{ (\Bx_3 \wedge \Bx_1 \wedge \Bx_2) \cdot \Bx^2 } \\
&=
\partial_3 \lr{ \Bx_1 \wedge \Bx_2 }
+
\partial_1 \lr{ \Bx_2 \wedge \Bx_3 }
+
\partial_2 \lr{ \Bx_3 \wedge \Bx_1 } \\
&=
         (\partial_3 \Bx_1) \wedge \Bx_2 + \Bx_1 \wedge (\partial_3 \Bx_2) \\
&\quad + (\partial_1 \Bx_2) \wedge \Bx_3 + \Bx_2 \wedge (\partial_1 \Bx_3) \\
&\quad + (\partial_2 \Bx_3) \wedge \Bx_1 + \Bx_3 \wedge (\partial_2 \Bx_1) \\
&=
\Bx_2 \wedge \lr{ - \partial_3 \Bx_1 + \partial_1 \Bx_3 }
+
\Bx_3 \wedge \lr{ - \partial_1 \Bx_2 + \partial_2 \Bx_1 }
+
\Bx_1 \wedge \lr{ - \partial_2 \Bx_3 + \partial_3 \Bx_2 } \\
&=
\Bx_2 \wedge \lr{ - \frac{\partial^2 \Bx}{\partial_3 \partial_1} + \frac{\partial^2 \Bx}{\partial_1 \partial_3} }
+
\Bx_3 \wedge \lr{ - \frac{\partial^2 \Bx}{\partial_1 \partial_2} + \frac{\partial^2 \Bx}{\partial_2 \partial_1} }
+
\Bx_1 \wedge \lr{ - \frac{\partial^2 \Bx}{\partial_2 \partial_3} + \frac{\partial^2 \Bx}{\partial_3 \partial_2} },
\end{aligned}
\end{dmath}
which is zero by equality of mixed partials.
This leaves three perfect differentials, which can integrated separately, giving
\begin{dmath}\label{eqn:volumeintegral:400}
\int_V F d^3\Bx \boldpartial G
=
\int du_2 du_3
\evalbar{ \lr{ F \Bx_2 \wedge \Bx_3 G } }{\Delta u_1}
+
\int du_3 du_1
\evalbar{ \lr{ F \Bx_3 \wedge \Bx_1 G } }{\Delta u_2}
+
\int du_1 du_2
\evalbar{ \lr{ F \Bx_1 \wedge \Bx_2 G } }{\Delta u_3}
=
\int
\evalbar{ \lr{ F d\Bx_2 \wedge d\Bx_3 G } }{\Delta u_1}
+
\int
\evalbar{ \lr{ F d\Bx_3 \wedge d\Bx_1 G } }{\Delta u_2}
+
\int
\evalbar{ \lr{ F d\Bx_1 \wedge d\Bx_2 G } }{\Delta u_3}.
\end{dmath}

This proves the theorem from an algebraic point of view.
With the aid of a geometrical model, such as that of \cref{fig:differentialVolume:differentialVolumeFig}, if
assuming that \( d\Bx_1, d\Bx_2, d\Bx_3 \) is a right handed triple).
it is possible to convince oneself that the two parameter integrands describe an integral over a counterclockwise oriented surface (
\imageTwoFigures{../figures/GAelectrodynamics/differentialVolumeFig1}{../figures/GAelectrodynamics/differentialVolumeFig2}{Differential surface of a volume.}{fig:differentialVolume:differentialVolumeFig}{scale=0.05}

We obtain the RHS of \cref{thm:volumeintegral:100} if we
introduce a mnemonic for the bounding oriented surface of the volume
\begin{dmath}\label{eqn:volumeintegral:1641}
d^2 \Bx \equiv d\Bx_1 \wedge d\Bx_2 + d\Bx_2 \wedge d\Bx_3 + d\Bx_3 \wedge d\Bx_1,
\end{dmath}
where it is implied that each component of this area element and anything that it is multiplied with is evaluated on the boundaries of the integration volume (for the parameter omitted) as detailed explicitly in
\cref{eqn:volumeintegral:400}.

\subsubsection{Three parameter Stokes' theorem.}

Three special cases of \cref{thm:volumeintegral:100} can be obtained by integrating scalar, vector or bivector functions over the volume, as follows

\maketheorem{Volume integral of scalar function (Stokes').}{thm:volumeintegral:420}{
Given a scalar function \( f(\Bx) \) its volume integral is given by
\begin{equation*}
\int_V d^3 \Bx \cdot \boldpartial f =
\int_V d^3 \Bx \cdot \spacegrad f = \ointctrclockwise_{\partial V} d^2\Bx f.
\end{equation*}
In \R{3} this can be written as
\begin{equation*}
\int_V dV \spacegrad f = \int_{\partial V} dA \ncap f
\end{equation*}
where \( \ncap \) is the outwards normal specified by \( d^2 \Bx = I \ncap dA, \) and \( d^3 \Bx = I dV \).
} % theorem

\maketheorem{Volume integral of vector function (Stokes').}{thm:volumeintegral:1661}{
Given a vector function \( \Bf(\Bx) \) the volume
integral of the (bivector) curl is related to a surface integral by
\begin{equation*}
\int_V d^3 \Bx \cdot \lr{ \boldpartial \wedge \Bf } =
\int_V d^3 \Bx \cdot \lr{ \spacegrad \wedge \Bf } = \ointctrclockwise_{\partial V} d^2\Bx \cdot \Bf.
\end{equation*}
In \R{3} this can be written as
\begin{equation*}
\int_V dV \spacegrad \cross \Bf = \int_{\partial V} dA \ncap \cross \Bf,
\end{equation*}
or with a duality transformation \( \Bf = I B \), where \( B \) is a bivector
\begin{equation*}
\int_V dV \spacegrad \cdot B = \int_{\partial V} dA \ncap \cdot \Bf,
\end{equation*}
where \( \ncap \) is the outwards normal specified by \( d^2 \Bx = I \ncap dA, \) and \( d^3 \Bx = I dV \).
} % theorem

\maketheorem{Volume integral of bivector function (Stokes', divergence).}{thm:volumeintegral:1681}{
Given a bivector function \( B(\Bx) \), the volume
integral of the (trivector) curl is related to a surface integral by
\begin{equation*}
\int_V d^3 \Bx \cdot \lr{ \boldpartial \wedge B } =
\int_V d^3 \Bx \cdot \lr{ \spacegrad \wedge B } = \ointctrclockwise_{\partial V} d^2\Bx \cdot B.
\end{equation*}
In \R{3} this can be written as
\begin{equation*}
\int_V dV \spacegrad \wedge B = \int_{\partial V} dA \ncap \wedge B,
\end{equation*}
or, making a duality transformation \( B(\Bx) = I \Bf(\Bx) \), where \( \Bf \) is a vector, by
\begin{equation*}
\int_V dV \spacegrad \cdot \Bf = \int_{\partial V} dA \ncap \cdot \Bf,
\end{equation*}
where \( \ncap \) is the outwards normal specified by \( d^2 \Bx = I \ncap dA, \) and \( d^3 \Bx = I dV \).
} % theorem

\subsubsection{Divergence theorem.}

Observe that for \R{3} we there are dot product relations in each of
\cref{thm:volumeintegral:420},
\cref{thm:volumeintegral:1661} and
\cref{thm:volumeintegral:1681} which can be summarized as
\maketheorem{Divergence theorem.}{thm:volumeintegral:2661}{
Given an \R{3} multivector \( M \) containing only grades 0,1, or 2
\begin{equation*}
\int_V dV \spacegrad \cdot M = \int_{\partial V} dA \ncap \cdot M,
\end{equation*}
where \( \ncap \) is the outwards normal to the surface bounding \( V \).
} % theorem

%}
%\EndNoBibArticle
   \subsection{Multivector Fourier transform and phasors.}
%
%
%{
\label{fourier}
It will often be convient to utilize time harmonic (frequency domain) representations.
%of \cref{eqn:greensFunctionOverview:200}.
This can be achieved by utilizing Fourier transform pairs or with a phasor representation.

We may define Fourier transform pairs of multivector fields and sources in the conventional fashion

\makedefinition{Multivector Fourier transform pairs}{dfn:greensFunctionOverview:280}{
The Fourier transform pair for a multivector valued function \( f(\Bx, t) \) will be written as
\begin{equation*}
\begin{aligned}
f(\Bx, t) &= \int f_\omega(\Bx) e^{j \omega t} d\omega \\
f_\omega(\Bx) &= \inv{2 \pi} \int f(\Bx, t) e^{-j \omega t} dt,
\end{aligned}
\end{equation*}
where \( j \) is an arbitrary scalar imaginary that commutes with all multivectors.
} % definition

In these transform pairs, the imaginary \( j \) need not be represented by any geometrical imaginary such as \( \Be_{12} \).
In particular, we need not assume that the represention of \( j \) is the
\R{3} pseudoscalar \( I \), despite the fact that \( I \) does commute with all \R{3} multivectors.
We wish to have the freedom to
assume that non-geometric real and imaginary operations can be performed without picking or leaving out any specific grade pseudoscalar components of the multivector fields or sources, so we won't impose any a-priori restrictions on the representations of \( j \).
In particular, this provides the freedom to utilize phasor (fixed frequency) representions of our multivector functions.
%Introduction of yet another imaginary quantity in a geometric algebra context where we have so many to pick it somewhat unfortunate, but it allows us to apply Fourier transform techniques without worry about the non-commutative effects that might have to be considered should we choose to use a geometric imaginary to represent the frequency dependency.
We will use the engineering convention for our
phasor representations, where assuming a complex exponential time dependence of the following form is assumed

\makedefinition{Multivector phasor representation.}{dfn:greensFunctionOverview:300}{
The phasor representation \( f(\Bx) \) of a multivector valued (real) function \( f(\Bx, t) \) is defined implicitly as
\begin{equation*}
f(\Bx, t) = \Real\lr{ f(\Bx) e^{j \omega t} },
\end{equation*}
where \( j \) is an arbitrary scalar imaginary that commutes with all multivectors.
} % definition

The complex valued multivector \( f(\Bx) \) is still generated from the real Euclidean basis for \R{3}, so
there will be
no reason to introduce complex inner products spaces into the mix.

The reader must take care when reading any literature that utilizes Fourier transforms or phasor representation, since the conventions vary.
In particular the physics representation of a phasor typically uses the opposite sign convention
\( f(\Bx, t) = \Real\lr{ f(\Bx) e^{-i \omega t }} \), which toggles the sign of all the imaginaries in derived results.
%}
   \subsection{Green's functions.}
%
%
%{
\label{chap:GreensFunctions}

\subsubsection{Motivation.}

%We will now introduce Green's functions, which provide a general method of solving many of the linear differential equations that will be encountered in electromagnetism.

\input{greensIntro.tex}

\paragraph{Time domain problems in electromagnetism}
 
Examples of the PDEs that we can apply Green's function techniques to include

\begin{subequations}
\label{eqn:greensFunctionOverview:200}
\begin{equation}\label{eqn:greensFunctionOverview:220}
\stgrad F(\Bx, t) = J(\Bx, t)
\end{equation}
\begin{equation}\label{eqn:greensFunctionOverview:162}
\lr{ \spacegrad^2 - \inv{c^2} \PDSq{t}{} } F(\Bx, t) =
\conjstgrad
\stgrad F(\Bx, t) =
B(\Bx, t).
\end{equation}
\end{subequations}

The reader is no doubt familiar with the wave equation (\cref{eqn:greensFunctionOverview:162}), where \( F \) is the waving function, and \( B \) is the forcing function.
Scalar and vector valued wave equations are
encountered in scalar and vector forms in conventional electromagnetism.
We will see multivector variations of the wave equation, so it should be assumed that \( F \) and \( B \) are multivector valued.

\Cref{eqn:greensFunctionOverview:220} is actually the geometric algebra form of Maxwell's equation (singular),
where \( F \) is a multivector with grades 1 and 2, and \( J \) is a multivector containing all the charge and current density contributions.
We will call the operator in \cref{eqn:greensFunctionOverview:220} the spacetime gradient\footnote{A slightly different operator is also called the spacetime gradient in STA (Space Time Algebra) \citep{doran2003gap}, which employs a non-Euclidean basis to generate a four dimensional relativistic geometric algebra.
Our spacetime gradient is related to the STA spacetime gradient by a constant factor.}.


Armed with Fourier transform or phasor representations, the frequency domain representations of
\cref{eqn:greensFunctionOverview:200} are found to be

\begin{subequations}
\label{eqn:greensFunctionOverview:320}
\begin{equation}\label{eqn:greensFunctionOverview:240}
\lr{ \spacegrad + j k } F(\Bx) = J(\Bx)
\end{equation}
\begin{equation}\label{eqn:greensFunctionOverview:260}
\lr{ \spacegrad^2 + k^2 } F(\Bx)
=
\lr{ \spacegrad - j k } \lr{ \spacegrad + j k } F(\Bx)
= B(\Bx),
\end{equation}
\end{subequations}

where \( k = \omega/c \), and any explicit frequency dependence in our transform pairs has been suppressed.
We will call these equations the first and second order Helmholtz equations respectively.
The first order equation applies a multivector differential operator to a multivector field, which must equal the multivector forcing function (the sources).

For statics problems (\( k = 0 \)), we may work with real fields and sources, dispensing with any need to take real parts.

%}
      \subsubsection{Green's function solutions.}
%
%
%{
\paragraph{Unbounded.}

The operators in \cref{eqn:greensFunctionOverview:200}, and \cref{eqn:greensFunctionOverview:320} all have a similar linear structure.
Abstracting that structure, all these problems have the form
\begin{dmath}\label{eqn:greensFunctionSolutions:340}
\LL F(\Bx) = J(\Bx),
\end{dmath}
where \( \LL \) is an operator formed from a linear combination of linear operators \( 1, \spacegrad, \spacegrad^2, \partial_t, \partial_{tt} \).

Given the linear structure of the PDE that we wish to solve, it makes sense to assume that the solutions also have a linear structure.
The most general such solution we can assume has the form

\begin{dmath}\label{eqn:greensFunctionSolutions:360}
F(\Bx, t) = \int G(\Bx, \Bx' ; t, t') J(\Bx', t') dV' dt' + F_0(\Bx, t),
\end{dmath}
where \( F_0(\Bx, t) \) is any solution to the equivalent homogeneous equation \( \LL F_0 = 0 \), and \( G(\Bx, \Bx' ; t, t') \) is the Green's function (to be determined) associated with \cref{eqn:greensFunctionSolutions:340}.
Operating on the presumed solution
\cref{eqn:greensFunctionSolutions:360} with \( \LL \) yields
\begin{dmath}\label{eqn:greensFunctionSolutions:380}
J(\Bx, t) = \LL F(\Bx, t) = \LL\lr{
\int G(\Bx, \Bx' ; t, t') J(\Bx', t') dV' dt' + F_0(\Bx, t) }
=
\int \lr{ \LL G(\Bx, \Bx'; t, t') } J(\Bx', t') dV' dt',
\end{dmath}
which shows that we require the Green's function to have delta function semantics satisfying
\begin{dmath}\label{eqn:greensFunctionSolutions:400}
\LL G(\Bx, \Bx' ; t, t') = \delta(\Bx - \Bx') \delta(t - t').
\end{dmath}

The scalar valued Green's functions for the Laplacian and the (2nd order) Helmholtz equations are well known.
The Green's functions for the spacetime gradient and the 1st order Helmholtz equation (which is just the gradient when \( k = 0 \)) are multivector valued and will be derived here.

\paragraph{Green's theorem.}

When the presumed solution is a superposition of only states in a bounded region
then life gets a bit more interesting.  For instance, consider a problem for which the differential operator is a function of space only, with a presumed solution such as
\begin{dmath}\label{eqn:greensFunctionSolutions:200}
F(\Bx) = \int_V dV' B(\Bx') G(\Bx, \Bx') + F_0(\Bx),
\end{dmath}
then life gets a bit more interesting.
This sort of problem requires different treatment for operators that are first and second order in the gradient.

For the second order problems, we require Green's theorem, which must be generalized slightly for use with multivector fields.

The basic idea is that we can relate the Laplacian's of the Green's function and the field
\( F(\Bx') \lr{ (\spacegrad')^2 G(\Bx, \Bx') } = G(\Bx, \Bx') \lr{ (\spacegrad')^2 F(\Bx')} + \cdots \).
That relationship can be expressed as the integral of an antisymmetric sandwich of the two functions

\maketheorem{Green's theorem}{thm:gradientGreensFunctionEuclidean:220}{
Given a multivector function \( F \) and a scalar function \( G \)
\begin{equation*}
\int_V \lr{ F \spacegrad^2 G - G \spacegrad^2 F } dV = \int_{\partial V} \lr{ F \ncap \cdot \spacegrad G - G \ncap \cdot \spacegrad F },
\end{equation*}
where \( \partial V \) is the boundary of the volume \( V \).
} % theorem

A straightforward, but perhaps inelligant way of proving this theorem is to expand the antisymmetric product in coordinates
\begin{dmath}\label{eqn:greensFunctionSolutions:260}
F \spacegrad^2 G - G \spacegrad^2 F
=
\sum_k F \partial_k \partial_k G - G \partial_k \partial_k F
=
\sum_k \partial_k \lr{
F \partial_k G - G \partial_k F
}
-
(\partial_k F)(\partial_k G) + (\partial_k G)(\partial_k F).
\end{dmath}

Since \( G \) is a scalar, the last two terms cancel, and we can integrate
\begin{dmath}\label{eqn:greensFunctionSolutions:280}
\int_V \lr{ F \spacegrad^2 G - G \spacegrad^2 F } dV
=
\sum_k \int_V \partial_k \lr{ F \partial_k G - G \partial_k F }.
\end{dmath}

Each integral above involves one component of the gradient.
From
%the fundamental theorem of geometric calculus
\cref{thm:fundamentalTheoremOfCalculus:1}
we know that
\begin{dmath}\label{eqn:greensFunctionSolutions:300}
\int_V \spacegrad Q dV = \int_{\partial V} \ncap Q dA,
\end{dmath}
for any multivector \( Q \).
Equating components gives
\begin{dmath}\label{eqn:greensFunctionSolutions:460}
\int_V \partial_k Q dV = \int_{\partial V} \ncap \cdot \Be_k Q dA,
\end{dmath}
which can be substituted into \cref{eqn:greensFunctionSolutions:280} to find
\begin{dmath}\label{eqn:greensFunctionSolutions:480}
\int_V \lr{ F \spacegrad^2 G - G \spacegrad^2 F } dV
=
\sum_k \int_{\partial V} \ncap \cdot \Be_k \lr{ F \partial_k G - G \partial_k F } dA
=
\int_{\partial V} \lr{ F (\ncap \cdot \spacegrad) G - G (\ncap \cdot \spacegrad) F } dA,
\end{dmath}
which proves the theorem.

\paragraph{Bounded solutions to first order problems.}

For first order problems we will need an intermediate result similar to Green's theorem.

\makelemma{Normal relations for a gradient sandwich.}{lemma:greensFunctionOverview:420}{
Given multivector functions \( F(\Bx'), G(\Bx, \Bx') \), and a gradient \( \spacegrad' \) acting bidirectionally on functions of \( \Bx' \), we have
\begin{equation*}
- \int_V \lr{ G(\Bx, \Bx') \lspacegrad' } F(\Bx') dV'
=
\int_V G(\Bx, \Bx') \lr{ \rspacegrad' F(\Bx') } dV'
-
\int_{\partial V} G(\Bx, \Bx') \ncap' F(\Bx') dA'.
\end{equation*}
} % lemma

This follows directly from \cref{thm:fundamentalTheoremOfCalculus:1}
\begin{dmath}\label{eqn:greensFunctionSolutions:440}
\int_{\partial V} G(\Bx, \Bx') \ncap' F(\Bx') dA'
=
\int_V G(\Bx, \Bx') \lrspacegrad' F(\Bx') dV'
=
\int_V \lr{ G(\Bx, \Bx') \lspacegrad' } F(\Bx') dV'
+
\int_V G(\Bx, \Bx') \lr{ \rspacegrad' F(\Bx') } dV',
\end{dmath}
which can be rearranged to prove \cref{lemma:greensFunctionOverview:420}.

%}
      \subsubsection{Helmholtz equation.}
%
%
%{
\paragraph{Unbounded superposition solutions for the Helmholtz equation.}

The specialization of \cref{eqn:greensFunctionSolutions:400} to the Helmholtz equation \cref{eqn:greensFunctionOverview:260} is
\begin{dmath}\label{eqn:greensFunctionHelmholtz:420}
\lr{ \spacegrad^2 + k^2 } G(\Bx, \Bx') = \delta(\Bx - \Bx').
\end{dmath}

While it is possible \citep{schwinger1998classical} to derive the Green's function using Fourier transform techniques, we will state the result instead, which is well known

\maketheorem{Green's function for the Helmholtz operator.}{thm:gradientGreensFunctionEuclidean:3}{
The advancing (causal), and the receding (acausal) Green's functions satisfying
\cref{eqn:greensFunctionHelmholtz:420} are respectively
\begin{equation*}
\begin{aligned}
G_{\textrm{adv}}(\Bx, \Bx') &= -\frac{e^{-j k \Norm{ \Bx - \Bx' } }}{ 4 \pi \Norm{\Bx - \Bx'}} \\
G_{\textrm{rec}}(\Bx, \Bx') &= -\frac{e^{j k \Norm{ \Bx - \Bx' } }}{ 4 \pi \Norm{\Bx - \Bx'}}.
\end{aligned}
\end{equation*}
} % theorem

We will use the advancing (causal) Green's function, and refer to this function as \( G(\Bx, \Bx') \) without any subscript.
Because it may not be obvious that these
Green's function representations are valid in a multivector context, a demonstration of this fact can be found in \cref{chap:helmholtzGreens}.

Observe that as a special case, the Helmholtz Green's function reduces to the Green's function for the Laplacian when \( k = 0 \)
\begin{dmath}\label{eqn:greensFunctionHelmholtz:80}
G(\Bx, \Bx') = -\inv{ 4 \pi \Norm{\Bx - \Bx'}}.
\end{dmath}

\paragraph{Bounded superposition solutions for the Helmholtz equation.}

For our application of
\cref{thm:gradientGreensFunctionEuclidean:3} to the Helmholtz problem, we
are actually interested in a antisymmetric sandwich of the Helmholtz operator by the function \( F \) and the scalar (Green's) function \( G \), but
that reduces to an asymmetric sandwich of our functions around the Laplacian
\begin{dmath}\label{eqn:greensFunctionHelmholtz:240}
F \lr{ \spacegrad^2 + k^2 } G - G \lr{ \spacegrad^2 + k^2 } F
=
F \spacegrad^2 G + \cancel{F k^2 G} - G \spacegrad^2 F - \cancel{G k^2 F}
=
F \spacegrad^2 G - G \spacegrad^2 F,
\end{dmath}
so
\begin{dmath}\label{eqn:greensFunctionHelmholtz:380}
\int_V F(\Bx') \lr{ (\spacegrad')^2 + k^2 } G(\Bx, \Bx')
=
\int_V G(\Bx, \Bx') \lr{ (\spacegrad')^2 + k^2} F(\Bx') dV'
+
\int_{\partial V} \lr{ F(\Bx') (\ncap' \cdot \spacegrad') G(\Bx, \Bx') - G(\Bx, \Bx') (\ncap' \cdot \spacegrad') F(\Bx') } dA'.
\end{dmath}

This shows that if we assume the Green's function satisfies
the delta function condition
\cref{eqn:greensFunctionHelmholtz:420}
, then the general solution of
\cref{eqn:greensFunctionOverview:260} is
formed from a bounded superposition of sources is
\boxedEquation{eqn:gradientGreensFunctionEuclidean:400}{
\begin{aligned}
F(\Bx) &=
\int_V G(\Bx, \Bx') B( \Bx' ) dV' \\
&+
\int_{\partial V} \lr{
 G(\Bx, \Bx') (\ncap' \cdot \spacegrad') F(\Bx')
-F(\Bx') (\ncap' \cdot \spacegrad') G(\Bx, \Bx')
} dA'.
\end{aligned}
}

We are also free to add in any specific solution \( F_0(\Bx) \) that satisfies the
homogeneous Helmholtz equation.
There is also freedom to add any solution of the homogeneous Helmholtz equation to the Green's function itself, so it is not unique.
For a bounded superposition we generally desire that the solution \( F \) and its normal derivative, or the Green's function \( G \) (and it's normal derivative) or an appropriate combination of the two are zero on the boundary, so that the surface integral is killed.

%}
      \subsubsection{First order Helmholtz equation.}
%
%
%{

The specialization of \cref{eqn:greensFunctionSolutions:400} to the first order Helmholtz equation \cref{eqn:greensFunctionOverview:240} is
\begin{dmath}\label{eqn:greensFunctionFirstOrderHelmholtz:700}
\lr{ \spacegrad + j k } G(\Bx, \Bx')  = \delta(\Bx - \Bx').
\end{dmath}

This Green's function is multivector valued

\maketheorem{Green's function for the first order Helmholtz operator.}{thm:gradientGreensFunctionEuclidean:720}{
The Green's function satisfying
\begin{equation*}
\lr{ \rspacegrad + j k } G(\Bx, \Bx') = G(\Bx, \Bx') \lr{ -\lspacegrad' + j k } = \delta(\Bx - \Bx'),
\end{equation*}
is
\begin{equation*}
G(\Bx, \Bx') = \frac{e^{-j k r}}{4 \pi r} \lr{ j k \lr{ 1 + \rcap } + \frac{\rcap}{r} },
\end{equation*}
where \( \Br = \Bx - \Bx', r = \Norm{\Br} \) and \( \rcap = \Br/r \).
} % theorem

A special but important case is the \( k = 0 \) condition, which provides the
Green's function for the gradient, which is vector valued
\begin{equation}\label{eqn:greensFunctionFirstOrderHelmholtz:900}
G(\Bx, \Bx' ; k = 0) = \inv{4 \pi} \frac{\rcap}{r^2}.
\end{equation}

If we denote the (advanced) Green's function for the 2nd order Helmholtz operator
\cref{thm:gradientGreensFunctionEuclidean:3}
as \( \phi(\Bx, \Bx') \), we must have
\begin{equation}\label{eqn:greensFunctionFirstOrderHelmholtz:740}
\lr{ \rspacegrad + j k } G(\Bx, \Bx') = \delta(\Bx - \Bx') =
\lr{ \rspacegrad + j k } \lr{ \rspacegrad - j k } \phi(\Bx, \Bx'),
\end{equation}
we see that the Green's function is given by
\begin{dmath}\label{eqn:greensFunctionFirstOrderHelmholtz:760}
G(\Bx, \Bx')
=
\lr{ \rspacegrad - j k } \phi(\Bx, \Bx').
\end{dmath}

This can be computed directly
\begin{dmath}\label{eqn:greensFunctionFirstOrderHelmholtz:780}
G(\Bx, \Bx')
=
\lr{ \rspacegrad - j k } \lr{ -\frac{e^{-j k r}}{4 \pi r} }
=
\lr{ \rcap \PD{r}{} -j k } \lr{ -\frac{e^{-j k r}}{4 \pi r} }
=
\frac{-e^{-j k r}}{4 \pi}
\lr{
\rcap \lr{ -\frac{j k}{r} - \inv{ r^2 } } - \frac{j k}{r}
}
=
\frac{e^{-j k r}}{4 \pi}
\lr{
j k \lr{ 1 + \rcap } + \frac{\rcap}{r}
},
\end{dmath}
as claimed.
Observe that since \( \phi \) is scalar valued, we can also rewrite
\cref{eqn:greensFunctionFirstOrderHelmholtz:760} in terms of a right acting operator
\begin{dmath}\label{eqn:greensFunctionFirstOrderHelmholtz:800}
G(\Bx, \Bx')
=
\phi(\Bx, \Bx')
\lr{ \lspacegrad - j k }
=
\phi(\Bx, \Bx')
\lr{ -\lspacegrad' - j k },
\end{dmath}
so
\begin{equation}\label{eqn:greensFunctionFirstOrderHelmholtz:820}
G(\Bx, \Bx') \lr{ -\lspacegrad' + j k } =
\phi(\Bx, \Bx') \lr{ (\lspacegrad')^2 + k^2 }
=
\delta(\Bx - \Bx').
\end{equation}

This is relavant for bounded superposition states, which we will discuss next now that the proof of
\cref{thm:gradientGreensFunctionEuclidean:720} is complete.
In particular addition of
\( \int_V G(\Bx, \Bx') j k F(\Bx') dV' \) to both sides of \cref{lemma:greensFunctionOverview:420} gives
\begin{dmath}\label{eqn:greensFunctionFirstOrderHelmholtz:860}
\begin{aligned}
\int_V \lr{ G(\Bx, \Bx') \lr{ -\lspacegrad' + j k } } F(\Bx') dV'
&=
\int_V G(\Bx, \Bx') \lr{ \lr{ \rspacegrad' + j k } F(\Bx') } dV' \\
&-
\int_{\partial V} G(\Bx, \Bx') \ncap' F(\Bx') dA'.
\end{aligned}
\end{dmath}

Utilizing \cref{thm:gradientGreensFunctionEuclidean:720}, and substituting \( J(\Bx') \)
from \cref{eqn:greensFunctionOverview:240},
we find that one solution to the first order Helmholtz equation is
\begin{dmath}\label{eqn:greensFunctionFirstOrderHelmholtz:880}
F(\Bx)
=
\int_V G(\Bx, \Bx') J(\Bx') dV'
-
\int_{\partial V} G(\Bx, \Bx') \ncap' F(\Bx') dA'.
\end{dmath}

We are free to
add any specific solution \( F_0 \) that satisfies the homogeneous equation \( \lr{ \spacegrad + j k } F_0 = 0 \).
%}
      \subsubsection{Spacetime gradient.}
%
%
%{

We want to find the Green's function that solves spacetime gradient equations of the form \cref{eqn:greensFunctionOverview:220}.
For the wave equation operator, it is helpful to introduce a d'Lambertian operator, defined as follows.

\makedefinition{d'Lambertian (wave equation) operator.}{dfn:continuity:120}{
Let
\begin{equation*}
\dLambertian =
\conjstgrad
\stgrad
=
\spacegrad^2 - \inv{c^2} \PDSq{t}{}.
\end{equation*}
} % definition

We will be able to derive the Green's function for the spacetime gradient from the Green's function for the d'Lambertian.  The Green's function for the spacetime gradient is multivector valued and given by the following.
\maketheorem{Green's function for the spacetime gradient.}{thm:greensFunctionSpacetimeGradient:120}{
The Green's function for the spacetime gradient, satisfying
\begin{equation*}
\stgrad G(\Bx - \Bx', t - t') = \delta(\Bx - \Bx') \delta(t - t'),
\end{equation*}
is
\begin{equation*}
G(\Bx - \Bx', t - t')
=
\inv{4\pi} \lr{
- \frac{\rcap}{r^2} \PD{r}{}
+ \frac{\rcap}{r}
+ \inv{c r} \PD{t}{}
}
\delta( -r/c + t - t' ),
\end{equation*}
where \( \Br = \Bx - \Bx', r = \Norm{\Br} \) and \( \rcap = \Br/r \).
} % theorem

With the help of \cref{eqn:derivativeOfDeltaFunction:140}
it is possible to further evaluate the delta function derivatives, however, we will defer doing so until we are ready to apply this Green's
function in a convolution integral to solve Maxwell's equation.

To prove this result, let \( \phi(\Bx - \Bx', t - t') \) be the retarded time (causal)
Green's function for the wave equation, satisfying
\begin{dmath}\label{eqn:greensFunctionSpacetimeGradient:40}
\dLambertian
\phi(\Bx - \Bx', t - t')
=
\stgrad
\conjstgrad
\phi(\Bx - \Bx', t - t')
= \delta(\Bx - \Bx') \delta(t - t').
\end{dmath}

This function has the value
\begin{dmath}\label{eqn:greensFunctionSpacetimeGradient:60}
\phi(\Br, t - t')
=
-\inv{4 \pi r} \delta( -r/c + t - t' ),
\end{dmath}
where \( \Br = \Bx - \Bx', r = \Norm{\Br} \).  Derivations of this Green's function, and it's acausal advanced time friend, can be found in
\citep{schwinger1998classical}, \citep{jackson1975cew}, and use the usual Fourier transform and contour integration tricks.

Comparing \cref{eqn:greensFunctionSpacetimeGradient:40} to the defining statement of \cref{thm:greensFunctionSpacetimeGradient:120}, we see that the spacetime gradient Green's function is given by
\begin{dmath}\label{eqn:greensFunctionSpacetimeGradient:80}
G(\Bx - \Bx', t - t')
=
\conjstgrad \phi(\Br, t - t')
=
\lr{ \rcap \PD{r}{} - \inv{c} \PD{t}{} } \phi(\Br, t - t'),
\end{dmath}
where \( \rcap = \Br/r \).  Evaluating the derivatives gives
\begin{dmath}\label{eqn:greensFunctionSpacetimeGradient:100}
G(\Br, t - t')
=
-\inv{4\pi} \lr{ \rcap \PD{r}{} - \inv{c} \PD{t}{} } \frac{ \delta( -r/c + t - t' ) }{r}
=
-\inv{4\pi} \lr{
\frac{\rcap}{r} \PD{r}{} \delta( -r/c + t - t' )
- \frac{\rcap}{r^2} \delta( -r/c + t - t' )
- \inv{c r} \PD{t}{} \delta( -r/c + t - t' )
},
\end{dmath}
which completes the proof after some sign cancellation and minor rearrangement.
%}
   \subsection{Helmholtz theorem.}
%
%
In conventional electromagnetism Maxwell's equations are posed in terms of separate divergence and curl equations.  It is therefore desirable to show that the divergence and curl of a function and it's normal characteristics on the boundary of an integraion volume determine that function uniquely.  This is known as the Helmholtz theorem
\maketheorem{Helmholtz first theorem.}{thm:helmholtzDerviationMultivectorStatement:1}{
A vector \( \BM \) is uniquely determined by its
divergence
\begin{equation*}
\spacegrad \cdot \BM = s,
\end{equation*}
and curl
\begin{equation*}
\spacegrad \cross \BM = \BC,
\end{equation*}
and its value
over the boundary.
} % theorem

%It could be argued that Helmholtz's theorem is irrelavent when using the GA formalism, since we consolidate the separate divergence and curl equations into one gradient operator.
%We include a proof here regardless, since it can be performed in a compact and interesting fashion using
%%the fundamental theorem of geometric calculus
%\cref{thm:fundamentalTheoremOfCalculus:1}.
%
%
%{
The conventional proof of Helmholtz's theorem uses the Green's function for the (second order) Helmholtz operator.
Armed with a vector valued Green's function for the gradient, a first order proof is also possible.
As illustrations of the geometric integration theory developed in this chapter, both
strategies will be applied here to this problem.

In either case, we start by forming an even grade multivector (gradient) equation containing both the dot and cross product contributions
\begin{equation}\label{eqn:helmholtzDerviationMultivectorSolution:60}
\spacegrad \BM
= \spacegrad \cdot \BM + I \spacegrad \cross \BM
= s + I \BC.
\end{equation}

\paragraph{First order proof.}

For the first order case, we
perform a grade one selection of \cref{lemma:greensFunctionOverview:420}, setting
\( F = \BM \) where \( G \) is the Green's function for the gradient given by
\cref{eqn:greensFunctionFirstOrderHelmholtz:900}.  The proof follows directly

\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:820}
M(\Bx)
=
- \int_V \lr{ G(\Bx, \Bx') \lspacegrad' } \BM(\Bx') dV'
=
\int_V \gpgradeone{G(\Bx, \Bx') \lr{ \rspacegrad' \BM(\Bx') }} dV'
-
\int_{\partial V} \gpgradeone{ G(\Bx, \Bx') \ncap' \BM(\Bx') } dA'
=
\int_V
\inv{4 \pi \Norm{\Bx - \Bx'}^3 }
\gpgradeone{ (\Bx - \Bx') \lr{ s(\Bx') + I \BC(\Bx') }} dV'
-
\int_{\partial V}
\inv{4 \pi \Norm{\Bx - \Bx'}^3 }
\gpgradeone{ (\Bx - \Bx') \ncap' \BM(\Bx') } dA'
=
\int_V
\inv{4 \pi \Norm{\Bx - \Bx'}^3 }
\lr{ (\Bx - \Bx') s(\Bx') - (\Bx - \Bx') \cross \BC(\Bx') } dV'
-
\int_{\partial V}
\inv{4 \pi \Norm{\Bx - \Bx'}^3 }
\gpgradeone{ (\Bx - \Bx') \ncap' \BM(\Bx') } dA'.
\end{dmath}
If \( \BM \) is well behaved enough that the boundary integral vanishes on an infinite surface, we see that \( \BM \) is completely specified by the divergence and the curl.
In general, the divergence and the curl, must also be supplemented by the the value of vector valued function on the boundary.

Observe that the boundary integral has a particularly simple form for a spherical surface or radius \( R \) centered on \( \Bx' \).
Switching to spherical coordinates \( \Br = \Bx' - \Bx = R\, \rcap(\theta, \phi) \) where \( \rcap = (\Bx' - \Bx)/\Norm{\Bx' - \Bx} \) is the outwards normal, we have
\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:840}
-
\int_{\partial V}
\inv{4 \pi \Norm{\Bx - \Bx'}^3 }
\gpgradeone{ (\Bx - \Bx') \ncap' \BM(\Bx') } dA'
=
\int_{\partial V}
\frac{\BM(\Bx')}{4 \pi \Norm{\Bx - \Bx'}^2 } dA'
= \inv{4\pi} \int_{\theta = 0}^\pi \int_{\phi = 0}^{2 \pi} \BM(R, \theta, \phi) \sin\theta d\theta d\phi.
\end{dmath}
This is an average of \( \BM \) over the surface of the radius-\(R\) sphere surrounding the point \( \Bx \) where the field \( \BM \) is evaluated.

\paragraph{Second order proof.}

%Observe that the Laplacian of \( \BM \) is vector valued
%
%\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:760}
%\spacegrad^2 \BM = \spacegrad s + I \spacegrad \BC.
%\end{dmath}
%
%This means that \( \spacegrad \BC \) must be a bivector \( \spacegrad \BC = \spacegrad \wedge \BC \), or that \( \BC \) has zero divergence
%
%\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:780}
%\spacegrad \cdot \BC = 0.
%\end{dmath}

Again, we use \cref{eqn:helmholtzDerviationMultivectorSolution:60}
to discover the relation between the vector \( \BM \) and its divergence and curl.
The vector \( \BM \) can be expressed at the point of interest as a convolution with the delta function at all other points in space
\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:80}
\BM(\Bx) = \int_V dV' \delta(\Bx - \Bx') \BM(\Bx').
\end{dmath}

The Laplacian representation of the delta function in \R{3} is
\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:100}
\delta(\Bx - \Bx') = -\inv{4\pi} \spacegrad^2 \inv{\Norm{\Bx - \Bx'}},
\end{dmath}
so \( \BM \) can be represented as the following convolution
\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:120}
\BM(\Bx) = -\inv{4\pi} \int_V dV' \spacegrad^2 \inv{\Norm{\Bx - \Bx'}} \BM(\Bx').
\end{dmath}

%As noted in \cref{eqn:helmholtzDerviationMultivector:460} the Laplacian of a vector can be factored as
%
%\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:140}
%\spacegrad^2 \Ba
%=
%\spacegrad (\spacegrad \cdot \Ba)
%-
%\spacegrad \cross (\spacegrad \cross \Ba).
%\end{dmath}
%
%Note that the last term can be written in cross product notation using \( \Bc \cdot (\Ba \wedge \Bb) = -\Bc \cross (\Ba \cross \Bb) \) if desired.

Using this relation and proceeding with a few applications of the chain rule, plus the fact that \( \spacegrad 1/\Norm{\Bx - \Bx'} = -\spacegrad' 1/\Norm{\Bx - \Bx'} \), we find
%
%I previously posted a Geometric Algebra attack on the Helmholtz theorem.  Here is
%
%Here's a third way of deriving the Helmholtz theorem inversion relation.  This is a refinement of the traditional vector algebra solution that led to \cref{eqn:helmholtzDerviationMultivector:200}, that uses a factorization of the Laplacian directly, deferring any expansion in terms of dot and cross (or wedge) products until the very end.
%
%Starting from the first line of \cref{eqn:helmholtzDerviationMultivector:160}, we have
\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:720}
-4 \pi \BM(\Bx)
= \int_V dV' \spacegrad^2 \inv{\Norm{\Bx - \Bx'}} \BM(\Bx')
= \gpgradeone{\int_V dV' \spacegrad^2 \inv{\Norm{\Bx - \Bx'}} \BM(\Bx')}
= -\gpgradeone{\int_V dV' \spacegrad \lr{ \spacegrad' \inv{\Norm{\Bx - \Bx'}}} \BM(\Bx')}
= -\gpgradeone{\spacegrad \int_V dV' \lr{
\spacegrad' \frac{\BM(\Bx')}{\Norm{\Bx - \Bx'}}
-\frac{\spacegrad' \BM(\Bx')}{\Norm{\Bx - \Bx'}}
} }
=
-\gpgradeone{\spacegrad \int_{\partial V} dA'
\ncap \frac{\BM(\Bx')}{\Norm{\Bx - \Bx'}}
 }
+\gpgradeone{\spacegrad \int_V dV'
\frac{s(\Bx') + I\BC(\Bx')}{\Norm{\Bx - \Bx'}}
 }
=
-\gpgradeone{\spacegrad \int_{\partial V} dA'
\ncap \frac{\BM(\Bx')}{\Norm{\Bx - \Bx'}}
 }
+\spacegrad \int_V dV'
\frac{s(\Bx')}{\Norm{\Bx - \Bx'}}
+\spacegrad \cdot \int_V dV'
\frac{I\BC(\Bx')}{\Norm{\Bx - \Bx'}}.
\end{dmath}

By inserting a no-op grade selection operation in the second step, the trivector terms that would show up in subsequent steps are automatically filtered out.
%the troublesome trivector term that shows up in my first purely Geometric Algebra
%attempt is eliminated.
This leaves us with a boundary term dependent on the surface and the normal and tangential components of \( \BM \).
Added to that is a pair of volume integrals that provide the unique dependence of \( \BM \) on its divergence and curl.
When the surface is taken to infinity, which requires \( \Norm{\BM}/\Norm{\Bx - \Bx'} \rightarrow 0 \), then the dependence of \( \BM \) on its divergence and curl is unique.

In order to express final result in traditional vector algebra form, a couple transformations are required.
The first is that
\begin{equation}\label{eqn:helmholtzDerviationMultivectorSolution:800}
\gpgradeone{ \Ba I \Bb } = I^2 \Ba \cross \Bb = -\Ba \cross \Bb.
\end{equation}

For the grade selection in the boundary integral, note that
\begin{dmath}\label{eqn:helmholtzDerviationMultivectorSolution:740}
\gpgradeone{ \spacegrad \ncap \BX }
=
\gpgradeone{ \spacegrad (\ncap \cdot \BX) }
+
\gpgradeone{ \spacegrad (\ncap \wedge \BX) }
=
\spacegrad (\ncap \cdot \BX)
+
\gpgradeone{ \spacegrad I (\ncap \cross \BX) }
=
\spacegrad (\ncap \cdot \BX)
-
\spacegrad \cross (\ncap \cross \BX).
\end{dmath}

These give
\boxedEquation{eqn:helmholtzDerviationMultivectorSolution:721}{
\begin{aligned}
\BM(\Bx)
&=
\spacegrad \inv{4\pi} \int_{\partial V} dA' \ncap \cdot \frac{\BM(\Bx')}{\Norm{\Bx - \Bx'}}
-
\spacegrad \cross \inv{4\pi} \int_{\partial V} dA' \ncap \cross \frac{\BM(\Bx')}{\Norm{\Bx - \Bx'}} \\
&-\spacegrad \inv{4\pi} \int_V dV'
\frac{s(\Bx')}{\Norm{\Bx - \Bx'}}
+\spacegrad \cross \inv{4\pi} \int_V dV'
\frac{\BC(\Bx')}{\Norm{\Bx - \Bx'}}.
\end{aligned}
}
%}

%XX
\EndArticle
