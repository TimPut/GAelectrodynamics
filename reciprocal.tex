%
% Copyright © 2016 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
%\input{../blogpost.tex}
%\renewcommand{\basename}{reciprocal}
%%\renewcommand{\dirname}{notes/phy1520/}
%\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}
%%\newcommand{\dateintitle}{}
%%\newcommand{\keywords}{}
%
%\input{../peeter_prologue_print2.tex}
%
%\usepackage{peeters_layout_exercise}
%\usepackage{peeters_braket}
%\usepackage{peeters_figures}
%\usepackage{siunitx}
%%\usepackage{mhchem} % \ce{}
%%\usepackage{macros_bm} % \bcM
%%\usepackage{macros_qed} % \qedmarker
%%\usepackage{txfonts} % \ointclockwise
%
%\beginArtNoToc
%
%\generatetitle{Reciprocal frame vectors}
%%\chapter{reciprocal frame vectors}
%%\label{chap:reciprocal}
%
The end goal of this chapter is to be able to integrate multivector functions along curves and surfaces, known collectively as manifolds.
For our purposes, a manifold is defined by a parameterization, such as the vector valued function \( \Bx(a,b) \) where \( a, b\) are scalar parameters.  With one parameter the vector traces out a curve, with two a surface, three a volume, and so forth.
The respective partial derivatives of such a parameterized vector define a local basis for the surface at the point at which the partials are evaluated.
The span of such a basis is called the tangent space, and the partials that constitute it are not necessarily orthonormal, or even normal.

Unfortunately, in order to work with the curvilinear non-orthonormal bases that will be encountered in general integration theory, some
additional tools are required.

\begin{itemize}
\item
We introduce a reciprocal frame which partially generalizes the notion of normal to non-orthonormal bases.
\item
We will borrow the upper and lower index (tensor) notation from relativistic physics that is useful for the intrinsically non-orthonormal spaces encountered in that study, as this notation works well to define the reciprocal frame.
\end{itemize}

\index{reciprocal frame}
\makedefinition{Reciprocal frame}{dfn:reciprocal:frame}{
Given a basis for a subspace \( \setlr{ \Bx_1, \Bx_2, \cdots \Bx_n } \), where the vectors \( \Bx_i \) are not necessarily orthonormal, the reciprocal frame is defined as the set of vectors \( \setlr{ \Bx^1, \Bx^2, \cdots \Bx^n } \) satisfying

\begin{dmath*}
\Bx_i \cdot \Bx^j = {\delta_i}^j,
\end{dmath*}

where the vector \( \Bx^j \) is not the j-th power of \( \Bx \), but is a superscript index, the conventional way of denoting a reciprocal frame vector, and \( {\delta_i}^j \) is the Kronecker delta.
} % definition

This definition introduces mixed index variables for the first time in this text, which may be unfamiliar.  These are most often used in tensor algebra, where any expression that has pairs of upper and lower indexes implies a sum, and is called the summation (or Einstein) convention.  For example:

\begin{dmath}\label{eqn:reciprocal:400}
\begin{aligned}
a_i b^i &\equiv \sum_i a_i b^i \\
{A^{i}}_j B_i C^j &\equiv \sum_{i,j} {A^{i}}_j B_i C^j.
\end{aligned}
\end{dmath}

Summation convention will not be used explicitly in this text, as it deviates from normal practises in electrical engineering\footnote{Generally, when summation convention is used, explicit summation is only used explicitly when upper and lower indexes are not perfectly matched, but summation is still implied.  Readers of texts that use summation convention can check for proper matching of upper and lower indexes to ensure that the expressions make sense.  Such matching is the reason a mixed index Kronecker delta has been used in the definition of the reciprocal frame.}.

The most important application of a reciprocal frame is for the computation of the coordinates of a vector with respect to a non-orthonormal frame.
Let a vector \( \Ba \) have coordinates \( a^i \) with respect to a basis \( \setlr{ \Bx_i } \)

\begin{dmath}\label{eqn:reciprocal:20}
\Ba = \sum_j a^j \Bx_j,
\end{dmath}

where \( j \) is an index not a power\footnote{In tensor algebra, any index that is found in matched upper and lower index pairs, is known as a dummy summation index, whereas an index that is unmatched is known as a free index.  For example, in \( a^j b_{ij} \) (summation implied) \( j \) is a summation index, and \( i \) is a free index.  We are free to make a change of variables of any summation index, so for the same example we can write
\( a^k b_{ik} \).  These index tracking conventions are obvious when summation symbols are included explicitly, as we will do.}.

Dotting with the reciprocal frame vectors \( \Bx^i \) provides these coordinates \( a^i \)

\begin{dmath}\label{eqn:reciprocal:40}
\Ba \cdot \Bx^i
= \lr{\sum_j a^j \Bx_j} \cdot \Bx^i
= \sum_j a^j {\delta_j}^i
= a^i.
\end{dmath}

The vector can also be expressed with coordinates taken with respect to the reciprocal frame.  Let those coordinates be \( a_i \), so that

\begin{dmath}\label{eqn:reciprocal:60}
\Ba = \sum_i a_i \Bx^i.
\end{dmath}

Dotting with the basis vectors \( \Bx_i \) provides the reciprocal frame relative coordinates \( a_i \)

\begin{dmath}\label{eqn:reciprocal:80}
\Ba \cdot \Bx_i
= \lr{\sum_j a_j \Bx^j} \cdot \Bx_i
= \sum_j a_j {\delta^j}_i
= a_i.
\end{dmath}

We can summarize \cref{eqn:reciprocal:40} and \cref{eqn:reciprocal:80} by stating that a vector can be expressed in terms of coordinates relative to either the original or reciprocal basis as follows

\begin{equation}\label{eqn:reciprocal:420}
\Ba
= \sum_j \lr{ \Ba \cdot \Bx^j } \Bx_j
= \sum_j \lr{ \Ba \cdot \Bx_j } \Bx^j.
\end{equation}

In tensor algebra the basis is generally implied\footnote{
In tensor algebra, a vector, identified by the coordinates \( a^i \) is called a contravariant vector.
When that vector is identified by the coordinates \( a_i \) it is called a covariant vector.  These labels relate to how the coordinates transform with respect to norm preserving transformations.
We have no need of this nomenclature, since we never transform coordinates in isolation, but will always transform the coordinates along with their associated basis vectors.}.

%When doing tensor algebra manipulations, you'll generally have the freedom to swap any pairs of upper and lower indexes as done above.

An example of a 2D oblique Euclidean basis and a corresponding reciprocal basis is plotted in \cref{fig:obliqueReciprocal:obliqueReciprocalFig2}.
Also plotted are the superposition of the projections required to arrive at a given point \( (4,2) \)) along the \( \Be_1, \Be_2 \) directions or the \( \Be^1, \Be^2 \) directions.
In this plot, neither of the reciprocal frame vectors \( \Be^i \) are normal to the corresponding basis vectors \( \Be_i \).
When one of \( \Be_i \) is increased(decreased) in magnitude, there will be a corresponding decrease(increase) in the magnitude of \( \Be^i \), but if the orientation is remained fixed, the corresponding direction of the reciprocal frame vector stays the same.

\imageFigure{../figures/GAelectrodynamics/obliqueReciprocalFig2}{Oblique and reciprocal bases.}{fig:obliqueReciprocal:obliqueReciprocalFig2}{0.5}

How are the reciprocal frame vectors computed?  While these vectors have a natural GA representation, this is not intrinsically a GA problem, and can be solved with standard linear algebra, using a matrix inversion.
For example, given a 2D basis \( \setlr{ \Bx_1, \Bx_2 } \), the reciprocal basis can be assumed to have a coordinate representation in the original basis

\begin{dmath}\label{eqn:reciprocal:100}
\begin{aligned}
\Bx^1 &= a \Bx_1 + b \Bx_2 \\
\Bx^2 &= c \Bx_1 + d \Bx_2.
\end{aligned}
\end{dmath}

Imposing the constraints of \cref{dfn:reciprocal:frame} leads to a pair of 2x2 linear systems that are easily solved to find
\begin{dmath}\label{eqn:reciprocal:120}
\begin{aligned}
\Bx^1 &= \inv{ (\Bx_1)^2 (\Bx_2)^2 - \lr{ \Bx_1 \cdot \Bx_2}^2 } \lr{ (\Bx_2)^2 \Bx_1 - \lr{ \Bx_1 \cdot \Bx_2 } \Bx_2 } \\
\Bx^2 &= \inv{ (\Bx_1)^2 (\Bx_2)^2 - \lr{ \Bx_1 \cdot \Bx_2}^2 } \lr{ (\Bx_1)^2 \Bx_2 - \lr{ \Bx_1 \cdot \Bx_2 } \Bx_1 } \\
\end{aligned}
\end{dmath}

The reader may notice that for \R{3} the denominator is related to the norm of the cross product \( \Bx_1 \cross \Bx_2 \).
More generally, this can be expressed as the square of the bivector \( \Bx_1 \wedge \Bx_2 \)

\begin{dmath}\label{eqn:reciprocal:140}
-\lr{\Bx_1 \wedge \Bx_2 }^2
=
-\lr{\Bx_1 \wedge \Bx_2 } \cdot \lr{\Bx_1 \wedge \Bx_2 }
=
-\lr{ \lr{\Bx_1 \wedge \Bx_2 } \cdot \Bx_1 } \cdot \Bx_2
=
(\Bx_1)^2 (\Bx_2)^2 - \lr{\Bx_1 \cdot \Bx_2}^2.
\end{dmath}

Additionally, the numerators are each dot products of \( \Bx_1, \Bx_2 \) with that same bivector

\begin{dmath}\label{eqn:reciprocal:160}
\begin{aligned}
\Bx^1 &= \frac{\Bx_2 \cdot \lr{ \Bx_1 \wedge \Bx_2 } }{ \lr{ \Bx_1 \wedge \Bx_2}^2 } \\
\Bx^2 &= \frac{\Bx_1 \cdot \lr{ \Bx_2 \wedge \Bx_1 } }{ \lr{ \Bx_1 \wedge \Bx_2}^2 },
\end{aligned}
\end{dmath}

or

%\begin{dmath}\label{eqn:reciprocal:180}
\boxedEquation{eqn:reciprocal:180}{
\begin{aligned}
\Bx^1 &= \Bx_2 \cdot \inv{ \Bx_1 \wedge \Bx_2 } \\
\Bx^2 &= \Bx_1 \cdot \inv{ \Bx_2 \wedge \Bx_1 }.
\end{aligned}
}
%\end{dmath}

Geometrically, dotting with the bivector of the plane is a hybrid rotation and scaling operation.
For example, for \R{2} with \( \Bx_1 = a_1 \Be_1 + a_2 \Be_2, \Bx_2 = b_1 \Be_1 + b_2 \Be_2 \), that pseudoscalar for this basis is

\begin{dmath}\label{eqn:reciprocal:260}
\Bx_1 \wedge \Bx_2
=
\lr{ a_1 \Be_1 + a_2 \Be_2 } \wedge \lr{ b_1 \Be_1 + b_2 \Be_2 }
=
\lr{ a_1 b_2 - a_2 b_1 } \Be_{12}.
\end{dmath}

This has inverse
\begin{dmath}\label{eqn:reciprocal:280}
\inv{\Bx_1 \wedge \Bx_2 }
=
\inv{ a_1 b_2 - a_2 b_1 } \Be_{21}.
\end{dmath}

So for the \R{2} the reciprocal frame is just

\begin{dmath}\label{eqn:reciprocal:300}
\begin{aligned}
\Bx^1 &= \Bx_2 \frac{\Be_{21}}{ a_1 b_2 - a_2 b_1 } \\
\Bx^2 &= \Bx_1 \frac{\Be_{12}}{ a_1 b_2 - a_2 b_1 }
\end{aligned}
\end{dmath}

The vector \( \Bx^1 \) is obtained by rotating \( \Bx_2 \) by \( -\pi/2 \), and rescaling it.
The vector \( \Bx^2 \) is similarly obtained by a scaling and a rotation of \( \Bx_1 \) by \( \pi/2 \).

Generalizing \cref{eqn:reciprocal:180} is almost possible by inspection.
Given
a subspace spanned by a three vector basis \( \setlr{ \Bx_1, \Bx_2, \Bx_3 } \) the reciprocal frame vectors can be written as dot products

\begin{dmath}\label{eqn:reciprocal:320}
\begin{aligned}
\Bx^1 &= \lr{ \Bx_2 \wedge \Bx_3 } \cdot \lr{ \Bx^3 \wedge \Bx^2 \wedge \Bx^1 } \\
\Bx^2 &= \lr{ \Bx_3 \wedge \Bx_1 } \cdot \lr{ \Bx^1 \wedge \Bx^3 \wedge \Bx^2 } \\
\Bx^3 &= \lr{ \Bx_1 \wedge \Bx_2 } \cdot \lr{ \Bx^2 \wedge \Bx^1 \wedge \Bx^3 } \\
\end{aligned}
\end{dmath}

Each of those trivector terms equals \( - \Bx^1 \wedge \Bx^2 \wedge \Bx^3 \) and can be related to the (known) pseudoscalar \( \Bx_1 \wedge \Bx_2 \wedge \Bx_3 \) by observing that

\begin{dmath}\label{eqn:reciprocal:340}
\lr{ \Bx^1 \wedge \Bx^2 \wedge \Bx^3 } \cdot \lr{ \Bx_3 \wedge \Bx_2 \wedge \Bx_1 }
=
\Bx^1 \cdot \lr{ \Bx^2 \cdot \lr{ \Bx^3 \cdot \lr{ \Bx_3 \wedge \Bx_2 \wedge \Bx_1 } }}
=
\Bx^1 \cdot \lr{ \Bx^2 \cdot \lr{ \Bx_2 \wedge \Bx_1 } }
=
\Bx^1 \cdot \Bx_1
=
1,
\end{dmath}

which means that

\begin{dmath}\label{eqn:reciprocal:360}
-\Bx^1 \wedge \Bx^2 \wedge \Bx^3
= -\inv{ \Bx_3 \wedge \Bx_2 \wedge \Bx_1 }
= \inv{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 },
\end{dmath}

and

\boxedEquation{eqn:reciprocal:380}{
\begin{aligned}
\Bx^1 &= \lr{ \Bx_2 \wedge \Bx_3 } \cdot \inv{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } \\
\Bx^2 &= \lr{ \Bx_3 \wedge \Bx_1 } \cdot \inv{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } \\
\Bx^3 &= \lr{ \Bx_1 \wedge \Bx_2 } \cdot \inv{ \Bx_1 \wedge \Bx_2 \wedge \Bx_3 }
\end{aligned}
}

Geometrically, this trivector division is a duality transformation within the subspace spanned by the three vectors \( \Bx_1, \Bx_2, \Bx_3 \), also scaling the result so that the \( \Bx_i \cdot \Bx^j = {\delta_i}^j \) condition is satisfied.

It should be clear how to generalize the reciprocal basis calculation formulas of
\cref{eqn:reciprocal:180} and \cref{eqn:reciprocal:380} to higher dimensions if desired.
%}
%\EndNoBibArticle
